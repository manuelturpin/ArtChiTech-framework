# Rapport de Recherche : Phase 5 - QualitÃ©
## "Tester & Valider"

**Version** : 1.0
**Date** : 29 dÃ©cembre 2024
**Auteur** : Claude (Deep Research - OPUS 4.5)
**Statut** : Final

---

## Table des matiÃ¨res complÃ¨te

### Section 1 : Introduction
- Executive Summary
- 1.1 Objectif de cette phase
- 1.2 Place dans le cycle de vie projet
- 1.3 PrÃ©requis (outputs de DÃ©veloppement)
- 1.4 Outputs attendus (inputs pour Lancement)

### Section 2 : StratÃ©gie de Test et QA
- 2.1 Test Strategy vs Test Plan
- 2.2 Quality Assurance vs Quality Control
- 2.3 Shift-Left Testing Philosophy
- 2.4 Risk-Based Testing
- 2.5 Test Coverage Strategies
- 2.6 Testing Quadrants (Brian Marick / Crispin & Gregory)
- 2.7 Exploratory Testing

### Section 3 : Tests Fonctionnels
- 3.1 Unit Testing
- 3.2 Integration Testing
- 3.3 End-to-End Testing (E2E)
- 3.4 System Testing
- 3.5 Regression Testing
- 3.6 Smoke Testing vs Sanity Testing
- 3.7 API Testing
- 3.8 Database Testing

### Section 4 : Tests Non-Fonctionnels
- 4.1 Performance Testing (Load, Stress, Spike, Endurance, Scalability)
- 4.2 Security Testing (OWASP, SAST, DAST)
- 4.3 Accessibility Testing (WCAG)
- 4.4 Compatibility Testing
- 4.5 Usability Testing
- 4.6 Localization Testing

### Section 5 : Automatisation des Tests
- 5.1 Automation Strategy et ROI
- 5.2 Test Automation Pyramid
- 5.3 Page Object Model (POM)
- 5.4 Test Data Management
- 5.5 Flaky Tests
- 5.6 Visual Regression Testing
- 5.7 Continuous Testing in CI/CD
- 5.8 Test Environment Management

### Section 6 : User Acceptance Testing (UAT)
- 6.1 UAT Planning et Execution
- 6.2 Alpha vs Beta Testing
- 6.3 Acceptance Criteria Verification
- 6.4 UAT Environments
- 6.5 Sign-off Process
- 6.6 Feedback Collection et Triage
- 6.7 UAT dans un contexte e-commerce

### Section 7 : Gestion des Bugs
- 7.1 Bug Lifecycle
- 7.2 Bug Report Writing
- 7.3 Severity vs Priority
- 7.4 Bug Triage Meetings
- 7.5 Root Cause Analysis
- 7.6 Defect Metrics

### Section 8 : Go/No-Go Decision
- 8.1 Release Criteria et Exit Criteria
- 8.2 Quality Gates
- 8.3 Go/No-Go Checklist
- 8.4 Risk Acceptance Documentation
- 8.5 Stakeholder Sign-off
- 8.6 Release Readiness Review

### Section 9 : Questions Transversales
- 9.1 Couverture de tests optimale
- 9.2 Balance automated vs manual
- 9.3 QA intÃ©grÃ© vs Ã©quipe sÃ©parÃ©e
- 9.4 Testing sous pression temporelle
- 9.5 Testing en solo
- 9.6 SpÃ©cificitÃ©s e-commerce

### Section 10 : MÃ©tiers et CompÃ©tences
- 10.1 QA Engineer / Test Engineer
- 10.2 QA Lead / Test Manager
- 10.3 SDET (Software Development Engineer in Test)
- 10.4 Test Automation Engineer
- 10.5 Performance Engineer
- 10.6 Security Tester / Penetration Tester

### Section 11 : Annexes
- 11. Checklist de Phase QualitÃ©
- 12. Red Flags et Anti-Patterns
- 13. Quick Reference
- Glossaire
- Bibliographie et Sources
- Notes et Limitations

---

## Executive Summary

La phase de **QualitÃ©** constitue le dernier rempart avant la mise en production d'un produit digital. Elle ne se limite pas Ã  "trouver des bugs" : c'est un processus stratÃ©gique qui valide que le produit rÃ©pond aux exigences fonctionnelles, non-fonctionnelles et aux attentes des utilisateurs finaux.

### Points clÃ©s de ce rapport

**1. La qualitÃ© est une responsabilitÃ© partagÃ©e**
Le paradigme moderne (Shift-Left Testing) dÃ©place les activitÃ©s de test le plus tÃ´t possible dans le cycle de dÃ©veloppement. L'ISTQB Foundation Level v4.0 (2023) confirme cette tendance en intÃ©grant DevOps et Continuous Delivery dans son syllabus.

**2. Les Testing Quadrants structurent la stratÃ©gie**
Le modÃ¨le de Brian Marick, Ã©tendu par Lisa Crispin et Janet Gregory, organise les types de tests en quatre quadrants selon deux axes : business-facing vs technology-facing, et supporting the team vs critique the product. Cette matrice aide Ã  Ã©quilibrer les efforts de test.

**3. L'automatisation est un investissement, pas une fin**
La Test Automation Pyramid de Mike Cohn recommande une base large de tests unitaires (rapides, stables), une couche intermÃ©diaire de tests d'intÃ©gration, et un sommet Ã©troit de tests E2E (lents, fragiles). Le ROI de l'automatisation doit Ãªtre calculÃ© : tous les tests ne mÃ©ritent pas d'Ãªtre automatisÃ©s.

**4. La sÃ©curitÃ© est non-nÃ©gociable pour l'e-commerce**
L'OWASP Top 10 (2021) et le Web Security Testing Guide (WSTG v4.2) fournissent les rÃ©fÃ©rentiels incontournables. Pour le B2B/B2C, les tests de paiement, la gestion des donnÃ©es personnelles (RGPD) et la protection contre les injections sont critiques.

**5. L'UAT valide la valeur mÃ©tier**
Les tests d'acceptation utilisateur ne sont pas des tests fonctionnels "refaits par le client" : ils vÃ©rifient que le produit rÃ©sout le problÃ¨me mÃ©tier initial. Un processus de sign-off formalisÃ© protÃ¨ge toutes les parties.

**6. Les mÃ©triques guident les dÃ©cisions**
Defect density, defect escape rate, test coverage, mean time to resolution : ces KPIs objectivisent la qualitÃ© et alimentent la dÃ©cision Go/No-Go.

### Recommandations prioritaires pour le contexte e-commerce B2B/B2C

| PrioritÃ© | Action | Impact |
|----------|--------|--------|
| ğŸ”´ Critique | Automatiser les tests du tunnel de paiement | Revenus |
| ğŸ”´ Critique | ImplÃ©menter SAST/DAST dans la CI/CD | SÃ©curitÃ© |
| ğŸŸ  Haute | DÃ©finir des exit criteria mesurables | Gouvernance |
| ğŸŸ  Haute | Former l'Ã©quipe aux principes ISTQB | Professionnalisation |
| ğŸŸ¡ Moyenne | Mettre en place le visual regression testing | UX consistency |
| ğŸŸ¡ Moyenne | Structurer le bug triage hebdomadaire | EfficacitÃ© |

### Avertissement mÃ©thodologique

Ce rapport compile les bonnes pratiques issues de rÃ©fÃ©rentiels reconnus (ISTQB, OWASP, IEEE, ISO 25010) et de praticiens Ã©tablis (Crispin, Gregory, Bach, Kaner, Cohn). Les recommandations doivent Ãªtre adaptÃ©es au contexte spÃ©cifique de chaque projet. Les sections marquÃ©es **[Ã€ VÃ‰RIFIER]** indiquent des informations qui nÃ©cessitent une validation supplÃ©mentaire.

---

## 1. Introduction

### 1.1 Objectif de cette phase

La phase de QualitÃ© a pour mission de **garantir que le produit dÃ©veloppÃ© est conforme aux attentes** avant sa mise en production. Elle rÃ©pond Ã  trois questions fondamentales :

1. **Le produit fonctionne-t-il correctement ?** (Tests fonctionnels)
2. **Le produit fonctionne-t-il bien ?** (Tests non-fonctionnels : performance, sÃ©curitÃ©, accessibilitÃ©)
3. **Le produit rÃ©pond-il au besoin mÃ©tier ?** (Validation utilisateur)

Cette phase ne se limite pas Ã  l'exÃ©cution de tests. Elle englobe :
- La **dÃ©finition de la stratÃ©gie de test** alignÃ©e sur les risques projet
- La **conception des cas de test** couvrant les exigences
- L'**exÃ©cution des tests** (manuels et automatisÃ©s)
- La **gestion des anomalies** dÃ©couvertes
- La **validation par les parties prenantes** (UAT)
- La **dÃ©cision de mise en production** (Go/No-Go)

### 1.2 Place dans le cycle de vie projet

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Discovery  â”‚â”€â”€â”€â–¶â”‚  StratÃ©gie  â”‚â”€â”€â”€â–¶â”‚ Conception  â”‚â”€â”€â”€â–¶â”‚DÃ©veloppementâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                               â”‚
                                                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Croissance  â”‚â—€â”€â”€â”€â”‚  Lancement  â”‚â—€â”€â”€â”€â”‚      â˜… QUALITÃ‰ â˜…               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   (Tester & Valider)           â”‚
                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Position** : Entre le DÃ©veloppement et le Lancement

**Remarque importante** : Dans les approches Agile et DevOps, la phase QualitÃ© n'est pas sÃ©quentielle mais **continue**. Les tests sont intÃ©grÃ©s tout au long du dÃ©veloppement (Shift-Left). Cependant, une phase formelle de validation reste nÃ©cessaire avant chaque release majeure.

### 1.3 PrÃ©requis (outputs de la phase DÃ©veloppement)

Pour dÃ©marrer efficacement la phase QualitÃ©, les Ã©lÃ©ments suivants doivent Ãªtre disponibles :

| PrÃ©requis | Description | Responsable |
|-----------|-------------|-------------|
| **Code source complet** | FonctionnalitÃ©s implÃ©mentÃ©es selon les spÃ©cifications | Ã‰quipe Dev |
| **Environnement de test** | Infrastructure dÃ©diÃ©e, isolÃ©e de la production | DevOps/Ops |
| **Documentation technique** | Architecture, API, modÃ¨le de donnÃ©es | Tech Lead |
| **CritÃ¨res d'acceptation** | Pour chaque User Story / fonctionnalitÃ© | Product Owner |
| **DonnÃ©es de test** | Jeux de donnÃ©es reprÃ©sentatifs | QA + PO |
| **Tests unitaires** | Couverture minimale dÃ©finie (ex: 80%) | DÃ©veloppeurs |
| **Build stable** | Aucune erreur de compilation, CI verte | Ã‰quipe Dev |

### 1.4 Outputs attendus (inputs pour la phase Lancement)

La phase QualitÃ© produit les livrables suivants :

| Output | Description | Destinataire |
|--------|-------------|--------------|
| **Rapport de tests** | SynthÃ¨se des tests exÃ©cutÃ©s, rÃ©sultats, couverture | Stakeholders |
| **Liste des bugs rÃ©siduels** | Bugs connus avec severity/priority, workarounds | Support, Ops |
| **Sign-off UAT** | Validation formelle par les parties prenantes | Direction |
| **Release notes** | FonctionnalitÃ©s livrÃ©es, bugs corrigÃ©s, known issues | Utilisateurs |
| **DÃ©cision Go/No-Go** | Documentation de la dÃ©cision avec justification | Projet |
| **MÃ©triques qualitÃ©** | KPIs finaux (coverage, defect density, etc.) | Management |
| **Plan de rollback** | ProcÃ©dure testÃ©e de retour arriÃ¨re | Ops |

---

# Section 2 : StratÃ©gie de Test et QA

---

## 2.1 Test Strategy vs Test Plan

### Niveau 1 - Vulgarisation

**Test Strategy** et **Test Plan** sont deux documents souvent confondus mais qui servent des objectifs diffÃ©rents.

- **Test Strategy** = "Comment on teste en gÃ©nÃ©ral dans notre organisation" (vision long terme)
- **Test Plan** = "Comment on va tester ce projet spÃ©cifique" (vision projet)

**Analogie** : La Test Strategy est comme la politique de sÃ©curitÃ© d'une entreprise (rÃ¨gles gÃ©nÃ©rales), tandis que le Test Plan est le plan de sÃ©curitÃ© pour un Ã©vÃ©nement particulier (mesures spÃ©cifiques).

**Pourquoi c'est important** : Sans stratÃ©gie claire, chaque projet rÃ©invente la roue. Sans plan dÃ©taillÃ©, l'exÃ©cution manque de structure.

### Niveau 2 - Approfondissement Expert

#### DÃ©finitions formelles

**Test Strategy** (selon ISTQB) :
> "Documentation de haut niveau dÃ©finissant les niveaux de test Ã  rÃ©aliser et les tests au sein de ces niveaux pour une organisation ou un programme (un ou plusieurs projets)."

**Test Plan** (selon ISTQB CTFL v4.0) :
> "Documentation dÃ©crivant les objectifs de test, les ressources et les processus pour un projet de test ; elle identifie notamment les Ã©lÃ©ments Ã  tester, les fonctionnalitÃ©s Ã  tester, les tÃ¢ches de test, les responsabilitÃ©s et les risques."

#### Origine et Ã©volution

- **IEEE 829-1998** : Premier standard dÃ©finissant le format des documents de test
- **IEEE 829-2008** : Mise Ã  jour avec Master Test Plan et niveaux de dÃ©tail
- **ISTQB** : Adoption mondiale via les certifications (Foundation depuis 2005)
- **Ã‰volution Agile** : Documents plus lÃ©gers, stratÃ©gie dans le "Definition of Done"

#### Contenu type d'une Test Strategy

| Section | Contenu |
|---------|---------|
| Scope | Types de produits couverts |
| Niveaux de test | Unit, Integration, System, UAT |
| Types de test | Fonctionnel, Performance, SÃ©curitÃ©, etc. |
| CritÃ¨res d'entrÃ©e/sortie | GÃ©nÃ©riques par niveau |
| MÃ©triques | KPIs standards de l'organisation |
| Outils | Stack technologique de test |
| Environnements | Politique de gestion des environnements |
| Gestion des dÃ©fauts | Process standard |
| RÃ´les et responsabilitÃ©s | Matrice RACI gÃ©nÃ©rique |

#### Contenu type d'un Test Plan

| Section | Contenu |
|---------|---------|
| Identifiant | RÃ©fÃ©rence unique du document |
| Introduction | Contexte projet, objectifs |
| Ã‰lÃ©ments Ã  tester | FonctionnalitÃ©s spÃ©cifiques |
| FonctionnalitÃ©s exclues | Ce qui n'est PAS testÃ© (et pourquoi) |
| Approche | Techniques de test choisies |
| CritÃ¨res pass/fail | SpÃ©cifiques au projet |
| Livrables | Documents produits |
| Planning | Dates, jalons |
| Ressources | Ã‰quipe, outils, environnements |
| Risques | Risques projet spÃ©cifiques |
| Approbations | Signatures requises |

#### Standards et certifications

- **ISTQB CTFL** : Couvre Test Strategy et Test Plan
- **ISO/IEC/IEEE 29119-3:2021** : Documentation des processus de test
- **TMMi (Test Maturity Model integration)** : Niveaux de maturitÃ© incluant la documentation

#### Tendances 2024-2025

1. **Lightweight documentation** : Moins de paperasse, plus d'action
2. **Living documents** : Test plans dans des wikis collaboratifs (Confluence, Notion)
3. **Strategy as Code** : Configuration des pipelines = stratÃ©gie de test
4. **Context-Driven Testing** : Adaptation continue plutÃ´t que plan rigide

#### Critiques et limites

- **Sur-documentation** : Risk de documents jamais lus ni mis Ã  jour
- **Fausse sÃ©curitÃ©** : Un plan n'est pas une garantie de qualitÃ©
- **RigiditÃ©** : Plans dÃ©taillÃ©s obsolÃ¨tes face aux changements Agile
- **Ã‰coles alternatives** : Le Context-Driven Testing (James Bach, Cem Kaner) critique les approches trop formalisÃ©es

### Niveau 3 - Application Pratique

#### Contexte d'utilisation optimal

- **Test Strategy** : Organisations avec plusieurs projets, Ã©quipes distribuÃ©es, besoin de cohÃ©rence
- **Test Plan** : Tout projet nÃ©cessitant traÃ§abilitÃ© (rÃ©glementaire, contractuel, complexitÃ©)

#### Anti-patterns

| Anti-pattern | ProblÃ¨me | Solution |
|--------------|----------|----------|
| Copy-paste plan | Plan gÃ©nÃ©rique non adaptÃ© | Personnaliser chaque section |
| Plan exhaustif jamais lu | Trop long, trop dÃ©taillÃ© | Limiter Ã  10-15 pages max |
| Plan figÃ© | Non mis Ã  jour | Reviews rÃ©guliÃ¨res (sprint) |
| Plan sans propriÃ©taire | Personne ne maintient | Assigner un responsable |

#### Exemple concret : E-commerce B2C

**Extrait de Test Plan pour une refonte checkout**

```
PROJET: Refonte Tunnel de Paiement v2.0
DATE: Janvier 2025

1. SCOPE
   - Panier (ajout, modification, suppression)
   - Identification (guest, compte, SSO)
   - Adresse (livraison, facturation)
   - Paiement (CB, PayPal, Apple Pay)
   - Confirmation et email

2. HORS SCOPE
   - Back-office (testÃ© sÃ©parÃ©ment)
   - IntÃ©gration ERP (tests d'intÃ©gration dÃ©diÃ©s)

3. APPROCHE
   - Tests E2E automatisÃ©s: 100% du happy path
   - Tests manuels: Edge cases, paiements rÃ©els sandbox
   - Tests de charge: 500 users concurrent (objectif Black Friday)
   - Tests sÃ©curitÃ©: OWASP Top 10, PCI-DSS compliance

4. CRITÃˆRES GO/NO-GO
   - 0 bug critique/bloquant
   - < 5 bugs majeurs avec workaround
   - Performance: < 3s page load, < 500ms API response
   - Taux de conversion test A/B: non infÃ©rieur Ã  -2%

5. PLANNING
   - Semaine 1-2: Tests fonctionnels
   - Semaine 3: Tests performance + sÃ©curitÃ©
   - Semaine 4: UAT + corrections
   - Semaine 5: Go-live
```

---

## 2.2 Quality Assurance vs Quality Control

### Niveau 1 - Vulgarisation

**Quality Assurance (QA)** et **Quality Control (QC)** sont deux approches complÃ©mentaires :

- **QA** = "PrÃ©venir les problÃ¨mes" (processus, mÃ©thodes, formation)
- **QC** = "DÃ©tecter les problÃ¨mes" (tests, inspections, revues)

**Analogie culinaire** :
- QA = S'assurer que le chef est formÃ©, que les recettes sont standardisÃ©es, que la cuisine est propre
- QC = GoÃ»ter chaque plat avant de le servir au client

**Pourquoi c'est important** : QC sans QA = on trouve les bugs mais on ne rÃ©duit pas leur source. QA sans QC = on a des processus mais pas de vÃ©rification finale.

### Niveau 2 - Approfondissement Expert

#### DÃ©finitions formelles

**Quality Assurance** (ISO 9000:2015) :
> "Partie du management de la qualitÃ© visant Ã  donner confiance en ce que les exigences pour la qualitÃ© seront satisfaites."

**Quality Control** (ISO 9000:2015) :
> "Partie du management de la qualitÃ© axÃ©e sur la satisfaction des exigences pour la qualitÃ©."

#### Tableau comparatif

| Aspect | Quality Assurance | Quality Control |
|--------|-------------------|-----------------|
| **Focus** | Processus | Produit |
| **Objectif** | PrÃ©vention des dÃ©fauts | DÃ©tection des dÃ©fauts |
| **TemporalitÃ©** | Proactif | RÃ©actif |
| **ResponsabilitÃ©** | Toute l'Ã©quipe | Ã‰quipe QC/Test |
| **ActivitÃ©s** | Standards, revues de code, formation | Tests, inspections |
| **Orientation** | Process-oriented | Product-oriented |
| **Exemples** | DÃ©finition de DoD, revues de design | ExÃ©cution de tests, validation |

#### Origine et Ã©volution

- **1920s** : Walter Shewhart introduit le contrÃ´le statistique de la qualitÃ© (Bell Labs)
- **1950s** : W. Edwards Deming et Joseph Juran dÃ©veloppent le TQM (Total Quality Management)
- **1987** : ISO 9000 premiÃ¨re version
- **1990s** : Adaptation au dÃ©veloppement logiciel (CMM, puis CMMI)
- **2000s** : IntÃ©gration dans Agile ("Quality is everyone's responsibility")
- **2010s** : DevOps et "Quality Engineering"

#### ActivitÃ©s QA typiques

1. **Process Definition**
   - DÃ©finition des workflows de dÃ©veloppement
   - Standards de codage
   - Templates et checklists

2. **Reviews et Audits**
   - Code reviews systÃ©matiques
   - Design reviews
   - Audits de processus

3. **MÃ©triques et AmÃ©lioration**
   - Collecte de mÃ©triques
   - Analyse des tendances
   - Actions correctives

4. **Formation**
   - Onboarding qualitÃ©
   - Workshops bonnes pratiques

#### ActivitÃ©s QC typiques

1. **Test Design**
   - CrÃ©ation des cas de test
   - PrÃ©paration des donnÃ©es de test

2. **Test Execution**
   - ExÃ©cution manuelle et automatisÃ©e
   - Enregistrement des rÃ©sultats

3. **Defect Management**
   - Identification et logging des bugs
   - VÃ©rification des corrections

4. **Reporting**
   - Rapports de test
   - MÃ©triques de qualitÃ© produit

#### Certifications associÃ©es

| Certification | Focus | Organisme |
|---------------|-------|-----------|
| ISTQB CTFL | Test fondamentaux | ISTQB |
| ISTQB CTAL | Test avancÃ© | ISTQB |
| CSQA | Software Quality Assurance | QAI |
| Six Sigma | Process improvement | ASQ |
| ISO 9001 Lead Auditor | SystÃ¨mes qualitÃ© | Divers |

#### Ã‰volution vers "Quality Engineering"

Tendance 2020s : fusion des rÃ´les QA et QC vers le **Quality Engineer** qui :
- Participe Ã  la dÃ©finition des processus (QA)
- ConÃ§oit et exÃ©cute les tests (QC)
- Automatise les vÃ©rifications (Dev)
- Analyse les mÃ©triques (Data)

### Niveau 3 - Application Pratique

#### Contexte d'utilisation optimal

- **QA fort** : Environnements rÃ©glementÃ©s (finance, santÃ©), grandes organisations
- **QC fort** : Projets courts, MVP, startups early-stage
- **Ã‰quilibre** : Toute organisation mature

#### Anti-patterns

| Anti-pattern | ProblÃ¨me | Solution |
|--------------|----------|----------|
| "QA Police" | QA vu comme obstacle | IntÃ©grer QA dans l'Ã©quipe |
| QC only | Aucune amÃ©lioration des processus | Investir dans QA |
| Over-process | Bureaucratie qualitÃ© | Lean QA, juste ce qu'il faut |
| Quality silo | QualitÃ© = job du QA uniquement | "Quality is everyone's job" |

#### Exemple concret : Mise en place QA/QC pour startup e-commerce

**Phase 1 - QC minimal (0-10 employÃ©s)**
- Tests manuels sur les parcours critiques
- Bug tracking simple (GitHub Issues)
- Pas de process formalisÃ©

**Phase 2 - Introduction QA (10-30 employÃ©s)**
- Definition of Done avec critÃ¨res qualitÃ©
- Code reviews obligatoires
- Tests automatisÃ©s CI/CD
- PremiÃ¨re personne dÃ©diÃ©e QA

**Phase 3 - QA mature (30+ employÃ©s)**
- StratÃ©gie de test documentÃ©e
- MÃ©triques suivies (coverage, defect rate)
- Audits pÃ©riodiques
- Formation continue Ã©quipe
- Quality Engineer vs QA manuel

---

## 2.3 Shift-Left Testing Philosophy

### Niveau 1 - Vulgarisation

**Shift-Left Testing** signifie "dÃ©caler les tests vers la gauche" sur la timeline du projet, c'est-Ã -dire **tester le plus tÃ´t possible**.

**Analogie** : Au lieu d'attendre qu'une maison soit construite pour vÃ©rifier les plans, on vÃ©rifie les plans avant de poser la premiÃ¨re brique.

**Pourquoi c'est important** : Un bug dÃ©tectÃ© en production coÃ»te 100x plus cher Ã  corriger qu'un bug dÃ©tectÃ© en phase de design. Plus on teste tÃ´t, moins on paie cher.

### Niveau 2 - Approfondissement Expert

#### DÃ©finition technique

> "Shift-Left Testing est une approche qui consiste Ã  intÃ©grer les activitÃ©s de test dÃ¨s les premiÃ¨res phases du cycle de dÃ©veloppement logiciel, plutÃ´t que de les concentrer en fin de cycle."

#### Origine et Ã©volution

- **2001** : Larry Smith introduit le terme dans un article pour Dr. Dobb's Journal
- **2000s** : Adoption progressive avec l'Agile
- **2010s** : AccÃ©lÃ©ration avec DevOps et CI/CD
- **2020s** : Extension Ã  "Shift-Left Security" et "Shift-Left Performance"

#### Le coÃ»t des dÃ©fauts selon la phase

| Phase de dÃ©tection | CoÃ»t relatif | Source |
|--------------------|--------------|--------|
| Requirements | 1x | IBM Systems Sciences Institute |
| Design | 3-6x | (Ã‰tude classique, chiffres indicatifs) |
| Development | 10x | |
| Testing | 15-40x | |
| Production | 30-100x | |

**Note** : Ces chiffres sont souvent citÃ©s mais leur source originale (IBM, annÃ©es 1970-80) est difficile Ã  vÃ©rifier prÃ©cisÃ©ment. L'ordre de grandeur reste pertinent. **[Ã€ VÃ‰RIFIER]** pour les ratios exacts.

#### Pratiques Shift-Left concrÃ¨tes

| Pratique | Description | Phase |
|----------|-------------|-------|
| **Requirements Testing** | Review et validation des exigences | Requirements |
| **TDD** (Test-Driven Development) | Ã‰crire le test avant le code | Development |
| **BDD** (Behavior-Driven Development) | SpÃ©cifications exÃ©cutables | Design â†’ Dev |
| **Static Analysis** | Analyse du code sans exÃ©cution | Development |
| **Pair Programming** | Revue en temps rÃ©el | Development |
| **Code Review** | Inspection avant merge | Development |
| **Unit Testing** | Tests automatisÃ©s par les devs | Development |
| **Contract Testing** | Validation des interfaces API | Design â†’ Dev |

#### Shift-Left vs Shift-Right

| Concept | Focus | ActivitÃ©s |
|---------|-------|-----------|
| **Shift-Left** | PrÃ©vention, dÃ©tection prÃ©coce | Tests early, static analysis, reviews |
| **Shift-Right** | Validation en conditions rÃ©elles | Testing in production, A/B tests, feature flags, observability |

Les deux approches sont **complÃ©mentaires** dans une stratÃ©gie moderne.

#### Mesure de l'efficacitÃ© Shift-Left

**MÃ©triques clÃ©s** :
- **Defect Detection Percentage (DDP)** par phase
- **Defect Removal Efficiency (DRE)** avant production
- **Cost of Quality (CoQ)** : prÃ©vention vs dÃ©tection vs dÃ©faillance

Formule DRE :
```
DRE = (DÃ©fauts trouvÃ©s avant production / Total dÃ©fauts) Ã— 100
```

Objectif : DRE > 95%

#### ISTQB v4.0 et Shift-Left

Le syllabus ISTQB Foundation Level v4.0 (2023) intÃ¨gre explicitement Shift-Left :
> "Early testing saves time and money. [...] Both static testing and dynamic testing should start as early as possible."

### Niveau 3 - Application Pratique

#### Contexte d'utilisation optimal

- **Projets Agile/DevOps** : CI/CD avec tests automatisÃ©s
- **Ã‰quipes matures** : Capables de TDD, code review systÃ©matique
- **Produits critiques** : OÃ¹ le coÃ»t des bugs en prod est Ã©levÃ©

#### Anti-patterns

| Anti-pattern | ProblÃ¨me | Solution |
|--------------|----------|----------|
| Shift-Left sans formation | Devs non formÃ©s aux tests | Investir en formation |
| Tout automatiser trop tÃ´t | ROI nÃ©gatif sur MVP | Automatiser progressivement |
| Oublier Shift-Right | Pas de feedback production | Monitoring, feature flags |
| Tests unitaires = suffisant | Fausse sÃ©curitÃ© | Maintenir tests E2E |

#### Exemple concret : Pipeline CI/CD Shift-Left pour e-commerce

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        SHIFT-LEFT PIPELINE                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  COMMIT â”€â”€â–¶ LINT â”€â”€â–¶ UNIT â”€â”€â–¶ SAST â”€â”€â–¶ BUILD â”€â”€â–¶ INTEGRATION   â”‚
â”‚              â”‚       TESTS     â”‚                    TESTS        â”‚
â”‚              â”‚        â”‚        â”‚                     â”‚           â”‚
â”‚              â–¼        â–¼        â–¼                     â–¼           â”‚
â”‚           Style    Coverage  Security            Contract        â”‚
â”‚           Check     > 80%    Scan                Tests           â”‚
â”‚                                                                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚                                                                  â”‚
â”‚  â”€â”€â–¶ E2E TESTS â”€â”€â–¶ PERF â”€â”€â–¶ DEPLOY â”€â”€â–¶ SMOKE â”€â”€â–¶ MONITORING    â”‚
â”‚       (Critical    TESTS    STAGING    TESTS                     â”‚
â”‚        Paths)                                                    â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Temps total : < 15 minutes (objectif)
Blocage sur : Lint errors, Test failures, Security critical
```

---

## 2.4 Risk-Based Testing

### Niveau 1 - Vulgarisation

**Risk-Based Testing (RBT)** consiste Ã  **prioriser les tests en fonction des risques**. On teste davantage ce qui est le plus susceptible de causer des problÃ¨mes graves.

**Analogie** : Un mÃ©decin ne fait pas tous les examens possibles Ã  chaque visite. Il priorise selon les symptÃ´mes et l'historique du patient. De mÃªme, on concentre les tests lÃ  oÃ¹ les risques sont les plus Ã©levÃ©s.

**Pourquoi c'est important** : Avec des ressources et du temps limitÃ©s, tester tout de maniÃ¨re Ã©gale est impossible. RBT permet d'optimiser l'effort de test.

### Niveau 2 - Approfondissement Expert

#### DÃ©finition technique (ISTQB)

> "Risk-based testing est une approche de test oÃ¹ les activitÃ©s de test sont priorisÃ©es et sÃ©lectionnÃ©es en fonction de l'analyse des risques."

**Risque Produit** = ProbabilitÃ© de dÃ©faillance Ã— Impact de la dÃ©faillance

#### Origine et Ã©volution

- **Concept gÃ©nÃ©ral** : Issu du Risk Management (ISO 31000)
- **Application au test** : FormalisÃ© dans les annÃ©es 1990-2000
- **ISTQB** : IntÃ©grÃ© au syllabus Foundation depuis les premiÃ¨res versions
- **Standards** : ISO/IEC/IEEE 29119-1 inclut le Risk-Based Testing

#### Processus RBT

```
1. IDENTIFICATION DES RISQUES
   â””â”€â–¶ Brainstorming, historique, expertise

2. Ã‰VALUATION DES RISQUES
   â””â”€â–¶ ProbabilitÃ© Ã— Impact = Niveau de risque

3. PRIORISATION
   â””â”€â–¶ Focus sur risques Ã©levÃ©s

4. CONCEPTION DES TESTS
   â””â”€â–¶ Plus de tests sur zones Ã  risque

5. EXÃ‰CUTION
   â””â”€â–¶ Commencer par les risques Ã©levÃ©s

6. RÃ‰Ã‰VALUATION
   â””â”€â–¶ Ajuster selon les rÃ©sultats
```

#### Matrice de risque classique

|              | Impact Faible | Impact Moyen | Impact Ã‰levÃ© |
|--------------|---------------|--------------|--------------|
| **Proba Ã‰levÃ©e** | Moyen | Ã‰levÃ© | Critique |
| **Proba Moyenne** | Faible | Moyen | Ã‰levÃ© |
| **Proba Faible** | NÃ©gligeable | Faible | Moyen |

#### Facteurs influenÃ§ant le risque produit

**Facteurs de probabilitÃ© (Likelihood)** :
- ComplexitÃ© du code
- Nouvelles technologies
- DÃ©veloppeur junior
- DÃ©pendances externes
- Historique de bugs
- FrÃ©quence de changement

**Facteurs d'impact (Impact)** :
- CriticitÃ© business (ex: paiement vs page "Ã€ propos")
- Nombre d'utilisateurs affectÃ©s
- RÃ©glementation (RGPD, PCI-DSS)
- RÃ©putation
- Financier direct (pertes de ventes)
- SÃ©curitÃ© des utilisateurs

#### Techniques d'identification des risques

1. **Analyse d'historique** : OÃ¹ Ã©taient les bugs prÃ©cÃ©demment ?
2. **Workshops risques** : Brainstorming avec Ã©quipe et stakeholders
3. **Checklists** : Listes de risques types par domaine
4. **FMEA** (Failure Mode and Effects Analysis) : Analyse formelle
5. **Expertise mÃ©tier** : Le Product Owner connaÃ®t les zones sensibles

#### MÃ©triques associÃ©es

- **Risk Coverage** : % des risques identifiÃ©s couverts par des tests
- **Defect Detection by Risk Level** : Bugs trouvÃ©s par niveau de risque
- **Residual Risk** : Risques non couverts ou partiellement couverts

### Niveau 3 - Application Pratique

#### Contexte d'utilisation optimal

- **Ressources limitÃ©es** : Impossible de tout tester
- **DÃ©lais serrÃ©s** : Priorisation nÃ©cessaire
- **Projets legacy** : Focus sur les parties modifiÃ©es + critiques
- **RÃ©glementaire** : DÃ©montrer une approche mÃ©thodique

#### Anti-patterns

| Anti-pattern | ProblÃ¨me | Solution |
|--------------|----------|----------|
| Ã‰valuation subjective | Biais personnels | CritÃ¨res objectifs, consensus Ã©quipe |
| Une seule Ã©valuation | Risques Ã©voluent | RÃ©Ã©valuation rÃ©guliÃ¨re |
| Ignorer les "petits" risques | Accumulation | Couvrir minimum sur tout |
| Sur-analyse | Paralysie | Timeboxer l'analyse |

#### Exemple concret : RBT pour site e-commerce B2B

**Contexte** : Plateforme de commande B2B avec 500 clients entreprises

**Analyse des risques** :

| FonctionnalitÃ© | ProbabilitÃ© | Impact | Risque | Effort Test |
|----------------|-------------|--------|--------|-------------|
| Calcul prix B2B (remises, volumes) | Ã‰levÃ©e (complexe) | Critique (CA) | **CRITIQUE** | 40% |
| Workflow validation commande | Moyenne | Ã‰levÃ© (opÃ©rations) | **Ã‰LEVÃ‰** | 25% |
| Authentification SSO | Moyenne | Ã‰levÃ© (sÃ©curitÃ©) | **Ã‰LEVÃ‰** | 15% |
| Catalogue produits | Faible | Moyen | MOYEN | 10% |
| Page "Mon compte" | Faible | Faible | FAIBLE | 5% |
| Footer, pages statiques | TrÃ¨s faible | Faible | NÃ‰GLIGEABLE | 5% |

**Allocation rÃ©sultante** :
- Tests automatisÃ©s : Calcul prix (100% coverage), Workflow (80%), SSO (100%)
- Tests manuels approfondis : Calcul prix (edge cases), Workflow (scÃ©narios complexes)
- Tests de non-rÃ©gression : Tout
- Tests exploratoires : ConcentrÃ©s sur risques Ã©levÃ©s

---

## 2.5 Test Coverage Strategies

### Niveau 1 - Vulgarisation

**Test Coverage** (couverture de test) mesure **quelle proportion du systÃ¨me est testÃ©e**. Il existe deux grandes approches :

- **Code Coverage** : Quel pourcentage du code est exÃ©cutÃ© par les tests ?
- **Requirements Coverage** : Quel pourcentage des exigences est vÃ©rifiÃ© par des tests ?

**Analogie** : Si vous vÃ©rifiez une maison, le code coverage dirait "j'ai visitÃ© 80% des piÃ¨ces", tandis que le requirements coverage dirait "j'ai vÃ©rifiÃ© 90% des points de la checklist d'inspection".

**Pourquoi c'est important** : La couverture indique les "zones d'ombre" non testÃ©es. Mais attention : 100% coverage â‰  100% qualitÃ©.

### Niveau 2 - Approfondissement Expert

#### Types de Code Coverage

| Type | Mesure | Exemple |
|------|--------|---------|
| **Line Coverage** | Lignes exÃ©cutÃ©es / Total lignes | 80/100 = 80% |
| **Branch Coverage** | Branches (if/else) exÃ©cutÃ©es | Toutes les conditions testÃ©es |
| **Function Coverage** | Fonctions appelÃ©es | Toutes les fonctions utilisÃ©es |
| **Statement Coverage** | Instructions exÃ©cutÃ©es | Similaire Ã  line coverage |
| **Condition Coverage** | Conditions boolÃ©ennes | Chaque condition true ET false |
| **Path Coverage** | Chemins d'exÃ©cution | Toutes les combinaisons (explosif) |
| **MC/DC** | Modified Condition/Decision | Standard aÃ©ronautique (DO-178C) |

#### HiÃ©rarchie de rigueur

```
Path Coverage (le plus strict, souvent impraticable)
        â†‘
MC/DC (aviation, safety-critical)
        â†‘
Branch Coverage (recommandÃ© minimum)
        â†‘
Line/Statement Coverage (baseline)
        â†‘
Function Coverage (minimum minimorum)
```

#### Requirements Coverage

**Formule** :
```
Requirements Coverage = (Exigences avec tests / Total exigences) Ã— 100
```

**Traceability Matrix** : Tableau liant chaque exigence Ã  ses cas de test

| Req ID | Description | Test Cases | Status |
|--------|-------------|------------|--------|
| REQ-001 | Login utilisateur | TC-001, TC-002, TC-003 | âœ… Covered |
| REQ-002 | RÃ©cupÃ©ration MDP | TC-004 | âœ… Covered |
| REQ-003 | 2FA | - | âŒ Not covered |

#### Objectifs de coverage : les chiffres du marchÃ©

**[Ã€ VÃ‰RIFIER]** - Ces chiffres sont indicatifs et varient selon les sources :

| Contexte | Code Coverage RecommandÃ© | Source indicative |
|----------|--------------------------|-------------------|
| Safety-critical (aviation) | 100% MC/DC | DO-178C |
| Finance, santÃ© | 80-90% branch | Pratique courante |
| Web/SaaS standard | 70-80% line | Convention industrie |
| MVP/Startup | 50-60% minimum | Pragmatique |
| Legacy (amÃ©lioration) | +10% par release | Progressif |

#### Limites du Code Coverage

**Ce que la couverture NE dit PAS** :
1. **QualitÃ© des assertions** : Le code est exÃ©cutÃ© mais les tests vÃ©rifient-ils quelque chose ?
2. **Edge cases** : Les valeurs limites sont-elles testÃ©es ?
3. **Combinaisons** : Les interactions entre fonctions sont-elles couvertes ?
4. **Non-fonctionnel** : Performance, sÃ©curitÃ© ne sont pas mesurÃ©es
5. **Code mort** : Du code jamais appelÃ© gonfle artificiellement le coverage

**Exemple piÃ¨ge** :
```javascript
function divide(a, b) {
  return a / b;
}

// Test avec 100% line coverage mais 0 vÃ©rification
test('divide', () => {
  divide(10, 2); // Pas d'assertion ! Le test passe mais ne vÃ©rifie rien
});
```

#### Mutation Testing : au-delÃ  du coverage

**Principe** : Introduire des "mutations" (bugs artificiels) dans le code et vÃ©rifier que les tests les dÃ©tectent.

**Mutation Score** = (Mutations dÃ©tectÃ©es / Total mutations) Ã— 100

**Outils** : Stryker (JS), PITest (Java), mutmut (Python)

Plus rÃ©vÃ©lateur que le coverage simple car vÃ©rifie la qualitÃ© des assertions.

### Niveau 3 - Application Pratique

#### Contexte d'utilisation optimal

- **Code coverage** : Projets avec bonne base de tests unitaires
- **Requirements coverage** : Projets avec traÃ§abilitÃ© exigÃ©e (contrats, rÃ©glementation)
- **Mutation testing** : Projets critiques oÃ¹ la qualitÃ© des tests est primordiale

#### Anti-patterns

| Anti-pattern | ProblÃ¨me | Solution |
|--------------|----------|----------|
| "Gaming" the coverage | Tests sans assertions pour gonfler % | VÃ©rifier assertions, mutation testing |
| Coverage unique mÃ©trique | Ignore autres aspects qualitÃ© | Dashboard multi-mÃ©triques |
| 100% comme objectif absolu | Rendements dÃ©croissants | DÃ©finir seuil pragmatique |
| Mesurer sans agir | MÃ©trique non utilisÃ©e | Alertes si coverage < seuil |

#### Exemple concret : Dashboard Coverage pour e-commerce

```
DASHBOARD COUVERTURE - CHECKOUT MODULE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Code Coverage (Jest)
â”œâ”€â”€ Line:     82% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ (target: 80%) âœ…
â”œâ”€â”€ Branch:   75% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘ (target: 70%) âœ…
â””â”€â”€ Function: 91% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ (target: 85%) âœ…

Requirements Coverage
â”œâ”€â”€ Panier:      100% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (12/12 tests)
â”œâ”€â”€ Paiement:    100% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (8/8 tests)
â”œâ”€â”€ Livraison:    89% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ (8/9 tests) âš ï¸
â””â”€â”€ Confirmation: 75% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘ (3/4 tests) âš ï¸

Mutation Score (Stryker) - Last run: 3 days ago
â””â”€â”€ Score: 68% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ (target: 70%) âš ï¸
    Surviving mutants: 24

ACTION REQUIRED:
- REQ-LIV-009: "Choix point relais" non couvert
- REQ-CONF-004: "Email rÃ©capitulatif" partiellement couvert
- 24 mutants survivants dans CartCalculator.js
```

---

## 2.6 Testing Quadrants

### Niveau 1 - Vulgarisation

Les **Testing Quadrants** (ou Agile Testing Quadrants) sont un modÃ¨le qui organise les diffÃ©rents types de tests en 4 catÃ©gories selon deux axes :

- **Axe horizontal** : Tests orientÃ©s "Business" (comprÃ©hensibles par le mÃ©tier) vs "Technology" (techniques)
- **Axe vertical** : Tests qui "supportent l'Ã©quipe" (guident le dÃ©veloppement) vs "critiquent le produit" (Ã©valuent aprÃ¨s)

**Analogie** : C'est comme organiser les outils d'un artisan en 4 boÃ®tes : outils de mesure prÃ©cis, outils de crÃ©ation, outils de finition, outils de vÃ©rification qualitÃ©.

**Pourquoi c'est important** : Les quadrants aident Ã  s'assurer qu'on n'oublie aucun type de test et Ã  Ã©quilibrer les efforts.

### Niveau 2 - Approfondissement Expert

#### Origine et Ã©volution

- **2003** : Brian Marick crÃ©e la matrice originale "Agile Testing Matrix"
- **2009** : Lisa Crispin et Janet Gregory popularisent et Ã©tendent le concept dans "Agile Testing: A Practical Guide for Testers and Agile Teams"
- **2014** : "More Agile Testing" affine le modÃ¨le
- **2024** : Le modÃ¨le reste une rÃ©fÃ©rence, intÃ©grÃ© au "Holistic Testing Model" de Crispin & Gregory

#### Les 4 Quadrants dÃ©taillÃ©s

```
                        Business-Facing
                              â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚         Q2             â”‚          Q3            â”‚
     â”‚    Functional Tests    â”‚   Exploratory Testing  â”‚
     â”‚    Story Tests         â”‚   Usability Testing    â”‚
     â”‚    Prototypes          â”‚   UAT                  â”‚
     â”‚    Simulations         â”‚   Alpha/Beta Testing   â”‚
     â”‚                        â”‚                        â”‚
     â”‚    [AUTOMATED]         â”‚   [MANUAL]             â”‚
     â”‚                        â”‚                        â”‚
Supportâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€Critique
Team   â”‚         Q1             â”‚          Q4            â”‚Product
     â”‚    Unit Tests          â”‚   Performance Testing  â”‚
     â”‚    Component Tests     â”‚   Load Testing         â”‚
     â”‚    Integration Tests   â”‚   Security Testing     â”‚
     â”‚                        â”‚   "-ility" Testing     â”‚
     â”‚                        â”‚                        â”‚
     â”‚    [AUTOMATED]         â”‚   [TOOLS]              â”‚
     â”‚                        â”‚                        â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                        Technology-Facing
```

#### Quadrant Q1 : Technology-Facing / Supporting the Team

**Objectif** : Guider le dÃ©veloppement, feedback rapide aux dÃ©veloppeurs

| Type de test | Description | Automatisation |
|--------------|-------------|----------------|
| Unit Tests | Test d'une fonction/mÃ©thode isolÃ©e | Fortement automatisÃ© |
| Component Tests | Test d'un composant/module | AutomatisÃ© |
| Integration Tests | Test des interactions entre composants | AutomatisÃ© |

**CaractÃ©ristiques** :
- Ã‰crits par les dÃ©veloppeurs
- ExÃ©cutÃ©s frÃ©quemment (Ã  chaque commit)
- Rapides (< quelques secondes)
- Base de la pyramide d'automatisation

#### Quadrant Q2 : Business-Facing / Supporting the Team

**Objectif** : VÃ©rifier que le produit rÃ©pond aux besoins mÃ©tier, dÃ©finir le comportement attendu

| Type de test | Description | Automatisation |
|--------------|-------------|----------------|
| Functional Tests | ScÃ©narios fonctionnels complets | Automatisable (E2E) |
| Story Tests | Tests dÃ©rivÃ©s des User Stories | Automatisable |
| Prototypes | Validation prÃ©coce des concepts | Manuel |
| Simulations | Maquettes interactives | Manuel |
| Examples (BDD) | SpÃ©cifications par l'exemple | Automatisable |

**CaractÃ©ristiques** :
- Collaboration dev/QA/PO
- Langage mÃ©tier (Gherkin, etc.)
- DÃ©finissent les critÃ¨res d'acceptation
- Servent de documentation vivante

#### Quadrant Q3 : Business-Facing / Critique Product

**Objectif** : Ã‰valuer le produit du point de vue utilisateur, trouver ce qui manque

| Type de test | Description | Automatisation |
|--------------|-------------|----------------|
| Exploratory Testing | Exploration crÃ©ative du produit | Manuel (cerveau humain) |
| Usability Testing | Ã‰valuation de l'expÃ©rience utilisateur | Manuel |
| UAT | Validation par les parties prenantes | Manuel |
| Alpha/Beta Testing | Tests par utilisateurs rÃ©els | Manuel |
| User Scenarios | Parcours utilisateurs rÃ©alistes | Partiellement auto |

**CaractÃ©ristiques** :
- Principalement manuel (intelligence humaine requise)
- DÃ©couverte de problÃ¨mes non anticipÃ©s
- Feedback qualitatif
- RÃ©alisÃ© tard dans le cycle (produit fonctionnel requis)

#### Quadrant Q4 : Technology-Facing / Critique Product

**Objectif** : Ã‰valuer les attributs non-fonctionnels (les "-ilities")

| Type de test | Description | Automatisation |
|--------------|-------------|----------------|
| Performance Testing | Temps de rÃ©ponse, throughput | Outils spÃ©cialisÃ©s |
| Load Testing | Comportement sous charge | Outils spÃ©cialisÃ©s |
| Stress Testing | Limites du systÃ¨me | Outils spÃ©cialisÃ©s |
| Security Testing | VulnÃ©rabilitÃ©s | Outils + expertise |
| Reliability Testing | StabilitÃ© dans le temps | AutomatisÃ© |
| Scalability Testing | CapacitÃ© de montÃ©e en charge | Outils + infra |

**CaractÃ©ristiques** :
- Outils spÃ©cialisÃ©s requis
- Expertise technique nÃ©cessaire
- Souvent rÃ©alisÃ© pÃ©riodiquement (pas Ã  chaque sprint)
- RÃ©sultats objectifs et mesurables

#### Utilisation stratÃ©gique des quadrants

**RÃ©partition effort typique** (indicatif, varie selon contexte) :

| Quadrant | % Effort | FrÃ©quence |
|----------|----------|-----------|
| Q1 | 40-50% | Continue (CI) |
| Q2 | 20-30% | Chaque story/feature |
| Q3 | 10-20% | Chaque sprint/release |
| Q4 | 10-20% | PÃ©riodique (mensuel, pre-release) |

#### Critiques et limites du modÃ¨le

1. **FrontiÃ¨res floues** : Certains tests chevauchent plusieurs quadrants
2. **InterprÃ©tations variables** : DiffÃ©rents auteurs placent diffÃ©remment certains tests
3. **Pas prescriptif** : Ne dit pas QUAND ni COMBIEN tester
4. **Contexte-dÃ©pendant** : Un MVP n'a pas besoin de la mÃªme couverture qu'un produit mature

### Niveau 3 - Application Pratique

#### Contexte d'utilisation optimal

- **Planning de sprint** : S'assurer qu'on couvre tous les quadrants
- **Audit de maturitÃ©** : Identifier les quadrants nÃ©gligÃ©s
- **Communication** : Expliquer la stratÃ©gie de test aux stakeholders
- **Ã‰quilibrage Ã©quipe** : RÃ©partir les compÃ©tences sur les 4 quadrants

#### Anti-patterns

| Anti-pattern | ProblÃ¨me | Solution |
|--------------|----------|----------|
| Q1 seulement | Pas de validation mÃ©tier ni NFR | Ã‰quilibrer les 4 quadrants |
| Q3 nÃ©gligÃ© | Pas d'exploratory testing | Allouer temps d'exploration |
| Q4 en dernier | Perf/Security dÃ©couvertes trop tard | Shift-left Q4 |
| Quadrants en silos | QA fait Q2-Q3, Dev fait Q1 | Cross-fonctionnel |

#### Exemple concret : Mapping des tests d'un sprint e-commerce

**Sprint Goal** : "Permettre le paiement en 3x sans frais via Alma"

| Quadrant | Tests planifiÃ©s |
|----------|-----------------|
| **Q1** | Unit tests: AlmaPaymentService, InstallmentCalculator |
| **Q1** | Integration test: Alma API mock |
| **Q2** | E2E: Parcours paiement 3x complet |
| **Q2** | Story tests: Ã‰ligibilitÃ© 3x (montant min/max, pays) |
| **Q3** | Exploratory: Edge cases, interruptions, erreurs |
| **Q3** | UAT: Validation par Ã©quipe finance |
| **Q4** | Performance: Impact sur le checkout time |
| **Q4** | Security: DonnÃ©es bancaires, webhook signature |

---

## 2.7 Exploratory Testing

### Niveau 1 - Vulgarisation

**Exploratory Testing** est une approche de test oÃ¹ le testeur **apprend, conÃ§oit et exÃ©cute simultanÃ©ment**, sans script prÃ©dÃ©fini. C'est comme explorer une ville inconnue sans GPS : on dÃ©couvre, on s'adapte, on suit son intuition.

**Analogie** : Un script de test, c'est suivre un guide touristique mot Ã  mot. L'exploratory testing, c'est flÃ¢ner librement en restant attentif aux dÃ©tails intÃ©ressants.

**Pourquoi c'est important** : Les tests scriptÃ©s ne trouvent que ce qu'on a prÃ©vu de chercher. L'exploration trouve l'inattendu.

### Niveau 2 - Approfondissement Expert

#### DÃ©finition (Cem Kaner, 1983)

> "Exploratory testing is a style of software testing that emphasizes the personal freedom and responsibility of the individual tester to continually optimize the quality of their work by treating test-related learning, test design, test execution, and test result interpretation as mutually supportive activities that run in parallel throughout the project."

**Traduction simplifiÃ©e** : Apprendre, concevoir et exÃ©cuter des tests en parallÃ¨le, de faÃ§on continue et adaptative.

#### Origine et figures clÃ©s

- **1983** : Cem Kaner utilise le terme pour la premiÃ¨re fois
- **1990s** : James Bach dÃ©veloppe "Session-Based Test Management" (SBTM)
- **2001** : Bach et Kaner formalisent l'approche dans "Lessons Learned in Software Testing"
- **2000s-2010s** : IntÃ©gration dans l'Agile, reconnaissance par ISTQB
- **2020s** : Reste essentiel malgrÃ© l'automatisation croissante

#### Exploratory Testing â‰  Ad-hoc Testing

| Aspect | Exploratory Testing | Ad-hoc Testing |
|--------|---------------------|----------------|
| Structure | StructurÃ© (charters, sessions) | Non structurÃ© |
| Documentation | Notes, rapports de session | Aucune ou minimale |
| Objectif | Clair (charter) | Vague ou inexistant |
| Apprentissage | Intentionnel | Accidentel |
| ReproductibilitÃ© | Session documentÃ©e | Difficile |

#### Session-Based Test Management (SBTM)

**Structure d'une session** :
- **Charter** : Mission de la session (quoi explorer, pourquoi)
- **Time-box** : DurÃ©e fixe (60-120 minutes typiquement)
- **Session Notes** : Log continu des dÃ©couvertes
- **Debriefing** : RÃ©sumÃ© post-session

**Format de Charter** :
```
Explore [cible]
With [ressources/outils]
To discover [information recherchÃ©e]
```

**Exemple** :
```
Explore le tunnel de paiement
With un compte client existant et diffÃ©rents moyens de paiement
To discover des problÃ¨mes d'UX et des edge cases non couverts
```

#### MÃ©triques SBTM

- **Session Duration** : Temps rÃ©el de la session
- **Charter vs Opportunity** : % temps sur le charter vs exploration opportuniste
- **Bug Clusters** : Zones oÃ¹ les bugs se concentrent
- **Test Coverage** : FonctionnalitÃ©s explorÃ©es
- **Issues Found** : Bugs, questions, risques identifiÃ©s

#### Heuristiques d'exploration

**SFDPOT** (San Francisco Depot) - James Bach :
- **S**tructure : Qu'est-ce qui compose le produit ?
- **F**unction : Que fait le produit ?
- **D**ata : Quelles donnÃ©es le produit traite-t-il ?
- **P**latform : Sur quoi tourne le produit ?
- **O**perations : Comment le produit sera-t-il utilisÃ© ?
- **T**ime : Comment le produit se comporte-t-il dans le temps ?

**Autres heuristiques** :
- **CRUSSPIC STMPL** : Capability, Reliability, Usability, Security, Scalability, Performance, Installability, Compatibility, Supportability, Testability, Maintainability, Portability, Localizability
- **Boundaries** : Tester aux limites (min, max, 0, nÃ©gatif, null)
- **RCRCRC** : Recent, Core, Risky, Configuration, Repaired, Chronic

#### CompÃ©tences du testeur explorateur

1. **CuriositÃ©** : Questionner tout, rien n'est Ã©vident
2. **Observation** : Remarquer les dÃ©tails anormaux
3. **ModÃ©lisation** : Construire une carte mentale du systÃ¨me
4. **Critique** : Challenger les hypothÃ¨ses
5. **Communication** : Documenter clairement les dÃ©couvertes
6. **Connaissance domaine** : Comprendre le mÃ©tier testÃ©

#### Place dans la stratÃ©gie de test moderne

L'exploratory testing est **complÃ©mentaire** de l'automatisation :
- Automatisation : VÃ©rifie ce qu'on sait (rÃ©gression, validation)
- Exploration : DÃ©couvre ce qu'on ne sait pas (nouveaux bugs, usability issues)

### Niveau 3 - Application Pratique

#### Contexte d'utilisation optimal

- **Nouvelles fonctionnalitÃ©s** : Avant l'automatisation
- **Domaine complexe** : RÃ¨gles mÃ©tier nombreuses
- **AprÃ¨s bugs critiques** : Explorer la zone suspecte
- **Pre-release** : Validation humaine finale
- **MVP** : Feedback rapide, peu de specs

#### Anti-patterns

| Anti-pattern | ProblÃ¨me | Solution |
|--------------|----------|----------|
| "On explore quand on a le temps" | Jamais fait | Time-box dÃ©diÃ© |
| Exploration sans notes | Pas reproductible | Session notes obligatoires |
| MÃªme testeur, mÃªme zone | Å’illÃ¨res | Rotation, pair testing |
| Confondre avec ad-hoc | Pas de structure | Charters et debriefs |

#### Exemple concret : Session Report e-commerce

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            SESSION DE TEST EXPLORATOIRE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Date: 2025-01-15
Testeur: Marie D.
DurÃ©e planifiÃ©e: 90 min
DurÃ©e rÃ©elle: 105 min

CHARTER:
Explore le processus de retour produit
With un compte client ayant des commandes rÃ©centes (< 30j)
To discover des problÃ¨mes d'UX et des cas limites

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
NOTES DE SESSION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

[00:00] Connexion compte test, 3 commandes rÃ©centes OK
[00:05] Parcours standard retour : RAS, fluide
[00:15] Test retour partiel (2 articles sur 3) : OK
[00:25] BUG #1 : Si je sÃ©lectionne "Autre" comme motif,
        le champ texte n'est pas obligatoire mais le message
        d'erreur dit "Veuillez prÃ©ciser" sans indiquer oÃ¹
        â†’ Screenshot bug-return-motif.png

[00:35] Test retour produit personnalisÃ© : Message OK
        "Ce produit ne peut Ãªtre retournÃ©", mais bouton
        "Demander retour" reste cliquable (disabled visuellement
        mais pas fonctionnellement)
        â†’ BUG #2 : Accessibility issue + confusing UX

[00:50] Test retour aprÃ¨s 28 jours (limite = 30j) : OK
[00:55] Test retour aprÃ¨s 31 jours : RefusÃ© OK
[01:00] Opportunity: je remarque que l'email de confirmation
        retour n'a pas le numÃ©ro de suivi Colissimo
        â†’ SUGGESTION #1 : Ajouter tracking number Ã  l'email

[01:15] Test 2 retours simultanÃ©s sur mÃªme commande : OK
[01:30] Test retour avec carte cadeau comme remboursement :
        Long loading (8+ secondes), succÃ¨s final
        â†’ PERFORMANCE #1 : Slow response gift card refund

[01:45] Debriefing

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RÃ‰SUMÃ‰
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Bugs trouvÃ©s: 2 (1 UX, 1 A11y)
Suggestions: 1
Performance issues: 1
Questions ouvertes: 0

Coverage:
- Retour standard âœ…
- Retour partiel âœ…
- Retour refusÃ© (dÃ©lai) âœ…
- Retour produit exclu âœ…
- Retours multiples âœ…
- Modes remboursement (CB, carte cadeau) âœ…

Non explorÃ© (pour prochaine session):
- Retour en point relais (vs domicile)
- Retour avec promo appliquÃ©e
- Retour cross-border

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

# Section 3 : Tests Fonctionnels

---

## 3.1 Unit Testing

### Niveau 1 - Vulgarisation

Un **test unitaire** vÃ©rifie le bon fonctionnement d'une **petite partie isolÃ©e du code**, gÃ©nÃ©ralement une fonction ou une mÃ©thode. C'est comme tester chaque ingrÃ©dient sÃ©parÃ©ment avant de prÃ©parer un plat.

**Analogie** : Avant de construire une maison, on teste individuellement chaque brique, chaque vis, chaque planche. Si une brique est dÃ©fectueuse, on le sait immÃ©diatement sans avoir Ã  dÃ©molir le mur.

**Pourquoi c'est important** : Les tests unitaires sont les plus rapides, les moins chers, et permettent de dÃ©tecter les bugs au plus tÃ´t (Shift-Left). Ils constituent la base de la pyramide des tests.

### Niveau 2 - Approfondissement Expert

#### DÃ©finition technique (ISTQB)

> "Component testing (also known as unit testing): Testing of individual hardware or software components."

**CaractÃ©ristiques clÃ©s** :
- IsolÃ© : Teste une unitÃ© sans ses dÃ©pendances rÃ©elles
- Rapide : Millisecondes d'exÃ©cution
- DÃ©terministe : MÃªme input â†’ mÃªme output
- AutomatisÃ© : IntÃ©grÃ© au CI/CD
- Ã‰crit par les dÃ©veloppeurs : Proche du code

#### Structure d'un test unitaire : AAA Pattern

```
ARRANGE  â†’ PrÃ©parer les donnÃ©es et conditions initiales
ACT      â†’ ExÃ©cuter l'action Ã  tester
ASSERT   â†’ VÃ©rifier le rÃ©sultat
```

**Exemple JavaScript (Jest)** :
```javascript
describe('CartService', () => {
  test('should calculate total with tax', () => {
    // Arrange
    const cart = new CartService();
    cart.addItem({ price: 100, quantity: 2 });

    // Act
    const total = cart.getTotalWithTax(0.20); // 20% TVA

    // Assert
    expect(total).toBe(240); // 200 + 40 TVA
  });
});
```

#### Frameworks par langage

| Langage | Frameworks populaires | Notes |
|---------|----------------------|-------|
| JavaScript/TypeScript | Jest, Vitest, Mocha | Jest = standard React |
| Python | pytest, unittest | pytest = prÃ©fÃ©rÃ© |
| Java | JUnit 5, TestNG | JUnit = standard |
| C# | NUnit, xUnit, MSTest | xUnit = moderne |
| PHP | PHPUnit, Pest | Pest = moderne, Ã©lÃ©gant |
| Go | testing (built-in), testify | testing = standard |
| Ruby | RSpec, Minitest | RSpec = BDD style |
| Rust | built-in (#[test]) | IntÃ©grÃ© au langage |

#### Isolation : Mocking, Stubbing, Faking

| Technique | Description | Cas d'usage |
|-----------|-------------|-------------|
| **Mock** | Objet qui vÃ©rifie les interactions (appels, paramÃ¨tres) | VÃ©rifier qu'une mÃ©thode a Ã©tÃ© appelÃ©e |
| **Stub** | Objet qui retourne des rÃ©ponses prÃ©dÃ©finies | Simuler une rÃ©ponse d'API |
| **Fake** | ImplÃ©mentation simplifiÃ©e fonctionnelle | Base de donnÃ©es en mÃ©moire |
| **Spy** | Objet rÃ©el avec suivi des appels | Observer sans modifier |

**Exemple avec Mock (Jest)** :
```javascript
test('should call payment gateway', async () => {
  // Mock du service externe
  const mockGateway = {
    processPayment: jest.fn().mockResolvedValue({ success: true })
  };

  const orderService = new OrderService(mockGateway);
  await orderService.checkout(order);

  // VÃ©rifie que le mock a Ã©tÃ© appelÃ© avec les bons params
  expect(mockGateway.processPayment).toHaveBeenCalledWith(
    expect.objectContaining({ amount: 100 })
  );
});
```

#### Code Coverage Metrics (rappel)

| MÃ©trique | Description | Seuil recommandÃ© |
|----------|-------------|------------------|
| Line Coverage | Lignes exÃ©cutÃ©es | 80% minimum |
| Branch Coverage | Branches testÃ©es | 70% minimum |
| Function Coverage | Fonctions appelÃ©es | 90% minimum |

#### Principes FIRST pour bons tests unitaires

- **F**ast : Rapide (ms, pas secondes)
- **I**ndependent : Pas de dÃ©pendance entre tests
- **R**epeatable : MÃªme rÃ©sultat Ã  chaque exÃ©cution
- **S**elf-validating : Pass/Fail automatique
- **T**imely : Ã‰crits au bon moment (idÃ©alement avant le code - TDD)

#### Test-Driven Development (TDD)

**Cycle Red-Green-Refactor** :
1. **RED** : Ã‰crire un test qui Ã©choue
2. **GREEN** : Ã‰crire le code minimal pour passer le test
3. **REFACTOR** : AmÃ©liorer le code en gardant les tests verts

**Avantages** : Design Ã©mergent, coverage naturellement Ã©levÃ©, documentation vivante

### Niveau 3 - Application Pratique

#### Contexte d'utilisation optimal

- **Logique mÃ©tier pure** : Calculs, validations, transformations
- **Fonctions utilitaires** : Helpers, formatters
- **Algorithmes** : Sorting, filtering, parsing
- **Edge cases** : Valeurs limites, cas d'erreur

#### Ce qu'il ne faut PAS tester unitairement

- Appels rÃ©seau rÃ©els (utiliser mocks)
- Base de donnÃ©es rÃ©elle (utiliser fakes)
- UI (tests d'intÃ©gration ou E2E)
- Code trivial (getters/setters simples)

#### Anti-patterns

| Anti-pattern | ProblÃ¨me | Solution |
|--------------|----------|----------|
| Tests trop couplÃ©s au code | Fragiles, cassent au refactoring | Tester le comportement, pas l'implÃ©mentation |
| Tests sans assertion | Faux positifs | Toujours une assertion significative |
| Mocks excessifs | Tests ne testent plus rien de rÃ©el | Limiter les mocks aux frontiÃ¨res |
| Tests non maintenus | CommentÃ©s ou ignorÃ©s | Traiter comme code de production |
| Nommage flou | Impossible de comprendre l'intent | Noms descriptifs : `should_return_error_when_email_invalid` |

#### Exemple concret : Tests unitaires pour calcul panier e-commerce

```javascript
// CartCalculator.test.js
describe('CartCalculator', () => {
  let calculator;

  beforeEach(() => {
    calculator = new CartCalculator();
  });

  describe('calculateSubtotal', () => {
    test('should return 0 for empty cart', () => {
      expect(calculator.calculateSubtotal([])).toBe(0);
    });

    test('should sum prices Ã— quantities', () => {
      const items = [
        { price: 10, quantity: 2 },
        { price: 25, quantity: 1 }
      ];
      expect(calculator.calculateSubtotal(items)).toBe(45);
    });

    test('should handle decimal prices', () => {
      const items = [{ price: 19.99, quantity: 3 }];
      expect(calculator.calculateSubtotal(items)).toBeCloseTo(59.97, 2);
    });
  });

  describe('applyDiscount', () => {
    test('should apply percentage discount', () => {
      const result = calculator.applyDiscount(100, { type: 'percent', value: 10 });
      expect(result).toBe(90);
    });

    test('should apply fixed discount', () => {
      const result = calculator.applyDiscount(100, { type: 'fixed', value: 15 });
      expect(result).toBe(85);
    });

    test('should not go below zero', () => {
      const result = calculator.applyDiscount(10, { type: 'fixed', value: 50 });
      expect(result).toBe(0);
    });

    test('should throw for unknown discount type', () => {
      expect(() => {
        calculator.applyDiscount(100, { type: 'unknown', value: 10 });
      }).toThrow('Unknown discount type');
    });
  });

  describe('calculateShipping', () => {
    test('should be free above threshold', () => {
      const result = calculator.calculateShipping(100, { freeAbove: 50 });
      expect(result).toBe(0);
    });

    test('should apply flat rate below threshold', () => {
      const result = calculator.calculateShipping(30, { freeAbove: 50, flatRate: 5.99 });
      expect(result).toBe(5.99);
    });
  });
});
```

---

## 3.2 Integration Testing

### Niveau 1 - Vulgarisation

Les **tests d'intÃ©gration** vÃ©rifient que **plusieurs composants fonctionnent correctement ensemble**. Si les tests unitaires vÃ©rifient chaque ingrÃ©dient, les tests d'intÃ©gration vÃ©rifient que les ingrÃ©dients se combinent bien dans la recette.

**Analogie** : Vous avez testÃ© le moteur et les roues sÃ©parÃ©ment. Le test d'intÃ©gration vÃ©rifie que le moteur fait bien tourner les roues quand on appuie sur l'accÃ©lÃ©rateur.

**Pourquoi c'est important** : Des composants qui fonctionnent parfaitement individuellement peuvent Ã©chouer ensemble (interfaces incompatibles, timing, Ã©tat partagÃ©).

### Niveau 2 - Approfondissement Expert

#### DÃ©finition technique (ISTQB)

> "Integration testing: Testing performed to expose defects in the interfaces and in the interactions between integrated components or systems."

#### StratÃ©gies d'intÃ©gration

| StratÃ©gie | Description | Avantages | InconvÃ©nients |
|-----------|-------------|-----------|---------------|
| **Big Bang** | IntÃ©grer tout en une fois | Simple si peu de composants | Debugging difficile |
| **Top-Down** | Commencer par les modules de haut niveau | Test prÃ©coce des flux principaux | Besoin de stubs pour modules bas |
| **Bottom-Up** | Commencer par les modules de bas niveau | Pas de stubs nÃ©cessaires | Flux principaux testÃ©s tard |
| **Sandwich** | Combinaison top-down et bottom-up | Ã‰quilibrÃ© | Plus complexe Ã  orchestrer |
| **Incremental** | Ajouter un composant Ã  la fois | Isolation des problÃ¨mes | Plus de cycles de test |

#### Niveaux d'intÃ©gration

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         INTEGRATION TESTS                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Component Integration                           â”‚   â”‚
â”‚  â”‚  (Module A + Module B dans mÃªme service)         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                       â†“                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Service Integration                             â”‚   â”‚
â”‚  â”‚  (Service A â†’ API â†’ Service B)                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                       â†“                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  System Integration                              â”‚   â”‚
â”‚  â”‚  (App + DB + Cache + External APIs)              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Contract Testing

Technique moderne pour l'intÃ©gration de microservices.

**Principe** : Le consommateur d'une API dÃ©finit un "contrat" (ce qu'il attend), le fournisseur s'engage Ã  le respecter.

**Outils** :
- **Pact** : Standard de facto pour contract testing
- **Spring Cloud Contract** : Ã‰cosystÃ¨me Spring

**Avantage** : Pas besoin de dÃ©ployer tous les services ensemble pour tester l'intÃ©gration.

#### Test Doubles pour l'intÃ©gration

| Situation | Solution |
|-----------|----------|
| Service externe (paiement, email) | Mock server (WireMock, MSW) |
| Base de donnÃ©es | Test containers (Docker) |
| Message queue | Embedded broker ou mock |
| Cache | In-memory (embedded Redis) |

#### Outils populaires

| CatÃ©gorie | Outils |
|-----------|--------|
| Mock Servers | WireMock, MockServer, MSW (Mock Service Worker) |
| Test Containers | Testcontainers (Java, Node, .NET, Go) |
| API Testing | Postman, REST Assured, Supertest |
| Contract Testing | Pact, Spring Cloud Contract |

### Niveau 3 - Application Pratique

#### Contexte d'utilisation optimal

- **Points d'intÃ©gration** : API calls, DB queries, file I/O
- **Workflows multi-composants** : Commande â†’ Paiement â†’ Stock
- **Changements d'interface** : Nouvelle version d'API

#### Anti-patterns

| Anti-pattern | ProblÃ¨me | Solution |
|--------------|----------|----------|
| Tester trop de composants | Lent, fragile | Limiter le scope |
| DÃ©pendance Ã  l'environnement | Non reproductible | Containers, mocks |
| DonnÃ©es de test partagÃ©es | Tests s'influencent | Isolation des donnÃ©es |
| Tests d'intÃ©gration comme unitaires | Confusion | SÃ©parer clairement |

#### Exemple concret : Test d'intÃ©gration API + DB pour e-commerce

```javascript
// order.integration.test.js
const request = require('supertest');
const { setupTestDb, teardownTestDb, seedProducts } = require('./testUtils');
const app = require('../app');

describe('Order API Integration', () => {
  beforeAll(async () => {
    await setupTestDb();
    await seedProducts([
      { id: 'PROD-001', name: 'Widget', price: 29.99, stock: 100 }
    ]);
  });

  afterAll(async () => {
    await teardownTestDb();
  });

  describe('POST /api/orders', () => {
    test('should create order and decrement stock', async () => {
      const orderData = {
        customerId: 'CUST-001',
        items: [{ productId: 'PROD-001', quantity: 2 }]
      };

      // Act
      const response = await request(app)
        .post('/api/orders')
        .send(orderData)
        .expect(201);

      // Assert - Order created
      expect(response.body).toMatchObject({
        id: expect.stringMatching(/^ORD-/),
        status: 'pending',
        total: 59.98
      });

      // Assert - Stock decremented (vÃ©rifie l'intÃ©gration DB)
      const stockResponse = await request(app)
        .get('/api/products/PROD-001')
        .expect(200);

      expect(stockResponse.body.stock).toBe(98);
    });

    test('should reject order if insufficient stock', async () => {
      const orderData = {
        customerId: 'CUST-001',
        items: [{ productId: 'PROD-001', quantity: 9999 }]
      };

      const response = await request(app)
        .post('/api/orders')
        .send(orderData)
        .expect(400);

      expect(response.body.error).toContain('Insufficient stock');
    });
  });
});
```

---

## 3.3 End-to-End Testing (E2E)

### Niveau 1 - Vulgarisation

Les **tests End-to-End (E2E)** vÃ©rifient un **parcours utilisateur complet**, du dÃ©but Ã  la fin, comme le ferait un vrai utilisateur. C'est le test le plus proche de la rÃ©alitÃ©.

**Analogie** : Au lieu de tester les ingrÃ©dients ou la cuisson, vous faites goÃ»ter le plat complet Ã  quelqu'un et observez sa rÃ©action du premier regard jusqu'Ã  la derniÃ¨re bouchÃ©e.

**Pourquoi c'est important** : Les tests unitaires et d'intÃ©gration peuvent tous passer, mais le parcours utilisateur complet peut Ã©chouer (problÃ¨me de configuration, timing, UI).

### Niveau 2 - Approfondissement Expert

#### DÃ©finition technique

> "End-to-end testing is a methodology that validates the entire software application from start to finish, including all integrated components, interfaces, and external dependencies."

#### Position dans la pyramide des tests

```
        â–³
       /  \          E2E Tests (peu, lents, chers)
      /â”€â”€â”€â”€\
     /      \        Integration Tests (quelques-uns)
    /â”€â”€â”€â”€â”€â”€â”€â”€\
   /          \      Unit Tests (beaucoup, rapides)
  /â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\
```

**Proportion recommandÃ©e** (indicative) :
- Unit : 70%
- Integration : 20%
- E2E : 10%

#### Frameworks E2E populaires (2024-2025)

| Framework | Langage | Points forts | Points faibles |
|-----------|---------|--------------|----------------|
| **Playwright** | JS/TS, Python, .NET, Java | Multi-browser, rapide, moderne | Relativement nouveau |
| **Cypress** | JavaScript | DX excellente, debugging visuel | Chrome-centric historiquement |
| **Selenium** | Multi-langages | Standard historique, large communautÃ© | Plus lent, moins moderne |
| **WebdriverIO** | JavaScript | Flexible, protocoles multiples | Courbe d'apprentissage |
| **Puppeteer** | JavaScript | Control fin de Chrome | Chrome uniquement |

**Tendance 2024-2025** : Playwright a pris le leadership grÃ¢ce Ã  sa rapiditÃ© et son support multi-navigateur natif.

#### Anatomie d'un test E2E

```javascript
// checkout.e2e.spec.js (Playwright)
test.describe('Checkout Flow', () => {
  test('should complete purchase as guest user', async ({ page }) => {
    // Navigation
    await page.goto('/products/widget-pro');

    // Add to cart
    await page.click('[data-testid="add-to-cart"]');
    await page.waitForSelector('[data-testid="cart-count"]');
    expect(await page.textContent('[data-testid="cart-count"]')).toBe('1');

    // Go to cart
    await page.click('[data-testid="cart-icon"]');
    await expect(page).toHaveURL('/cart');

    // Proceed to checkout
    await page.click('[data-testid="checkout-button"]');

    // Fill shipping info
    await page.fill('[name="email"]', 'test@example.com');
    await page.fill('[name="firstName"]', 'John');
    await page.fill('[name="lastName"]', 'Doe');
    await page.fill('[name="address"]', '123 Test Street');
    await page.fill('[name="city"]', 'Paris');
    await page.fill('[name="postalCode"]', '75001');

    // Select shipping method
    await page.click('[data-testid="shipping-standard"]');

    // Fill payment (sandbox)
    await page.fill('[name="cardNumber"]', '4242424242424242');
    await page.fill('[name="expiry"]', '12/28');
    await page.fill('[name="cvc"]', '123');

    // Place order
    await page.click('[data-testid="place-order"]');

    // Verify success
    await expect(page).toHaveURL(/\/order-confirmation/);
    await expect(page.locator('[data-testid="order-number"]')).toBeVisible();
  });
});
```

#### StratÃ©gies pour E2E fiables

1. **Data-testid attributes** : Ne pas dÃ©pendre des classes CSS ou du texte
2. **Waits explicites** : Attendre les Ã©lÃ©ments, pas des dÃ©lais fixes
3. **Isolation des donnÃ©es** : Chaque test crÃ©e ses propres donnÃ©es
4. **Retry mechanism** : RÃ©essayer automatiquement les tests flaky
5. **ParallÃ©lisation** : ExÃ©cuter plusieurs tests simultanÃ©ment
6. **Visual comparison** : Capturer des screenshots pour rÃ©gression visuelle

#### CoÃ»t et maintenance des E2E

| Aspect | RÃ©alitÃ© |
|--------|---------|
| Temps d'Ã©criture | 5-10x plus long qu'un test unitaire |
| Temps d'exÃ©cution | Secondes Ã  minutes par test |
| Flakiness | Plus frÃ©quent (rÃ©seau, timing, UI) |
| Maintenance | CoÃ»teuse si UI change souvent |
| Debugging | Plus difficile (beaucoup de couches) |

**ROI** : Prioriser les parcours critiques business (checkout, login, core features).

### Niveau 3 - Application Pratique

#### Contexte d'utilisation optimal

- **Parcours critiques** : Checkout, inscription, fonctions gÃ©nÃ©ratrices de revenus
- **Smoke tests** : VÃ©rification rapide post-dÃ©ploiement
- **Regression majeure** : AprÃ¨s refactoring important
- **Validation cross-browser** : VÃ©rifier sur diffÃ©rents navigateurs

#### Ce qu'il ne faut PAS tester en E2E

- Tous les edge cases (â†’ tests unitaires)
- Tous les messages d'erreur (â†’ tests unitaires)
- Performance (â†’ tests de charge dÃ©diÃ©s)
- Chaque combinaison possible (explosion combinatoire)

#### Anti-patterns

| Anti-pattern | ProblÃ¨me | Solution |
|--------------|----------|----------|
| Trop de tests E2E | Lent, fragile, coÃ»teux | Limiter aux parcours critiques |
| `sleep(5000)` | Fragile, lent | Waits intelligents |
| SÃ©lecteurs CSS fragiles | Cassent au redesign | data-testid |
| Tests interdÃ©pendants | Un Ã©chec cascade | Tests indÃ©pendants |
| Ignorer les flaky tests | Faux sentiment de sÃ©curitÃ© | Fix ou remove |

#### Exemple concret : Suite E2E pour e-commerce (Playwright)

```javascript
// e2e/critical-paths.spec.js

import { test, expect } from '@playwright/test';

test.describe('Critical E-commerce Paths', () => {

  test.describe('Guest Checkout', () => {
    test('complete purchase with credit card', async ({ page }) => {
      // ... (voir exemple ci-dessus)
    });

    test('complete purchase with PayPal', async ({ page }) => {
      // Parcours PayPal sandbox
    });
  });

  test.describe('Registered User', () => {
    test.beforeEach(async ({ page }) => {
      // Login une seule fois
      await page.goto('/login');
      await page.fill('[name="email"]', 'test@example.com');
      await page.fill('[name="password"]', 'testpassword');
      await page.click('[type="submit"]');
      await expect(page).toHaveURL('/account');
    });

    test('reorder from order history', async ({ page }) => {
      await page.goto('/account/orders');
      await page.click('[data-testid="reorder-btn"]');
      await expect(page).toHaveURL('/cart');
      await expect(page.locator('[data-testid="cart-item"]')).toHaveCount(2);
    });

    test('save address for future orders', async ({ page }) => {
      // ...
    });
  });

  test.describe('Search & Browse', () => {
    test('search product and filter results', async ({ page }) => {
      await page.goto('/');
      await page.fill('[data-testid="search-input"]', 'chaussures');
      await page.press('[data-testid="search-input"]', 'Enter');

      await expect(page.locator('[data-testid="product-card"]')).toHaveCount.greaterThan(0);

      // Filter by price
      await page.click('[data-testid="filter-price-50-100"]');
      await expect(page.locator('[data-testid="active-filter"]')).toContainText('50â‚¬ - 100â‚¬');
    });
  });
});
```

---

## 3.4 System Testing

### Niveau 1 - Vulgarisation

Le **System Testing** teste le **systÃ¨me complet** comme un tout, en conditions proches de la production. C'est la vÃ©rification finale avant de livrer.

**Analogie** : AprÃ¨s avoir assemblÃ© tous les composants d'une voiture, on la teste sur piste dans des conditions rÃ©elles (routes, mÃ©tÃ©o) avant de la vendre.

**Pourquoi c'est important** : Valider que tous les composants intÃ©grÃ©s forment un systÃ¨me fonctionnel qui rÃ©pond aux spÃ©cifications initiales.

### Niveau 2 - Approfondissement Expert

#### DÃ©finition technique (ISTQB)

> "System testing: Testing an integrated system to verify that it meets specified requirements."

#### DiffÃ©rence avec les autres niveaux

| Niveau | Scope | Environnement | Responsable |
|--------|-------|---------------|-------------|
| Unit | Fonction/Classe | Dev local | DÃ©veloppeur |
| Integration | Composants combinÃ©s | Test/Dev | Dev/QA |
| System | SystÃ¨me complet | Proche prod | QA |
| Acceptance | SystÃ¨me + processus mÃ©tier | Prod-like | Business/Users |

#### Types de tests systÃ¨me

1. **Functional System Testing** : VÃ©rifier les fonctionnalitÃ©s
2. **Non-Functional System Testing** : Performance, sÃ©curitÃ©, etc.
3. **Regression System Testing** : Pas de rÃ©gression aprÃ¨s changements
4. **Recovery Testing** : Comportement aprÃ¨s crash/failure
5. **Installation Testing** : Processus d'installation/mise Ã  jour

#### Environnement de System Testing

**CaractÃ©ristiques** :
- Configuration similaire Ã  la production
- DonnÃ©es reprÃ©sentatives (anonymisÃ©es si nÃ©cessaire)
- IntÃ©grations rÃ©elles (ou mocks rÃ©alistes)
- Isolation des autres environnements

### Niveau 3 - Application Pratique

#### Contexte e-commerce

**Checklist System Testing** :
- [ ] Parcours complet guest + registered
- [ ] Tous les moyens de paiement
- [ ] Gestion du stock (rÃ©servation, libÃ©ration)
- [ ] Emails transactionnels
- [ ] IntÃ©gration logistique (carrier APIs)
- [ ] Back-office : gestion commandes
- [ ] Multi-device (desktop, mobile, tablet)
- [ ] Multi-browser

---

## 3.5 Regression Testing

### Niveau 1 - Vulgarisation

Les **tests de rÃ©gression** vÃ©rifient que les **nouvelles modifications n'ont pas cassÃ© ce qui fonctionnait**. C'est comme vÃ©rifier que rÃ©parer une fuite n'a pas crÃ©Ã© une autre fuite ailleurs.

**Analogie** : AprÃ¨s avoir ajoutÃ© une extension Ã  votre maison, vous vÃ©rifiez que le reste de la maison fonctionne toujours (Ã©lectricitÃ©, plomberie, chauffage).

**Pourquoi c'est important** : Les effets de bord sont frÃ©quents. Un changement innocent peut casser une fonctionnalitÃ© distante.

### Niveau 2 - Approfondissement Expert

#### DÃ©finition technique (ISTQB)

> "Regression testing: Testing of a previously tested program following modification to ensure that defects have not been introduced or uncovered in unchanged areas of the software."

#### Quand exÃ©cuter les tests de rÃ©gression

| Trigger | Scope de rÃ©gression |
|---------|---------------------|
| Chaque commit | Tests unitaires (CI) |
| Chaque PR/MR | Unit + Integration |
| Chaque sprint | Full regression |
| Pre-release | Full regression + exploratory |
| Hotfix | CiblÃ© + smoke test |

#### StratÃ©gies de sÃ©lection des tests

1. **Retest All** : Tout exÃ©cuter (complet mais lent)
2. **Risk-Based Selection** : Prioriser par risque
3. **Change-Based Selection** : Tests impactÃ©s par les changements
4. **Combination** : Analyse d'impact + tests critiques toujours

#### Automation pour la rÃ©gression

La rÃ©gression est le **candidat idÃ©al pour l'automatisation** car :
- Tests rÃ©pÃ©titifs (mÃªme chose Ã  chaque release)
- Volume important
- Besoin de rapiditÃ© pour feedback CI/CD
- CoÃ»t manuel prohibitif Ã  terme

#### Outils d'analyse d'impact

- **Coverage-based** : Quels tests couvrent le code modifiÃ© ?
- **Git history** : Quels fichiers/modules changent ensemble ?
- **Dependency analysis** : Quels modules dÃ©pendent du code modifiÃ© ?

### Niveau 3 - Application Pratique

#### StratÃ©gie de rÃ©gression pour e-commerce

```
REGRESSION STRATEGY

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CONTINUOUS (chaque commit)                              â”‚
â”‚ - Unit tests (< 5 min)                                  â”‚
â”‚ - Lint + Static analysis                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PR/MR (avant merge)                                     â”‚
â”‚ - Unit + Integration (< 15 min)                         â”‚
â”‚ - E2E smoke (checkout, login)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ NIGHTLY (chaque nuit)                                   â”‚
â”‚ - Full E2E suite                                        â”‚
â”‚ - Cross-browser tests                                   â”‚
â”‚ - Visual regression                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PRE-RELEASE (avant chaque release)                      â”‚
â”‚ - Full regression all levels                            â”‚
â”‚ - Performance baseline                                  â”‚
â”‚ - Security scan                                         â”‚
â”‚ - Exploratory testing                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3.6 Smoke Testing vs Sanity Testing

### Niveau 1 - Vulgarisation

**Smoke Testing** et **Sanity Testing** sont deux tests rapides pour vÃ©rifier qu'un build mÃ©rite des tests plus approfondis.

- **Smoke Test** : "Est-ce que Ã§a dÃ©marre ?" (vÃ©rification trÃ¨s basique)
- **Sanity Test** : "Est-ce que les changements rÃ©cents fonctionnent ?" (focus sur le nouveau)

**Analogie** :
- Smoke Test : Tourner la clÃ© de la voiture pour voir si le moteur dÃ©marre
- Sanity Test : AprÃ¨s avoir changÃ© les freins, vÃ©rifier que les freins fonctionnent

### Niveau 2 - Approfondissement Expert

#### Tableau comparatif

| Aspect | Smoke Testing | Sanity Testing |
|--------|---------------|----------------|
| **Autre nom** | Build Verification Test (BVT) | Surface level testing |
| **Objectif** | VÃ©rifier la stabilitÃ© du build | VÃ©rifier la rationalitÃ© des changements |
| **Scope** | Large (fonctions critiques) | Ã‰troit (zones modifiÃ©es) |
| **Profondeur** | Superficiel | Un peu plus profond |
| **Quand** | AprÃ¨s chaque build | AprÃ¨s bug fixes ou minor changes |
| **AutomatisÃ©** | Oui, toujours | Souvent manuel |
| **DurÃ©e** | < 30 minutes | < 1 heure |
| **Si Ã©chec** | Build rejetÃ© | Investigation ciblÃ©e |

#### Origine du terme "Smoke Test"

Vient de l'Ã©lectronique/plomberie : on met sous tension un circuit ou sous pression un tuyau, et on regarde si de la fumÃ©e sort. Si oui, problÃ¨me fondamental.

#### Exemples de Smoke Tests pour e-commerce

1. La page d'accueil charge
2. La recherche retourne des rÃ©sultats
3. Une fiche produit s'affiche
4. L'ajout au panier fonctionne
5. Le panier s'affiche
6. La page de login est accessible
7. Une API health check rÃ©pond 200

### Niveau 3 - Application Pratique

#### Script de Smoke Test automatisÃ©

```javascript
// smoke.spec.js
test.describe('Smoke Tests', () => {
  test('homepage loads', async ({ page }) => {
    const response = await page.goto('/');
    expect(response.status()).toBe(200);
    await expect(page.locator('header')).toBeVisible();
  });

  test('product listing loads', async ({ page }) => {
    await page.goto('/products');
    await expect(page.locator('[data-testid="product-card"]')).toHaveCount.greaterThan(0);
  });

  test('search works', async ({ page }) => {
    await page.goto('/');
    await page.fill('[data-testid="search"]', 'test');
    await page.press('[data-testid="search"]', 'Enter');
    await expect(page).toHaveURL(/search/);
  });

  test('add to cart works', async ({ page }) => {
    await page.goto('/products/sample-product');
    await page.click('[data-testid="add-to-cart"]');
    await expect(page.locator('[data-testid="cart-count"]')).not.toHaveText('0');
  });

  test('login page loads', async ({ page }) => {
    await page.goto('/login');
    await expect(page.locator('form')).toBeVisible();
  });

  test('API health check', async ({ request }) => {
    const response = await request.get('/api/health');
    expect(response.status()).toBe(200);
  });
});
```

---

## 3.7 API Testing

### Niveau 1 - Vulgarisation

Les **tests d'API** vÃ©rifient que les **interfaces de programmation** (les "portes d'entrÃ©e" pour les donnÃ©es) fonctionnent correctement, sans passer par l'interface utilisateur.

**Analogie** : Au lieu de tester une distributrice en appuyant sur les boutons, vous testez directement le mÃ©canisme interne qui reÃ§oit les commandes et dÃ©livre les produits.

**Pourquoi c'est important** : L'API est le contrat entre le frontend et le backend. Si l'API a des bugs, tout ce qui l'utilise sera impactÃ©.

### Niveau 2 - Approfondissement Expert

#### Types de tests API

| Type | Description | Exemple |
|------|-------------|---------|
| **Contract Testing** | VÃ©rifie le format des requÃªtes/rÃ©ponses | Schema validation (OpenAPI) |
| **Functional Testing** | VÃ©rifie la logique mÃ©tier | CRUD operations |
| **Performance Testing** | VÃ©rifie les temps de rÃ©ponse | Response time < 200ms |
| **Security Testing** | VÃ©rifie les vulnÃ©rabilitÃ©s | Auth bypass, injection |
| **Integration Testing** | VÃ©rifie les dÃ©pendances | API â†’ DB â†’ External service |

#### Ã‰lÃ©ments Ã  tester pour chaque endpoint

```
REQUEST
â”œâ”€â”€ HTTP Method (GET, POST, PUT, DELETE, PATCH)
â”œâ”€â”€ URL / Path parameters
â”œâ”€â”€ Query parameters
â”œâ”€â”€ Headers (Auth, Content-Type, Accept)
â”œâ”€â”€ Request body (JSON, form-data)
â””â”€â”€ Edge cases (missing fields, invalid types)

RESPONSE
â”œâ”€â”€ Status code (200, 201, 400, 401, 403, 404, 500)
â”œâ”€â”€ Response body structure
â”œâ”€â”€ Response headers
â”œâ”€â”€ Response time
â””â”€â”€ Error messages
```

#### Outils populaires

| Outil | Type | Points forts |
|-------|------|--------------|
| **Postman** | GUI + Scripting | Populaire, collections, collaboration |
| **Insomnia** | GUI | Plus lÃ©ger que Postman |
| **REST Assured** | Java library | Pour projets Java |
| **Supertest** | Node.js library | IntÃ©grÃ© aux tests Jest/Mocha |
| **pytest + requests** | Python | Simple et efficace |
| **k6** | Performance | Tests de charge API |
| **Bruno** | GUI | Open source, Git-friendly |

#### Structure d'un test API (REST Assured - Java)

```java
@Test
public void shouldCreateOrderSuccessfully() {
    given()
        .contentType(ContentType.JSON)
        .header("Authorization", "Bearer " + token)
        .body("""
            {
                "customerId": "CUST-001",
                "items": [{"productId": "PROD-001", "quantity": 2}]
            }
        """)
    .when()
        .post("/api/orders")
    .then()
        .statusCode(201)
        .body("id", matchesPattern("ORD-\\d+"))
        .body("status", equalTo("pending"))
        .body("total", equalTo(59.98f))
        .time(lessThan(2000L)); // < 2 secondes
}
```

### Niveau 3 - Application Pratique

#### Checklist API Testing e-commerce

**Endpoints critiques** :
- [ ] `POST /auth/login` - Authentication
- [ ] `POST /auth/register` - Registration
- [ ] `GET /products` - Product listing
- [ ] `GET /products/:id` - Product detail
- [ ] `POST /cart/items` - Add to cart
- [ ] `PUT /cart/items/:id` - Update quantity
- [ ] `DELETE /cart/items/:id` - Remove from cart
- [ ] `POST /orders` - Create order
- [ ] `POST /payments` - Process payment
- [ ] `GET /orders/:id` - Order status

**Cas de test par endpoint** :
1. Happy path
2. Invalid input (400)
3. Unauthorized (401)
4. Forbidden (403)
5. Not found (404)
6. Server error handling (500)
7. Rate limiting (429)
8. Pagination
9. Filtering/sorting

---

## 3.8 Database Testing

### Niveau 1 - Vulgarisation

Les **tests de base de donnÃ©es** vÃ©rifient que les donnÃ©es sont **correctement stockÃ©es, rÃ©cupÃ©rÃ©es et modifiÃ©es**. C'est s'assurer que le "coffre-fort" de votre application fonctionne parfaitement.

**Analogie** : Tester un classeur de documents : les documents sont-ils rangÃ©s au bon endroit ? Peut-on les retrouver facilement ? Sont-ils protÃ©gÃ©s contre les modifications accidentelles ?

**Pourquoi c'est important** : Les donnÃ©es sont le cÅ“ur de toute application. Une corruption ou une perte de donnÃ©es peut Ãªtre catastrophique.

### Niveau 2 - Approfondissement Expert

#### Types de tests de base de donnÃ©es

| Type | Description | Exemple |
|------|-------------|---------|
| **Schema Testing** | Structure des tables, colonnes, types | Colonnes requises prÃ©sentes |
| **Data Integrity** | Contraintes, clÃ©s Ã©trangÃ¨res | FK violations impossibles |
| **CRUD Operations** | Create, Read, Update, Delete | Insert fonctionne |
| **Transaction Testing** | AtomicitÃ©, rollback | Transaction Ã©choue = rollback |
| **Migration Testing** | Scripts de migration | Up/down fonctionnent |
| **Performance** | Index, query time | Query < 100ms |
| **Security** | AccÃ¨s, injection SQL | Pas de SQL injection |

#### Techniques de test

**1. Direct Database Testing**
```sql
-- VÃ©rifier que la contrainte fonctionne
INSERT INTO orders (customer_id, total) VALUES (NULL, 100);
-- Doit Ã©chouer : customer_id NOT NULL
```

**2. Application-Level Testing**
```javascript
test('should enforce unique email constraint', async () => {
  await User.create({ email: 'test@example.com' });

  await expect(
    User.create({ email: 'test@example.com' })
  ).rejects.toThrow(/unique constraint/i);
});
```

**3. Migration Testing**
```javascript
describe('Migration 2024_01_15_add_discount_column', () => {
  test('up: adds discount column', async () => {
    await runMigration('up');
    const columns = await getTableColumns('orders');
    expect(columns).toContain('discount');
  });

  test('down: removes discount column', async () => {
    await runMigration('down');
    const columns = await getTableColumns('orders');
    expect(columns).not.toContain('discount');
  });
});
```

#### Outils et frameworks

| CatÃ©gorie | Outils |
|-----------|--------|
| ORM Testing | Jest + Sequelize/TypeORM/Prisma |
| Raw SQL Testing | pgTAP (PostgreSQL), MyTAP (MySQL) |
| Migration Testing | Flyway, Liquibase, Knex |
| Data Generation | Faker.js, Factory Bot |
| Test Containers | Testcontainers (DB Ã©phÃ©mÃ¨res) |

### Niveau 3 - Application Pratique

#### StratÃ©gie pour e-commerce

```
DATABASE TESTING STRATEGY

1. SCHEMA VALIDATION (CI/CD)
   - Tables critiques existent
   - Colonnes required prÃ©sentes
   - Types de donnÃ©es corrects
   - Index de performance

2. CONSTRAINTS (Unit/Integration)
   - NOT NULL sur champs obligatoires
   - UNIQUE sur email, SKU, order_number
   - FOREIGN KEYS (customer â†’ orders)
   - CHECK constraints (price > 0)

3. TRANSACTIONS (Integration)
   - Order creation = atomique
   - Payment + Stock update = atomique
   - Rollback on failure

4. PERFORMANCE (Periodic)
   - Query explain plans
   - Index effectiveness
   - N+1 query detection

5. MIGRATION (Pre-deploy)
   - Up migration fonctionne
   - Down migration fonctionne
   - Data preservation
```

---

# Section 4 : Tests Non-Fonctionnels

---

## 4.1 Performance Testing

### Niveau 1 - Vulgarisation

Les **tests de performance** vÃ©rifient que l'application **rÃ©pond assez vite** et **supporte suffisamment d'utilisateurs**. C'est comme tester si une route peut supporter le trafic d'heure de pointe sans embouteillage.

**Pourquoi c'est important** : Un site e-commerce qui met 5 secondes Ã  charger perd 50% de ses visiteurs. Pendant le Black Friday, un site qui crashe perd des millions.

### Niveau 2 - Approfondissement Expert

#### Types de tests de performance

| Type | Objectif | Question posÃ©e |
|------|----------|----------------|
| **Load Testing** | Comportement sous charge normale/attendue | "Tient-on 1000 utilisateurs simultanÃ©s ?" |
| **Stress Testing** | Trouver le point de rupture | "Ã€ combien d'utilisateurs on casse ?" |
| **Spike Testing** | RÃ©action Ã  une montÃ©e soudaine | "Que se passe-t-il si le trafic x10 en 1 minute ?" |
| **Endurance/Soak Testing** | StabilitÃ© sur longue durÃ©e | "Y a-t-il des memory leaks sur 24h ?" |
| **Scalability Testing** | CapacitÃ© Ã  monter en charge | "L'ajout de serveurs amÃ©liore-t-il les perfs ?" |
| **Volume Testing** | Comportement avec gros volumes de donnÃ©es | "Que se passe-t-il avec 10M de produits ?" |

#### MÃ©triques clÃ©s

| MÃ©trique | Description | Seuil e-commerce typique |
|----------|-------------|--------------------------|
| **Response Time** | Temps de rÃ©ponse d'une requÃªte | < 200ms API, < 3s page |
| **Throughput** | RequÃªtes/seconde | DÃ©pend du contexte |
| **Error Rate** | % de requÃªtes en erreur | < 1% |
| **Concurrent Users** | Utilisateurs simultanÃ©s | Variable selon business |
| **TTFB** | Time To First Byte | < 600ms |
| **TPS** | Transactions Per Second | Variable |
| **Apdex** | Application Performance Index | > 0.9 |
| **P95/P99** | 95e/99e percentile response time | Plus reprÃ©sentatif que moyenne |

#### Outils de performance testing

| Outil | Type | Points forts |
|-------|------|--------------|
| **k6** | Load testing | Scripting JS, moderne, dev-friendly |
| **JMeter** | Load testing | Mature, extensible, GUI |
| **Gatling** | Load testing | Scala DSL, rapports Ã©lÃ©gants |
| **Locust** | Load testing | Python, distribuÃ© |
| **Artillery** | Load testing | YAML config, simple |
| **Lighthouse** | Frontend perf | IntÃ©grÃ© Chrome, Web Vitals |
| **WebPageTest** | Frontend perf | Tests rÃ©els multi-localisation |

#### Core Web Vitals (Google, 2020+)

| MÃ©trique | Description | Bon | Ã€ amÃ©liorer | Mauvais |
|----------|-------------|-----|-------------|---------|
| **LCP** | Largest Contentful Paint | < 2.5s | 2.5-4s | > 4s |
| **INP** | Interaction to Next Paint | < 200ms | 200-500ms | > 500ms |
| **CLS** | Cumulative Layout Shift | < 0.1 | 0.1-0.25 | > 0.25 |

*Note : INP a remplacÃ© FID (First Input Delay) en mars 2024*

#### Script de Load Test (k6)

```javascript
// load-test.js
import http from 'k6/http';
import { check, sleep } from 'k6';

export const options = {
  stages: [
    { duration: '2m', target: 100 },  // Ramp-up Ã  100 users
    { duration: '5m', target: 100 },  // Maintien
    { duration: '2m', target: 200 },  // MontÃ©e Ã  200
    { duration: '5m', target: 200 },  // Maintien
    { duration: '2m', target: 0 },    // Ramp-down
  ],
  thresholds: {
    http_req_duration: ['p(95)<500'], // 95% des requÃªtes < 500ms
    http_req_failed: ['rate<0.01'],   // < 1% d'erreurs
  },
};

export default function () {
  // ScÃ©nario : browse â†’ search â†’ product â†’ cart
  const baseUrl = 'https://staging.example.com';

  // Homepage
  let res = http.get(`${baseUrl}/`);
  check(res, { 'homepage status 200': (r) => r.status === 200 });

  sleep(1);

  // Search
  res = http.get(`${baseUrl}/api/products?search=shoes`);
  check(res, {
    'search status 200': (r) => r.status === 200,
    'search has results': (r) => JSON.parse(r.body).length > 0,
  });

  sleep(2);

  // Product detail
  res = http.get(`${baseUrl}/api/products/PROD-001`);
  check(res, { 'product status 200': (r) => r.status === 200 });

  sleep(1);

  // Add to cart
  res = http.post(`${baseUrl}/api/cart/items`, JSON.stringify({
    productId: 'PROD-001',
    quantity: 1
  }), { headers: { 'Content-Type': 'application/json' } });

  check(res, { 'add to cart status 201': (r) => r.status === 201 });

  sleep(3);
}
```

### Niveau 3 - Application Pratique

#### Contexte e-commerce : PrÃ©parer Black Friday

**Objectifs typiques** :
- 10x le trafic normal
- Response time < 2s sous charge
- 0 downtime
- Checkout < 30s total

**Plan de test** :
1. **Baseline** : Mesurer performance actuelle
2. **Load Test** : Trafic attendu Black Friday
3. **Stress Test** : +50% au-delÃ  des prÃ©visions
4. **Spike Test** : Simulation flash sale
5. **Soak Test** : 24h Ã  charge Ã©levÃ©e (memory leaks)

#### Anti-patterns

| Anti-pattern | ProblÃ¨me | Solution |
|--------------|----------|----------|
| Tester en dev | Infra diffÃ©rente de prod | Environnement iso-prod |
| DonnÃ©es vides | Pas rÃ©aliste | Dataset reprÃ©sentatif |
| Test ponctuel | RÃ©gressions non dÃ©tectÃ©es | Tests rÃ©guliers (CI) |
| Ignorer le frontend | Backend OK mais UX lente | Inclure Lighthouse/WebVitals |

---

## 4.2 Security Testing

### Niveau 1 - Vulgarisation

Les **tests de sÃ©curitÃ©** vÃ©rifient que l'application **protÃ¨ge les donnÃ©es et rÃ©siste aux attaques**. C'est comme tester les serrures, alarmes et coffres-forts d'une banque.

**Pourquoi c'est important** : Une faille de sÃ©curitÃ© peut entraÃ®ner vol de donnÃ©es clients, pertes financiÃ¨res, destruction de rÃ©putation, et sanctions lÃ©gales (RGPD).

### Niveau 2 - Approfondissement Expert

#### OWASP Top 10 (2021)

L'**OWASP Top 10** est la rÃ©fÃ©rence mondiale des risques de sÃ©curitÃ© web.

| # | CatÃ©gorie | Description | Exemple |
|---|-----------|-------------|---------|
| A01 | **Broken Access Control** | AccÃ¨s non autorisÃ© Ã  des ressources | AccÃ©der aux commandes d'un autre utilisateur |
| A02 | **Cryptographic Failures** | Protection insuffisante des donnÃ©es sensibles | Mots de passe stockÃ©s en clair |
| A03 | **Injection** | DonnÃ©es non fiables interprÃ©tÃ©es comme code | SQL injection, XSS |
| A04 | **Insecure Design** | Failles de conception | Absence de rate limiting |
| A05 | **Security Misconfiguration** | Mauvaise configuration | Headers de sÃ©curitÃ© manquants |
| A06 | **Vulnerable Components** | DÃ©pendances avec vulnÃ©rabilitÃ©s connues | Log4Shell |
| A07 | **Identification and Authentication Failures** | Authentification faible | Pas de protection brute-force |
| A08 | **Software and Data Integrity Failures** | Code/donnÃ©es non vÃ©rifiÃ©s | CI/CD compromise |
| A09 | **Security Logging and Monitoring Failures** | Logs insuffisants | Intrusion non dÃ©tectÃ©e |
| A10 | **Server-Side Request Forgery (SSRF)** | RequÃªtes serveur manipulÃ©es | AccÃ¨s rÃ©seau interne |

*Source : https://owasp.org/Top10/*

#### Types de tests de sÃ©curitÃ©

| Type | Description | Quand |
|------|-------------|-------|
| **SAST** | Static Application Security Testing | Pendant dÃ©veloppement (CI) |
| **DAST** | Dynamic Application Security Testing | Sur application dÃ©ployÃ©e |
| **IAST** | Interactive AST | Runtime avec instrumentation |
| **SCA** | Software Composition Analysis | Analyse des dÃ©pendances |
| **Penetration Testing** | Test d'intrusion manuel | Pre-release, pÃ©riodique |
| **Security Audit** | Revue complÃ¨te | Annuel ou aprÃ¨s incident |

#### SAST vs DAST

| Aspect | SAST | DAST |
|--------|------|------|
| **Quand** | Code source, avant exÃ©cution | Application dÃ©ployÃ©e |
| **Quoi** | Analyse statique du code | Attaques simulÃ©es |
| **Avantages** | DÃ©tection prÃ©coce, couverture code | Trouve vulnÃ©rabilitÃ©s runtime |
| **Limites** | Faux positifs, pas de contexte runtime | Couverture limitÃ©e, tardif |
| **Outils** | SonarQube, Semgrep, Snyk Code | OWASP ZAP, Burp Suite, Nuclei |

#### Outils de sÃ©curitÃ©

| CatÃ©gorie | Outils |
|-----------|--------|
| SAST | SonarQube, Semgrep, CodeQL, Snyk Code |
| DAST | OWASP ZAP, Burp Suite, Nuclei |
| SCA (Dependencies) | Snyk, Dependabot, npm audit, OWASP Dependency-Check |
| Secrets Detection | GitLeaks, TruffleHog, detect-secrets |
| Container Security | Trivy, Clair, Snyk Container |

#### OWASP Web Security Testing Guide (WSTG)

Le **WSTG v4.2** est le guide complet de test de sÃ©curitÃ© web (400+ pages).

**CatÃ©gories de tests** :
1. Information Gathering
2. Configuration and Deployment Management Testing
3. Identity Management Testing
4. Authentication Testing
5. Authorization Testing
6. Session Management Testing
7. Input Validation Testing
8. Error Handling Testing
9. Cryptography Testing
10. Business Logic Testing
11. Client-Side Testing
12. API Testing

*Source : https://owasp.org/www-project-web-security-testing-guide/*

### Niveau 3 - Application Pratique

#### Checklist sÃ©curitÃ© e-commerce

**Authentification**
- [ ] Hashing passwords (bcrypt, Argon2)
- [ ] Protection brute-force (rate limiting, lockout)
- [ ] MFA disponible
- [ ] Session management sÃ©curisÃ©

**DonnÃ©es sensibles**
- [ ] HTTPS everywhere
- [ ] DonnÃ©es cartes jamais stockÃ©es (tokenization)
- [ ] PII encryptÃ© au repos
- [ ] Logs sans donnÃ©es sensibles

**Injections**
- [ ] Prepared statements (SQL)
- [ ] Output encoding (XSS)
- [ ] CSP headers
- [ ] CORS configurÃ©

**Infrastructure**
- [ ] Security headers (HSTS, X-Content-Type, X-Frame-Options)
- [ ] DÃ©pendances Ã  jour
- [ ] Scans rÃ©guliers

#### Pipeline CI/CD sÃ©curisÃ©

```yaml
# .github/workflows/security.yml
name: Security Checks

on: [push, pull_request]

jobs:
  sast:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Run Semgrep
        uses: returntocorp/semgrep-action@v1
        with:
          config: p/owasp-top-ten

  dependency-check:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Run npm audit
        run: npm audit --audit-level=high
      - name: Run Snyk
        uses: snyk/actions/node@master
        env:
          SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}

  secrets-scan:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: GitLeaks
        uses: gitleaks/gitleaks-action@v2

  dast:
    runs-on: ubuntu-latest
    needs: [sast, dependency-check]
    steps:
      - name: ZAP Baseline Scan
        uses: zaproxy/action-baseline@v0.11.0
        with:
          target: 'https://staging.example.com'
```

---

## 4.3 Accessibility Testing

### Niveau 1 - Vulgarisation

Les **tests d'accessibilitÃ©** vÃ©rifient que l'application est **utilisable par tous**, y compris les personnes en situation de handicap (visuel, auditif, moteur, cognitif).

**Pourquoi c'est important** :
- **Ã‰thique** : 15% de la population mondiale a un handicap
- **LÃ©gal** : Obligations lÃ©gales (RGAA en France, ADA aux USA, EAA en Europe)
- **Business** : Plus d'utilisateurs, meilleur SEO, meilleure UX pour tous

### Niveau 2 - Approfondissement Expert

#### WCAG 2.2 (Web Content Accessibility Guidelines)

**Principes POUR** :
- **P**erceivable : L'information peut Ãªtre perÃ§ue
- **O**perable : L'interface peut Ãªtre utilisÃ©e
- **U**nderstandable : Le contenu est comprÃ©hensible
- **R**obust : Compatible avec les technologies d'assistance

**Niveaux de conformitÃ©** :
- **A** : Minimum (critÃ¨res essentiels)
- **AA** : Standard recommandÃ© (requis lÃ©galement en gÃ©nÃ©ral)
- **AAA** : Optimal (pas toujours atteignable)

#### CritÃ¨res WCAG clÃ©s pour e-commerce

| CritÃ¨re | Niveau | Description | Impact e-commerce |
|---------|--------|-------------|-------------------|
| 1.1.1 Non-text Content | A | Alternatives textuelles aux images | Images produits avec alt text |
| 1.3.1 Info and Relationships | A | Structure sÃ©mantique | Formulaires accessibles |
| 1.4.3 Contrast | AA | Ratio contraste 4.5:1 | Textes lisibles |
| 2.1.1 Keyboard | A | Tout au clavier | Navigation/achat sans souris |
| 2.4.4 Link Purpose | A | Liens explicites | "Voir le produit" vs "Cliquez ici" |
| 3.3.1 Error Identification | A | Erreurs identifiÃ©es | Formulaire checkout |
| 4.1.2 Name, Role, Value | A | Composants accessibles | Boutons, selects custom |

#### Tests automatisÃ©s vs manuels

| Type | Ce qu'il dÃ©tecte | Limites |
|------|------------------|---------|
| **AutomatisÃ©** | ~30% des problÃ¨mes WCAG | Ne comprend pas le contexte |
| **Manuel** | ProblÃ¨mes complexes | Temps, expertise requise |
| **Utilisateurs rÃ©els** | ExpÃ©rience rÃ©elle | CoÃ»t, organisation |

**Recommandation** : AutomatisÃ© en CI + Manuel pÃ©riodique + Tests utilisateurs

#### Outils de test d'accessibilitÃ©

| Outil | Type | Usage |
|-------|------|-------|
| **axe DevTools** | Extension navigateur | Tests manuels rapides |
| **axe-core** | Library | IntÃ©gration CI (Jest, Cypress) |
| **Lighthouse** | Audit Chrome | Score accessibility |
| **WAVE** | Extension/Online | Visualisation des erreurs |
| **Pa11y** | CLI/CI | Automatisation CI/CD |
| **NVDA** | Screen reader (gratuit) | Tests manuels |
| **VoiceOver** | Screen reader (Mac/iOS) | Tests manuels |
| **JAWS** | Screen reader (payant) | Standard professionnel |

#### IntÃ©gration CI/CD

```javascript
// cypress/e2e/accessibility.cy.js
describe('Accessibility Tests', () => {
  beforeEach(() => {
    cy.injectAxe();
  });

  it('Homepage should have no critical violations', () => {
    cy.visit('/');
    cy.checkA11y(null, {
      rules: {
        'color-contrast': { enabled: true },
        'label': { enabled: true }
      }
    });
  });

  it('Product page should be accessible', () => {
    cy.visit('/products/sample-product');
    cy.checkA11y();
  });

  it('Checkout form should be accessible', () => {
    cy.visit('/checkout');
    cy.checkA11y('form');
  });
});
```

### Niveau 3 - Application Pratique

#### Checklist accessibilitÃ© e-commerce

**Images**
- [ ] Alt text sur toutes les images produits
- [ ] Alt vide pour images dÃ©coratives
- [ ] Zoom accessible au clavier

**Formulaires**
- [ ] Labels associÃ©s aux inputs
- [ ] Messages d'erreur explicites
- [ ] Focus visible

**Navigation**
- [ ] Skip link "Aller au contenu"
- [ ] Structure de headings logique (h1 â†’ h2 â†’ h3)
- [ ] Navigation au clavier complÃ¨te
- [ ] Focus trap dans les modales

**Checkout**
- [ ] Ã‰tapes clairement indiquÃ©es
- [ ] RÃ©sumÃ© commande accessible
- [ ] Erreurs de validation claires
- [ ] Timeout avec warning

---

## 4.4 Compatibility Testing

### Niveau 1 - Vulgarisation

Les **tests de compatibilitÃ©** vÃ©rifient que l'application fonctionne **sur diffÃ©rents navigateurs, appareils et systÃ¨mes d'exploitation**.

**Pourquoi c'est important** : Vos utilisateurs n'utilisent pas tous Chrome sur un MacBook Pro dernier cri. Certains sont sur Safari iPhone, d'autres sur Firefox Linux, d'autres sur Edge Windows.

### Niveau 2 - Approfondissement Expert

#### Types de compatibilitÃ©

| Type | Description | Exemples |
|------|-------------|----------|
| **Cross-browser** | DiffÃ©rents navigateurs | Chrome, Firefox, Safari, Edge |
| **Cross-device** | DiffÃ©rents appareils | Desktop, tablet, mobile |
| **Cross-OS** | DiffÃ©rents systÃ¨mes | Windows, macOS, Linux, iOS, Android |
| **Cross-resolution** | DiffÃ©rentes tailles d'Ã©cran | 1920x1080, 1366x768, 375x667 |
| **Backward compatibility** | Anciennes versions | IE11, vieux Android |

#### StratÃ©gie de test cross-browser

**1. Identifier le marchÃ© cible**
Utiliser analytics pour connaÃ®tre les browsers/devices des utilisateurs.

**2. DÃ©finir la matrice de support**

| Browser | Version | Support |
|---------|---------|---------|
| Chrome | Latest, Latest-1 | Full |
| Firefox | Latest, Latest-1 | Full |
| Safari | Latest, Latest-1 | Full |
| Edge | Latest | Full |
| Samsung Internet | Latest | Full |
| Chrome Mobile | Latest | Full |
| Safari iOS | Latest, Latest-1 | Full |
| IE11 | - | None |

**3. Tests automatisÃ©s multi-browser**

Playwright exemple :
```javascript
// playwright.config.js
module.exports = {
  projects: [
    { name: 'chromium', use: { ...devices['Desktop Chrome'] } },
    { name: 'firefox', use: { ...devices['Desktop Firefox'] } },
    { name: 'webkit', use: { ...devices['Desktop Safari'] } },
    { name: 'mobile-chrome', use: { ...devices['Pixel 5'] } },
    { name: 'mobile-safari', use: { ...devices['iPhone 12'] } },
  ],
};
```

#### Outils et services

| Outil | Usage |
|-------|-------|
| **BrowserStack** | Cloud testing, real devices |
| **Sauce Labs** | Cloud testing, CI integration |
| **LambdaTest** | Cloud testing |
| **Playwright** | Automation multi-browser |
| **Cypress** | Automation (Chrome, Firefox, Edge) |
| **BrowserSync** | Dev testing synchronisÃ© |

### Niveau 3 - Application Pratique

#### Matrice de test e-commerce France

**BasÃ© sur donnÃ©es marchÃ© France 2024 [Ã€ VÃ‰RIFIER]** :

| Browser | Part de marchÃ© | PrioritÃ© test |
|---------|----------------|---------------|
| Chrome Desktop | ~50% | Haute |
| Safari Mobile | ~20% | Haute |
| Chrome Mobile | ~15% | Haute |
| Safari Desktop | ~8% | Moyenne |
| Firefox | ~5% | Moyenne |
| Edge | ~2% | Basse |

---

## 4.5 Usability Testing

### Niveau 1 - Vulgarisation

Les **tests d'utilisabilitÃ©** Ã©valuent **si les utilisateurs arrivent Ã  utiliser l'application facilement**. Ce n'est pas "est-ce que Ã§a marche" mais "est-ce que c'est facile Ã  utiliser".

**Pourquoi c'est important** : Une fonctionnalitÃ© qui existe mais que personne ne trouve ou ne comprend est une fonctionnalitÃ© inutile.

### Niveau 2 - Approfondissement Expert

#### Types de tests d'utilisabilitÃ©

| Type | Description | Participants |
|------|-------------|--------------|
| **Moderated** | Facilitateur prÃ©sent, questions en direct | 5-8 par round |
| **Unmoderated** | Utilisateur seul, enregistrement | 10-20+ |
| **Remote** | Ã€ distance, outils en ligne | Variable |
| **In-person** | En prÃ©sentiel, observation directe | 5-8 |
| **Guerrilla** | Tests rapides, lieux publics | 3-5 |
| **A/B Testing** | Comparaison de variantes | 1000+ |

#### MÃ©triques d'utilisabilitÃ©

| MÃ©trique | Description | Mesure |
|----------|-------------|--------|
| **Task Success Rate** | % de tÃ¢ches rÃ©ussies | Objectif : > 80% |
| **Time on Task** | Temps pour complÃ©ter une tÃ¢che | Comparaison au benchmark |
| **Error Rate** | Nombre d'erreurs par tÃ¢che | Moins = mieux |
| **SUS** | System Usability Scale | Score /100 (>68 = OK) |
| **NPS** | Net Promoter Score | -100 Ã  +100 |
| **SEQ** | Single Ease Question | 1-7 par tÃ¢che |

#### System Usability Scale (SUS)

Questionnaire standardisÃ© de 10 questions (John Brooke, 1986).

**Questions** (alternance positive/nÃ©gative) :
1. Je pense que j'utiliserais volontiers ce systÃ¨me frÃ©quemment
2. J'ai trouvÃ© ce systÃ¨me inutilement complexe
3. J'ai trouvÃ© ce systÃ¨me facile Ã  utiliser
4. Je pense que j'aurais besoin d'une aide technique pour utiliser ce systÃ¨me
5. J'ai trouvÃ© que les fonctions Ã©taient bien intÃ©grÃ©es
6. J'ai trouvÃ© qu'il y avait trop d'incohÃ©rences dans ce systÃ¨me
7. Je pense que la plupart des gens apprendraient rapidement Ã  utiliser ce systÃ¨me
8. J'ai trouvÃ© ce systÃ¨me trÃ¨s contraignant Ã  utiliser
9. Je me suis senti(e) Ã  l'aise en utilisant ce systÃ¨me
10. J'ai dÃ» apprendre beaucoup de choses avant de pouvoir utiliser ce systÃ¨me

**Calcul** : Score de 0 Ã  100
- Score > 68 : Au-dessus de la moyenne
- Score > 80 : Bon
- Score > 90 : Excellent

#### Outils de test d'utilisabilitÃ©

| Outil | Type | Usage |
|-------|------|-------|
| **UserTesting** | Unmoderated remote | Tests avec vrais utilisateurs |
| **Maze** | Unmoderated | Tests de prototype |
| **Hotjar** | Analytics comportemental | Heatmaps, recordings |
| **FullStory** | Session replay | Comprendre les frictions |
| **Lookback** | Moderated remote | Entretiens utilisateurs |
| **Optimal Workshop** | Card sorting, tree testing | Architecture information |

### Niveau 3 - Application Pratique

#### ScÃ©nario de test e-commerce

**TÃ¢che** : "Achetez une paire de chaussures de running taille 42 pour moins de 100â‚¬"

**Observations** :
- L'utilisateur trouve-t-il les filtres ?
- Combien de temps pour trouver un produit ?
- Abandonne-t-il ? OÃ¹ ?
- ComplÃ¨te-t-il l'achat ?

**MÃ©triques collectÃ©es** :
- Task success : Oui/Non
- Time on task : 3 min 24 sec
- Errors : 1 (mauvais filtre sÃ©lectionnÃ©)
- SEQ : 5/7

---

## 4.6 Localization Testing

### Niveau 1 - Vulgarisation

Les **tests de localisation** vÃ©rifient que l'application fonctionne correctement **dans diffÃ©rentes langues et rÃ©gions** : traductions, formats de dates, devises, etc.

**Pourquoi c'est important** : Un e-commerce qui affiche les prix en dollars aux clients franÃ§ais, ou des dates au format amÃ©ricain (MM/DD/YYYY) perd en crÃ©dibilitÃ© et en conversions.

### Niveau 2 - Approfondissement Expert

#### i18n vs l10n

| Terme | Signification | Focus |
|-------|---------------|-------|
| **i18n** | Internationalization | PrÃ©parer le code pour la localisation |
| **l10n** | Localization | Adapter le contenu pour une locale |

#### Ã‰lÃ©ments Ã  tester

| CatÃ©gorie | Ã‰lÃ©ments | Exemples |
|-----------|----------|----------|
| **Textes** | Traductions, longueur | "Add to cart" â†’ "Ajouter au panier" |
| **Dates** | Format | 12/31/2024 vs 31/12/2024 |
| **Nombres** | SÃ©parateurs | 1,234.56 vs 1 234,56 |
| **Devises** | Symbole, position | $99.99 vs 99,99 â‚¬ |
| **Adresses** | Format | ZIP code vs Code postal |
| **TÃ©lÃ©phones** | Format | +1 (555) 123-4567 vs +33 1 23 45 67 89 |
| **UI** | Direction | LTR vs RTL (arabe, hÃ©breu) |
| **Images** | Contenu culturel | Adapter les visuels |
| **LÃ©gal** | CGV, mentions | SpÃ©cifiques par pays |

#### Checklist localisation e-commerce

**Technique**
- [ ] Tous les textes externalisÃ©s (pas de hardcoded)
- [ ] Formats dates/nombres selon locale
- [ ] Devises correctement converties/affichÃ©es
- [ ] Tri alphabÃ©tique respecte la locale
- [ ] RTL supportÃ© si nÃ©cessaire

**Contenu**
- [ ] Traductions complÃ¨tes (pas de texte manquant)
- [ ] Traductions de qualitÃ© (pas de machine translation brute)
- [ ] Contexte respectÃ© (bouton court vs description longue)
- [ ] Termes spÃ©cifiques au marchÃ© (shipping, VAT)

**UX**
- [ ] UI s'adapte aux textes plus longs (allemand +30%)
- [ ] Images culturellement appropriÃ©es
- [ ] SÃ©lecteur de langue visible
- [ ] URL localisÃ©es (/fr/, /de/, /es/)

### Niveau 3 - Application Pratique

#### Tests pseudo-localisation

**Technique** : Remplacer les textes par des versions artificielles pour dÃ©tecter les problÃ¨mes.

```
Original: "Add to cart"
Pseudo-L10n: "[Ã€á¸“á¸“ á¹­Ã¶ Ã§Ã á¹›á¹­ one two three]"
```

**Objectifs** :
- DÃ©tecter les textes hardcodÃ©s (non traduits)
- VÃ©rifier que l'UI supporte les textes longs
- Identifier les problÃ¨mes d'encodage (caractÃ¨res spÃ©ciaux)

#### Outils

| Outil | Usage |
|-------|-------|
| **Crowdin** | Gestion traductions |
| **Phrase** | Gestion traductions |
| **Lokalise** | Gestion traductions |
| **i18next** | Library JS i18n |
| **react-intl** | React i18n |

---

# Section 5 : Automatisation des Tests

---

## 5.1 Automation Strategy et ROI

### Niveau 1 - Vulgarisation

L'**automatisation des tests** consiste Ã  faire exÃ©cuter des tests par des programmes plutÃ´t que par des humains. Mais automatiser a un coÃ»t : il faut l'Ã©crire, le maintenir. La question est : **est-ce que Ã§a vaut le coup ?**

**Analogie** : Acheter un robot aspirateur. Le robot coÃ»te cher Ã  l'achat, mais si vous aspirez tous les jours, il sera rentable aprÃ¨s quelques mois. Si vous aspirez une fois par mois, gardez l'aspirateur manuel.

**Pourquoi c'est important** : Automatiser sans stratÃ©gie mÃ¨ne Ã  des tests fragiles, coÃ»teux Ã  maintenir, qui finissent ignorÃ©s. L'automatisation doit avoir un ROI positif.

### Niveau 2 - Approfondissement Expert

#### Quand automatiser ?

**Automatiser si** :
- Test exÃ©cutÃ© frÃ©quemment (rÃ©gression)
- Test stable (comportement prÃ©visible)
- Test critique (parcours business clÃ©)
- Test manuel long et rÃ©pÃ©titif
- Test nÃ©cessitant prÃ©cision (calculs)
- Test nÃ©cessitant volume (load testing)

**Ne PAS automatiser si** :
- Test one-shot (exploration initiale)
- FonctionnalitÃ© en Ã©volution rapide (instable)
- Test nÃ©cessitant jugement humain (UX, exploratory)
- ROI nÃ©gatif (coÃ»t > bÃ©nÃ©fice)

#### Calcul du ROI de l'automatisation

**Formule simplifiÃ©e** :
```
ROI = (CoÃ»t Manuel Ã— Nombre d'exÃ©cutions) - CoÃ»t Automatisation
                          CoÃ»t Automatisation

OÃ¹ :
- CoÃ»t Manuel = Temps exÃ©cution Ã— Tarif horaire
- CoÃ»t Automatisation = DÃ©veloppement + Maintenance
```

**Exemple concret** :
```
Test manuel checkout :
- Temps d'exÃ©cution : 30 minutes
- FrÃ©quence : 20x par mois
- Tarif QA : 50â‚¬/h
- CoÃ»t mensuel manuel : 0.5h Ã— 20 Ã— 50â‚¬ = 500â‚¬/mois

Automatisation :
- DÃ©veloppement initial : 8h Ã— 80â‚¬/h = 640â‚¬
- Maintenance mensuelle : 2h Ã— 80â‚¬/h = 160â‚¬/mois
- ExÃ©cution : ~0â‚¬ (CI/CD)

Break-even : 640â‚¬ / (500â‚¬ - 160â‚¬) â‰ˆ 2 mois
ROI aprÃ¨s 1 an : ((500 Ã— 12) - (640 + 160 Ã— 12)) / (640 + 160 Ã— 12) â‰ˆ 140%
```

#### Pyramide de l'automatisation appliquÃ©e

| Niveau | % Budget | ROI typique |
|--------|----------|-------------|
| Unit Tests | 40-50% | TrÃ¨s Ã©levÃ© |
| Integration | 30-40% | Ã‰levÃ© |
| E2E | 10-20% | Variable |

### Niveau 3 - Application Pratique

#### Framework de dÃ©cision

```
DÃ‰CISION D'AUTOMATISATION

Pour chaque test, Ã©valuer :

1. FRÃ‰QUENCE D'EXÃ‰CUTION
   â–¡ Quotidien (+3)
   â–¡ Hebdomadaire (+2)
   â–¡ Mensuel (+1)
   â–¡ Rare (0)

2. STABILITÃ‰ DU TEST
   â–¡ TrÃ¨s stable (+3)
   â–¡ Stable (+2)
   â–¡ Changeant (-1)
   â–¡ TrÃ¨s volatile (-3)

3. CRITICITÃ‰ BUSINESS
   â–¡ Bloquant si Ã©chec (+3)
   â–¡ Important (+2)
   â–¡ Nice-to-have (+1)
   â–¡ Marginal (0)

4. COMPLEXITÃ‰ D'AUTOMATISATION
   â–¡ Simple (+2)
   â–¡ Moyenne (+1)
   â–¡ Complexe (0)
   â–¡ TrÃ¨s complexe (-2)

SCORE TOTAL :
- >= 8 : Automatiser en prioritÃ©
- 5-7 : Automatiser si temps disponible
- 2-4 : Ã‰valuer au cas par cas
- < 2 : Garder manuel
```

#### Anti-patterns

| Anti-pattern | ProblÃ¨me | Solution |
|--------------|----------|----------|
| "Automatiser tout" | ROI nÃ©gatif, maintenance impossible | Prioriser par ROI |
| "Record & Playback" | Tests fragiles | Conception propre (POM) |
| "Tests abandonnÃ©s" | Suite rouge ignorÃ©e | Maintenance rÃ©guliÃ¨re |
| "Automatiser en retard" | Technical debt | Shift-left automation |

---

## 5.2 Test Automation Pyramid

### Niveau 1 - Vulgarisation

La **pyramide d'automatisation** est un modÃ¨le visuel qui recommande **plus de tests unitaires (base), moins de tests E2E (sommet)**. Comme une vraie pyramide : large base, sommet Ã©troit.

**Pourquoi** : Les tests unitaires sont rapides, stables, pas chers. Les tests E2E sont lents, fragiles, chers. Mieux vaut avoir beaucoup du premier et peu du second.

### Niveau 2 - Approfondissement Expert

#### Origine

Concept popularisÃ© par **Mike Cohn** dans "Succeeding with Agile" (2009), bien que l'idÃ©e existait avant.

#### La pyramide classique

```
            /\
           /  \         UI / E2E Tests
          /â”€â”€â”€â”€\        (Peu, lents, fragiles)
         /      \
        /â”€â”€â”€â”€â”€â”€â”€â”€\      Integration Tests
       /          \     (Quelques-uns)
      /â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\
     /              \   Unit Tests
    /â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\  (Beaucoup, rapides, stables)
```

#### Proportions recommandÃ©es

| Source | Unit | Integration | E2E |
|--------|------|-------------|-----|
| Google Testing Blog | 70% | 20% | 10% |
| Martin Fowler | ~70% | ~20% | ~10% |
| Pratique courante | 60-80% | 15-30% | 5-15% |

**Important** : Ces ratios sont indicatifs, pas des rÃ¨gles absolues. Le contexte compte.

#### CaractÃ©ristiques par niveau

| Aspect | Unit | Integration | E2E |
|--------|------|-------------|-----|
| Vitesse | ms | secondes | minutes |
| StabilitÃ© | Haute | Moyenne | Basse |
| CoÃ»t Ã©criture | Bas | Moyen | Ã‰levÃ© |
| CoÃ»t maintenance | Bas | Moyen | Ã‰levÃ© |
| Feedback | ImmÃ©diat | Rapide | Tardif |
| Isolement | Total | Partiel | Aucun |
| Debugging | Facile | Moyen | Difficile |

#### Variantes et critiques

**Ice Cream Cone (Anti-pattern)** :
```
    â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„
   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â† Trop de tests manuels
    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     â† Trop de E2E
      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       â† Peu d'integration
        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
         â–ˆâ–ˆâ–ˆâ–ˆ         â† TrÃ¨s peu de unit tests
          â–ˆâ–ˆ
```

**Testing Trophy (Kent C. Dodds)** :
```
        /\
       /  \         E2E (peu)
      /â”€â”€â”€â”€\
     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       Integration (beaucoup)
    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       Unit (quelques-uns)
      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
       Static (base)
```

Cette variante suggÃ¨re plus de tests d'intÃ©gration et moins de tests unitaires purs.

### Niveau 3 - Application Pratique

#### Application e-commerce

```
STRATÃ‰GIE DE TEST E-COMMERCE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

NIVEAU E2E (10%) - 20-30 tests
â”œâ”€â”€ Checkout guest complet
â”œâ”€â”€ Checkout user enregistrÃ©
â”œâ”€â”€ Recherche â†’ ajout panier â†’ achat
â”œâ”€â”€ Inscription / Connexion
â”œâ”€â”€ Paiements (CB, PayPal, etc.)
â””â”€â”€ Smoke tests critiques

NIVEAU INTEGRATION (25%) - 100-150 tests
â”œâ”€â”€ API endpoints (auth, products, cart, orders)
â”œâ”€â”€ Services â†” Database
â”œâ”€â”€ Services â†” External APIs (payment, shipping)
â”œâ”€â”€ Event handlers (order placed, payment received)
â””â”€â”€ Business workflows

NIVEAU UNIT (65%) - 500-1000+ tests
â”œâ”€â”€ Cart calculator (prix, promos, taxes)
â”œâ”€â”€ Validators (email, address, card)
â”œâ”€â”€ Formatters (dates, currency)
â”œâ”€â”€ Business logic (stock, eligibility)
â”œâ”€â”€ Utilities
â””â”€â”€ Components UI (render, props)
```

---

## 5.3 Page Object Model (POM)

### Niveau 1 - Vulgarisation

Le **Page Object Model** est une faÃ§on d'organiser le code des tests E2E/UI. Au lieu de rÃ©pÃ©ter les sÃ©lecteurs et actions dans chaque test, on les centralise dans des "objets page".

**Analogie** : Au lieu que chaque recette dÃ©crive comment allumer le four, vous avez un manuel du four. Les recettes disent juste "PrÃ©chauffer Ã  180Â°" et vous consultez le manuel si besoin.

**Pourquoi c'est important** : Si le bouton "Ajouter au panier" change de classe CSS, vous corrigez Ã  un seul endroit plutÃ´t que dans 50 tests.

### Niveau 2 - Approfondissement Expert

#### Principes

1. **Une classe par page/composant** : `LoginPage`, `ProductPage`, `CheckoutPage`
2. **Encapsulation des sÃ©lecteurs** : Les sÃ©lecteurs sont privÃ©s
3. **MÃ©thodes publiques pour les actions** : `login(user, pass)`, `addToCart()`
4. **Retourne des Page Objects** : Pour le chaÃ®nage

#### Structure type

```javascript
// pages/ProductPage.js
class ProductPage {
  // SÃ©lecteurs (privÃ©s ou dans un objet sÃ©parÃ©)
  selectors = {
    title: '[data-testid="product-title"]',
    price: '[data-testid="product-price"]',
    addToCartBtn: '[data-testid="add-to-cart"]',
    quantityInput: '[data-testid="quantity"]',
    sizeSelect: '[data-testid="size-select"]',
  };

  constructor(page) {
    this.page = page;
  }

  // Actions
  async selectSize(size) {
    await this.page.selectOption(this.selectors.sizeSelect, size);
  }

  async setQuantity(qty) {
    await this.page.fill(this.selectors.quantityInput, String(qty));
  }

  async addToCart() {
    await this.page.click(this.selectors.addToCartBtn);
    // Retourne la page suivante pour chaÃ®nage
    return new CartPage(this.page);
  }

  // Assertions
  async getTitle() {
    return await this.page.textContent(this.selectors.title);
  }

  async getPrice() {
    const priceText = await this.page.textContent(this.selectors.price);
    return parseFloat(priceText.replace(/[^0-9.]/g, ''));
  }
}
```

#### Utilisation dans un test

```javascript
// tests/checkout.spec.js
test('should add product to cart', async ({ page }) => {
  const homePage = new HomePage(page);
  const productPage = await homePage.searchAndClickFirstProduct('shoes');

  await productPage.selectSize('42');
  await productPage.setQuantity(2);
  const cartPage = await productPage.addToCart();

  const itemCount = await cartPage.getItemCount();
  expect(itemCount).toBe(2);
});
```

#### Variations et Ã©volutions

| Pattern | Description | Cas d'usage |
|---------|-------------|-------------|
| **Page Object** | Une classe par page | Pages distinctes |
| **Component Object** | Une classe par composant rÃ©utilisable | Header, Footer, Modal |
| **Screenplay Pattern** | Acteurs + TÃ¢ches + Questions | Tests trÃ¨s lisibles, complexitÃ© accrue |
| **App Actions** | Actions via API plutÃ´t que UI pour setup | Setup rapide avant tests UI |

### Niveau 3 - Application Pratique

#### Structure de projet

```
tests/
â”œâ”€â”€ e2e/
â”‚   â”œâ”€â”€ checkout.spec.js
â”‚   â”œâ”€â”€ search.spec.js
â”‚   â””â”€â”€ account.spec.js
â”œâ”€â”€ pages/
â”‚   â”œâ”€â”€ BasePage.js
â”‚   â”œâ”€â”€ HomePage.js
â”‚   â”œâ”€â”€ ProductPage.js
â”‚   â”œâ”€â”€ CartPage.js
â”‚   â”œâ”€â”€ CheckoutPage.js
â”‚   â””â”€â”€ ConfirmationPage.js
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ Header.js
â”‚   â”œâ”€â”€ SearchBar.js
â”‚   â””â”€â”€ ProductCard.js
â”œâ”€â”€ fixtures/
â”‚   â””â”€â”€ testData.js
â””â”€â”€ utils/
    â””â”€â”€ helpers.js
```

#### Anti-patterns

| Anti-pattern | ProblÃ¨me | Solution |
|--------------|----------|----------|
| SÃ©lecteurs dans les tests | Duplication, maintenance | Centraliser dans Page Objects |
| God Page Object | Classe de 1000 lignes | DÃ©composer en components |
| Assertions dans Page Object | MÃ©lange des responsabilitÃ©s | Actions dans PO, assertions dans tests |
| HÃ©ritage profond | ComplexitÃ© | Composition plutÃ´t qu'hÃ©ritage |

---

## 5.4 Test Data Management

### Niveau 1 - Vulgarisation

La **gestion des donnÃ©es de test** assure que vos tests ont toujours les **bonnes donnÃ©es disponibles** : utilisateurs test, produits, commandes, etc.

**Pourquoi c'est important** : Un test qui cherche le produit "PROD-001" Ã©choue si ce produit n'existe pas. Les donnÃ©es de test doivent Ãªtre fiables et reproductibles.

### Niveau 2 - Approfondissement Expert

#### Approches

| Approche | Description | Avantages | InconvÃ©nients |
|----------|-------------|-----------|---------------|
| **Fixtures statiques** | Fichiers JSON/SQL prÃ©chargÃ©s | Simple, rapide | Rigide, couplÃ© |
| **Factories** | Code qui gÃ©nÃ¨re des donnÃ©es | Flexible, maintenable | Plus complexe |
| **Seeding** | Peuplement de la DB avant tests | DonnÃ©es rÃ©alistes | Lent, lourd |
| **API Setup** | CrÃ©ation via API avant chaque test | IsolÃ©, propre | Plus lent |
| **Shared test DB** | Base partagÃ©e entre tests | Simple | Conflits, pollution |

#### Pattern Factory

```javascript
// factories/userFactory.js
const { faker } = require('@faker-js/faker');

const createUser = (overrides = {}) => ({
  id: faker.string.uuid(),
  email: faker.internet.email(),
  firstName: faker.person.firstName(),
  lastName: faker.person.lastName(),
  password: 'TestPassword123!',
  role: 'customer',
  createdAt: new Date(),
  ...overrides, // Permet de surcharger
});

const createAdmin = (overrides = {}) =>
  createUser({ role: 'admin', ...overrides });

const createOrder = (userId, overrides = {}) => ({
  id: faker.string.uuid(),
  userId,
  items: [
    {
      productId: 'PROD-001',
      quantity: faker.number.int({ min: 1, max: 5 }),
      price: faker.number.float({ min: 10, max: 200, precision: 0.01 }),
    },
  ],
  status: 'pending',
  createdAt: new Date(),
  ...overrides,
});

module.exports = { createUser, createAdmin, createOrder };
```

#### Utilisation

```javascript
const { createUser, createOrder } = require('../factories/userFactory');

test('should display order history', async () => {
  // Arrange - CrÃ©ation de donnÃ©es
  const user = await db.users.create(createUser());
  const order1 = await db.orders.create(createOrder(user.id, { status: 'delivered' }));
  const order2 = await db.orders.create(createOrder(user.id, { status: 'pending' }));

  // Act
  await loginAs(user);
  const ordersPage = await navigateTo('/account/orders');

  // Assert
  expect(await ordersPage.getOrderCount()).toBe(2);

  // Cleanup (si nÃ©cessaire)
  await db.orders.delete([order1.id, order2.id]);
  await db.users.delete(user.id);
});
```

#### Outils

| Langage | Outils |
|---------|--------|
| JavaScript | Faker.js, Fishery, Factory.ts |
| Python | Factory Boy, Faker |
| Java | Datafaker, Easy Random |
| Ruby | FactoryBot, Faker |

### Niveau 3 - Application Pratique

#### StratÃ©gie pour e-commerce

```
TEST DATA STRATEGY

1. FIXTURES STATIQUES (rÃ©fÃ©rence)
   â””â”€â”€ Produits catalogue (catÃ©gories, prix, images)
   â””â”€â”€ Configuration (pays, devises, shipping)

2. FACTORIES (tests individuels)
   â””â”€â”€ Users (customer, admin, guest)
   â””â”€â”€ Carts
   â””â”€â”€ Orders
   â””â”€â”€ Addresses

3. SEEDING (environnement staging)
   â””â”€â”€ DonnÃ©es anonymisÃ©es de production
   â””â”€â”€ Volume rÃ©aliste

4. ISOLATION
   â””â”€â”€ Chaque test crÃ©e ses donnÃ©es
   â””â”€â”€ Nettoyage aprÃ¨s chaque test (ou transaction rollback)
```

---

## 5.5 Flaky Tests

### Niveau 1 - Vulgarisation

Un **test flaky** (instable) est un test qui **parfois passe, parfois Ã©choue**, sans qu'on ait changÃ© le code. C'est comme un dÃ©tecteur de fumÃ©e qui sonne au hasard : on finit par l'ignorer, mÃªme quand il y a vraiment un problÃ¨me.

**Pourquoi c'est un problÃ¨me** : Les tests flaky Ã©rodent la confiance dans la suite de tests. L'Ã©quipe commence Ã  "relancer" les tests jusqu'Ã  ce qu'ils passent, ou les dÃ©sactive.

### Niveau 2 - Approfondissement Expert

#### Causes courantes

| Cause | Exemple | Solution |
|-------|---------|----------|
| **Timing/Race conditions** | Ã‰lÃ©ment pas encore chargÃ© | Waits explicites |
| **Ordre d'exÃ©cution** | Tests dÃ©pendants de l'ordre | Isolation complÃ¨te |
| **DonnÃ©es partagÃ©es** | Tests modifient les mÃªmes donnÃ©es | DonnÃ©es isolÃ©es |
| **Ã‰tat global** | Singleton non rÃ©initialisÃ© | Reset entre tests |
| **Ressources externes** | API tierce instable | Mock, retry |
| **Concurrence** | Tests en parallÃ¨le qui interfÃ¨rent | Isolation, locks |
| **Date/Heure** | Test basÃ© sur "aujourd'hui" | Freeze time |
| **AlÃ©atoire** | Math.random() dans le code | Seed dÃ©terministe |

#### StratÃ©gies de gestion

1. **DÃ©tection** : Marquer les tests qui Ã©chouent puis passent au retry
2. **Quarantaine** : Isoler les flaky tests dans une suite sÃ©parÃ©e
3. **Investigation** : Prioriser le fix selon l'impact
4. **Retry automatique** : RÃ©exÃ©cuter N fois avant de marquer comme Ã©chec
5. **Suppression** : Si non fixable et faible valeur, supprimer

#### Configuration retry (exemples)

**Jest** :
```javascript
// jest.config.js
module.exports = {
  testRetries: 2, // Retry 2 fois si Ã©chec
};
```

**Playwright** :
```javascript
// playwright.config.js
module.exports = {
  retries: process.env.CI ? 2 : 0, // Retry seulement en CI
};
```

**Cypress** :
```javascript
// cypress.config.js
module.exports = {
  retries: {
    runMode: 2,      // CI
    openMode: 0,     // Local dev
  },
};
```

#### MÃ©triques de flakiness

```
Flake Rate = (Runs with flaky failures / Total runs) Ã— 100

Objectif : < 1%
Alerte si : > 5%
Critique si : > 10%
```

### Niveau 3 - Application Pratique

#### Checklist anti-flaky pour E2E

```javascript
// âŒ MAUVAIS : Sleep fixe
await page.click('#submit');
await new Promise(r => setTimeout(r, 3000)); // Danger !
await expect(page.locator('#success')).toBeVisible();

// âœ… BON : Wait explicite
await page.click('#submit');
await expect(page.locator('#success')).toBeVisible({ timeout: 10000 });
```

```javascript
// âŒ MAUVAIS : SÃ©lecteur fragile
await page.click('.btn-primary:nth-child(2)');

// âœ… BON : SÃ©lecteur stable
await page.click('[data-testid="submit-order"]');
```

```javascript
// âŒ MAUVAIS : DÃ©pendance Ã  l'heure actuelle
const isNewYear = new Date().getMonth() === 0;

// âœ… BON : Heure contrÃ´lÃ©e
jest.useFakeTimers().setSystemTime(new Date('2025-01-01'));
```

---

## 5.6 Visual Regression Testing

### Niveau 1 - Vulgarisation

Le **visual regression testing** compare des **screenshots** de l'application pour dÃ©tecter des changements visuels non intentionnels. C'est comme comparer une photo "avant" et "aprÃ¨s" pour voir ce qui a changÃ©.

**Pourquoi c'est important** : Un changement CSS peut casser la mise en page sans qu'aucun test fonctionnel ne le dÃ©tecte. Le bouton fonctionne toujours, mais il est maintenant cachÃ© sous un autre Ã©lÃ©ment.

### Niveau 2 - Approfondissement Expert

#### Fonctionnement

1. **Baseline** : Capture des screenshots de rÃ©fÃ©rence
2. **Comparison** : Lors des tests, nouvelles captures
3. **Diff** : Comparaison pixel par pixel ou perceptuelle
4. **Review** : Humain valide les changements intentionnels
5. **Update** : Nouvelles baselines si changement acceptÃ©

#### Outils

| Outil | Type | Points forts |
|-------|------|--------------|
| **Percy** (BrowserStack) | SaaS | IntÃ©gration CI, review UI |
| **Chromatic** | SaaS (Storybook) | IdÃ©al pour composants |
| **Applitools** | SaaS | IA pour comparaison smart |
| **BackstopJS** | Open source | Config simple, local |
| **Playwright** | Built-in | `toHaveScreenshot()` |
| **Cypress** | Plugin | cypress-image-snapshot |

#### Playwright Visual Testing

```javascript
// visual.spec.js
import { test, expect } from '@playwright/test';

test('homepage visual regression', async ({ page }) => {
  await page.goto('/');

  // Full page screenshot
  await expect(page).toHaveScreenshot('homepage.png', {
    fullPage: true,
    maxDiffPixels: 100, // TolÃ©rance
  });
});

test('product card component', async ({ page }) => {
  await page.goto('/products/sample');

  // Screenshot d'un Ã©lÃ©ment spÃ©cifique
  const productCard = page.locator('[data-testid="product-card"]');
  await expect(productCard).toHaveScreenshot('product-card.png');
});
```

#### DÃ©fis et solutions

| DÃ©fi | Solution |
|------|----------|
| Contenu dynamique (dates, ads) | Masquer ou mocker |
| Animations | DÃ©sactiver ou attendre fin |
| Fonts non chargÃ©es | Attendre font loading |
| DiffÃ©rences cross-browser | Un baseline par browser |
| Anti-aliasing | TolÃ©rance pixel (threshold) |

### Niveau 3 - Application Pratique

#### StratÃ©gie e-commerce

**Pages Ã  tester** :
- Homepage (hero, catÃ©gories, promos)
- Listing produits (grid, filtres)
- Fiche produit (images, prix, CTA)
- Panier
- Checkout steps
- Emails (si possible)

**FrÃ©quence** :
- PR : Sur composants modifiÃ©s
- Nightly : Suite complÃ¨te
- Pre-release : Full regression

---

## 5.7 Continuous Testing in CI/CD

### Niveau 1 - Vulgarisation

Le **Continuous Testing** intÃ¨gre les tests automatisÃ©s dans le **pipeline CI/CD** pour avoir du feedback Ã  chaque changement de code. Chaque commit dÃ©clenche des tests automatiquement.

**Pourquoi c'est important** : Plus on dÃ©tecte un bug tÃ´t (Shift-Left), moins il coÃ»te cher Ã  corriger. Les tests en CI/CD dÃ©tectent les rÃ©gressions immÃ©diatement.

### Niveau 2 - Approfondissement Expert

#### Pipeline typique

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PUSH   â”‚â”€â”€â”€â–¶â”‚  BUILD  â”‚â”€â”€â”€â–¶â”‚  TEST   â”‚â”€â”€â”€â–¶â”‚ DEPLOY  â”‚â”€â”€â”€â–¶â”‚ VERIFY  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                   â”‚                   â”‚
              â–¼                   â–¼                   â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  LINT   â”‚         â”‚  UNIT   â”‚         â”‚  SAST   â”‚
         â”‚ STATIC  â”‚         â”‚  TESTS  â”‚         â”‚SECURITY â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                   â”‚                   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â–¼
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚ INTEGRATION â”‚
                           â”‚    TESTS    â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚   E2E TESTS â”‚
                           â”‚   (subset)  â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### GitHub Actions exemple

```yaml
# .github/workflows/ci.yml
name: CI Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
      - run: npm ci
      - run: npm run lint
      - run: npm run typecheck

  unit-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
      - run: npm ci
      - run: npm run test:unit -- --coverage
      - uses: codecov/codecov-action@v3

  integration-tests:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
        ports:
          - 5432:5432
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
      - run: npm ci
      - run: npm run test:integration
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test

  e2e-tests:
    runs-on: ubuntu-latest
    needs: [lint, unit-tests]
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
      - run: npm ci
      - run: npx playwright install --with-deps
      - run: npm run build
      - run: npm run test:e2e
      - uses: actions/upload-artifact@v4
        if: failure()
        with:
          name: playwright-report
          path: playwright-report/

  deploy-staging:
    runs-on: ubuntu-latest
    needs: [e2e-tests, integration-tests]
    if: github.ref == 'refs/heads/develop'
    steps:
      - run: echo "Deploy to staging..."
```

#### Optimisations

| Technique | BÃ©nÃ©fice |
|-----------|----------|
| **ParallÃ©lisation** | Tests plus rapides |
| **Caching** | node_modules, builds, Playwright browsers |
| **Test sharding** | RÃ©partir E2E sur plusieurs runners |
| **Selective testing** | Ne tester que ce qui a changÃ© |
| **Fail fast** | ArrÃªter dÃ¨s le premier Ã©chec critique |

### Niveau 3 - Application Pratique

#### DurÃ©es cibles

| Stage | DurÃ©e cible | Bloquant si Ã©chec |
|-------|-------------|-------------------|
| Lint + TypeCheck | < 2 min | Oui |
| Unit Tests | < 5 min | Oui |
| Integration Tests | < 10 min | Oui |
| E2E (smoke) | < 10 min | Oui |
| E2E (full) | < 30 min | Oui (pre-release) |
| Visual Regression | < 15 min | Non (review) |

---

## 5.8 Test Environment Management

### Niveau 1 - Vulgarisation

La **gestion des environnements de test** assure que vous avez des environnements **stables, isolÃ©s et reprÃ©sentatifs** pour exÃ©cuter vos tests.

**Pourquoi c'est important** : Un test qui passe en local mais Ã©choue en CI (ou vice versa) est un cauchemar. Les environnements doivent Ãªtre cohÃ©rents et contrÃ´lÃ©s.

### Niveau 2 - Approfondissement Expert

#### Types d'environnements

| Environnement | Usage | CaractÃ©ristiques |
|---------------|-------|------------------|
| **Local** | DÃ©veloppement | Machine du dev, DB locale |
| **CI** | Tests automatisÃ©s | Ã‰phÃ©mÃ¨re, containers |
| **Staging** | Tests manuels, UAT | Proche prod, donnÃ©es test |
| **Pre-prod** | Validation finale | Identique prod, donnÃ©es prod anonymisÃ©es |
| **Production** | Live | Vraies donnÃ©es, vrais users |

#### Infrastructure as Code pour les tests

**Docker Compose** pour environnement local/CI :

```yaml
# docker-compose.test.yml
version: '3.8'

services:
  app:
    build: .
    environment:
      - NODE_ENV=test
      - DATABASE_URL=postgresql://test:test@db:5432/testdb
      - REDIS_URL=redis://cache:6379
    depends_on:
      - db
      - cache

  db:
    image: postgres:15
    environment:
      POSTGRES_USER: test
      POSTGRES_PASSWORD: test
      POSTGRES_DB: testdb
    tmpfs:
      - /var/lib/postgresql/data  # RAM pour rapiditÃ©

  cache:
    image: redis:7

  mailhog:
    image: mailhog/mailhog
    ports:
      - "8025:8025"  # UI pour voir les emails
```

#### Testcontainers

Librairie pour crÃ©er des containers Ã  la volÃ©e dans les tests :

```javascript
// setup.js (Jest)
const { PostgreSqlContainer } = require('@testcontainers/postgresql');

let container;

beforeAll(async () => {
  container = await new PostgreSqlContainer()
    .withDatabase('testdb')
    .start();

  process.env.DATABASE_URL = container.getConnectionUri();
});

afterAll(async () => {
  await container.stop();
});
```

#### Gestion des donnÃ©es d'environnement

| StratÃ©gie | Description | Usage |
|-----------|-------------|-------|
| **Reset complet** | Vider et reseed entre les tests | Isolation totale |
| **Transaction rollback** | Rollback aprÃ¨s chaque test | Rapide, propre |
| **Fixtures par test** | Chaque test crÃ©e ses donnÃ©es | IsolÃ© mais plus lent |
| **Snapshot** | Restaurer un snapshot DB | DonnÃ©es rÃ©alistes |

### Niveau 3 - Application Pratique

#### Matrice d'environnements e-commerce

```
ENVIRONNEMENTS DE TEST

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                â”‚   LOCAL      â”‚     CI       â”‚   STAGING    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Database       â”‚ Docker PG    â”‚ Testcontainerâ”‚ RDS (dÃ©diÃ©)  â”‚
â”‚ Cache          â”‚ Docker Redis â”‚ Docker Redis â”‚ ElastiCache  â”‚
â”‚ Search         â”‚ Docker ES    â”‚ Mock         â”‚ OpenSearch   â”‚
â”‚ Payments       â”‚ Sandbox      â”‚ Mock         â”‚ Sandbox      â”‚
â”‚ Emails         â”‚ Mailhog      â”‚ Mock         â”‚ SES Sandbox  â”‚
â”‚ Files          â”‚ Local folder â”‚ Memory       â”‚ S3 (dÃ©diÃ©)   â”‚
â”‚ External APIs  â”‚ Mock/Sandbox â”‚ Mock         â”‚ Sandbox      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Data           â”‚ Fixtures     â”‚ Factories    â”‚ Anonymized   â”‚
â”‚ Refresh        â”‚ On demand    â”‚ Each run     â”‚ Nightly      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Section 6 : User Acceptance Testing (UAT)

---

## 6.1 UAT Planning et Execution

### Niveau 1 - Vulgarisation

Les **tests d'acceptation utilisateur (UAT)** sont rÃ©alisÃ©s par les **utilisateurs finaux ou leurs reprÃ©sentants** pour vÃ©rifier que le systÃ¨me rÃ©pond Ã  leurs besoins rÃ©els. C'est la validation finale avant le Go-Live.

**Analogie** : Vous avez fait construire une maison. L'architecte a validÃ© les plans, le constructeur a vÃ©rifiÃ© la structure. L'UAT, c'est quand VOUS visitez la maison pour vÃ©rifier qu'elle correspond Ã  ce que vous vouliez.

**Pourquoi c'est important** : Les dÃ©veloppeurs et QA peuvent passer Ã  cÃ´tÃ© de problÃ¨mes Ã©vidents pour un utilisateur mÃ©tier. L'UAT capture ces Ã©carts.

### Niveau 2 - Approfondissement Expert

#### DÃ©finition (ISTQB)

> "Acceptance testing: Formal testing with respect to user needs, requirements, and business processes conducted to determine whether a system satisfies the acceptance criteria."

#### Types d'Acceptance Testing

| Type | Description | Qui |
|------|-------------|-----|
| **User Acceptance Testing** | Validation par utilisateurs finaux | End users, business |
| **Business Acceptance Testing** | Validation des processus mÃ©tier | Business analysts |
| **Contract Acceptance Testing** | ConformitÃ© au contrat | Client/fournisseur |
| **Regulation Acceptance Testing** | ConformitÃ© rÃ©glementaire | Compliance, legal |
| **Alpha Testing** | Tests internes avant beta | Ã‰quipe interne |
| **Beta Testing** | Tests par utilisateurs externes | Users sÃ©lectionnÃ©s |

#### Planning UAT

**Ã‰lÃ©ments Ã  dÃ©finir** :

1. **Scope** : Quelles fonctionnalitÃ©s sont testÃ©es ?
2. **Participants** : Qui teste ? (reprÃ©sentatifs des personas)
3. **Environnement** : Staging/Pre-prod configurÃ©
4. **DonnÃ©es** : DonnÃ©es rÃ©alistes (anonymisÃ©es si prod)
5. **ScÃ©narios** : Cas de test dÃ©rivÃ©s des critÃ¨res d'acceptation
6. **Timeline** : DurÃ©e, jalons, deadlines
7. **CritÃ¨res de succÃ¨s** : DÃ©finition du "pass"
8. **Process de feedback** : Comment remonter les problÃ¨mes

#### Template de scÃ©nario UAT

```
SCÃ‰NARIO UAT : Commande avec code promo

ID: UAT-CHECKOUT-003
Feature: Tunnel d'achat
User Story: US-042 - En tant que client, je veux utiliser un code promo

PRÃ‰CONDITIONS:
- Utilisateur connectÃ© avec compte test
- Panier contenant 2 articles (total > 50â‚¬)
- Code promo WELCOME10 actif (-10%)

Ã‰TAPES:
1. Aller au panier
2. Cliquer sur "J'ai un code promo"
3. Saisir "WELCOME10"
4. Cliquer sur "Appliquer"
5. VÃ©rifier le rÃ©capitulatif
6. ProcÃ©der au paiement
7. VÃ©rifier le montant dÃ©bitÃ©

RÃ‰SULTAT ATTENDU:
- RÃ©duction de 10% appliquÃ©e et visible
- Total recalculÃ© correctement
- Code affichÃ© dans le rÃ©capitulatif
- Confirmation email mentionne la rÃ©duction

RÃ‰SULTAT OBTENU:
[Ã€ complÃ©ter par le testeur]

STATUT: [ ] PASS  [ ] FAIL  [ ] BLOQUÃ‰

COMMENTAIRES:
[Observations, captures d'Ã©cran, etc.]
```

#### MÃ©triques UAT

| MÃ©trique | Description | Seuil typique |
|----------|-------------|---------------|
| Test Pass Rate | % scÃ©narios rÃ©ussis | > 95% |
| Defect Discovery Rate | Bugs trouvÃ©s / scÃ©narios | Informatif |
| Critical Defects | Bugs bloquants | 0 pour Go-Live |
| User Satisfaction | Feedback qualitatif | Positif majoritaire |

### Niveau 3 - Application Pratique

#### Organisation UAT e-commerce

```
PLANNING UAT - REFONTE CHECKOUT

SEMAINE 1 : PrÃ©paration
â”œâ”€â”€ Lundi: Formation testeurs, accÃ¨s environnement
â”œâ”€â”€ Mardi: Walkthrough des scÃ©narios
â”œâ”€â”€ Mercredi: VÃ©rification donnÃ©es test
â””â”€â”€ Jeudi-Vendredi: Buffer

SEMAINE 2 : ExÃ©cution
â”œâ”€â”€ Lundi-Mardi: Parcours achat standard
â”œâ”€â”€ Mercredi: Cas particuliers (promos, multi-adresses)
â”œâ”€â”€ Jeudi: Paiements (CB, PayPal, 3x)
â””â”€â”€ Vendredi: Retours et corrections critiques

SEMAINE 3 : Finalisation
â”œâ”€â”€ Lundi-Mardi: Re-test des corrections
â”œâ”€â”€ Mercredi: Session de feedback
â”œâ”€â”€ Jeudi: RÃ©daction rapport UAT
â””â”€â”€ Vendredi: Sign-off meeting

PARTICIPANTS:
- 2 reprÃ©sentants Service Client
- 1 responsable E-commerce
- 1 responsable Logistique
- Product Owner (facilitation)
```

---

## 6.2 Alpha vs Beta Testing

### Niveau 1 - Vulgarisation

- **Alpha Testing** : Tests par l'Ã©quipe **interne** avant de montrer aux utilisateurs externes
- **Beta Testing** : Tests par des **utilisateurs rÃ©els sÃ©lectionnÃ©s** avant le lancement public

**Analogie restaurant** :
- Alpha = Le chef et l'Ã©quipe goÃ»tent les plats
- Beta = On invite quelques clients fidÃ¨les pour une soirÃ©e de test

### Niveau 2 - Approfondissement Expert

| Aspect | Alpha Testing | Beta Testing |
|--------|---------------|--------------|
| **Qui** | Ã‰quipe interne, employÃ©s | Utilisateurs externes sÃ©lectionnÃ©s |
| **OÃ¹** | Environnement contrÃ´lÃ© | Environnement rÃ©el de l'utilisateur |
| **Quand** | Avant beta | Avant release publique |
| **Objectif** | Trouver bugs majeurs | Feedback rÃ©el, usabilitÃ© |
| **Feedback** | Direct, dÃ©taillÃ© | Variable, rÃ©aliste |
| **ContrÃ´le** | Ã‰levÃ© | LimitÃ© |

#### Types de Beta Testing

| Type | Description |
|------|-------------|
| **Closed Beta** | Groupe restreint d'utilisateurs invitÃ©s |
| **Open Beta** | Ouvert Ã  tous (inscription) |
| **Technical Beta** | Focus sur la stabilitÃ© technique |
| **Marketing Beta** | Focus sur le feedback produit |

### Niveau 3 - Application Pratique

#### Programme Beta e-commerce

```
BETA TESTING PROGRAM

OBJECTIFS:
- Valider l'UX du nouveau checkout
- Identifier les bugs edge cases
- Collecter feedback utilisateurs
- Tester la charge rÃ©elle (soft launch)

PARTICIPANTS (Closed Beta):
- 500 clients fidÃ¨les (> 5 commandes/an)
- Mix dÃ©mographique reprÃ©sentatif
- Opt-in avec incentive (10% sur prochaine commande)

DURÃ‰E: 2 semaines

MÃ‰CANISME:
- Feature flag activÃ© pour beta users
- Feedback widget intÃ©grÃ©
- Hotline support dÃ©diÃ©e
- Analytics sÃ©parÃ©s

CRITÃˆRES DE SORTIE:
- < 5 bugs majeurs non rÃ©solus
- NPS beta > 30
- Taux de conversion >= version actuelle
- Aucun incident sÃ©curitÃ©/paiement
```

---

## 6.3 Acceptance Criteria Verification

### Niveau 1 - Vulgarisation

Les **critÃ¨res d'acceptation** dÃ©finissent **quand une fonctionnalitÃ© est "terminÃ©e"**. L'UAT vÃ©rifie que ces critÃ¨res sont respectÃ©s.

**Exemple** :
- User Story : "En tant que client, je veux filtrer les produits par taille"
- CritÃ¨re d'acceptation : "Quand je sÃ©lectionne 'Taille M', seuls les produits disponibles en M s'affichent"

### Niveau 2 - Approfondissement Expert

#### Format des critÃ¨res d'acceptation

**Gherkin (Given-When-Then)** :
```gherkin
Feature: Product Filtering

  Scenario: Filter by size
    Given I am on the product listing page
    And there are products available in sizes S, M, L
    When I select filter "Size: M"
    Then I should see only products available in size M
    And the filter "Size: M" should be displayed as active
    And the product count should update

  Scenario: Combine multiple filters
    Given I am on the product listing page
    When I select filter "Size: M"
    And I select filter "Color: Blue"
    Then I should see only products that are both size M and blue
```

#### Traceability Matrix

| Req ID | User Story | Acceptance Criteria | Test Cases | UAT Status |
|--------|------------|---------------------|------------|------------|
| REQ-101 | Filter by size | AC-101-1, AC-101-2 | TC-201, TC-202 | âœ… Pass |
| REQ-102 | Filter by color | AC-102-1 | TC-203 | âœ… Pass |
| REQ-103 | Filter by price | AC-103-1, AC-103-2 | TC-204, TC-205 | âš ï¸ Partial |
| REQ-104 | Save filters | AC-104-1 | TC-206 | âŒ Fail |

### Niveau 3 - Application Pratique

**Checklist de vÃ©rification** :
- [ ] Chaque User Story a des critÃ¨res d'acceptation
- [ ] Chaque critÃ¨re est testable (vÃ©rifiable objectivement)
- [ ] Chaque critÃ¨re a au moins un cas de test UAT
- [ ] Tous les critÃ¨res sont tracÃ©s dans la matrice
- [ ] Le statut est mis Ã  jour aprÃ¨s chaque test

---

## 6.4 UAT Environments

### Niveau 1 - Vulgarisation

L'**environnement UAT** doit Ãªtre **le plus proche possible de la production** pour que les tests soient reprÃ©sentatifs.

### Niveau 2 - Approfondissement Expert

#### CaractÃ©ristiques de l'environnement UAT

| Aspect | Recommandation |
|--------|----------------|
| **Infrastructure** | MÃªme architecture que prod (scaled down OK) |
| **DonnÃ©es** | DonnÃ©es rÃ©alistes, anonymisÃ©es si sensibles |
| **IntÃ©grations** | Vraies intÃ©grations (sandbox pour paiements) |
| **Performance** | Suffisante pour les tests (pas besoin de prod-scale) |
| **AccÃ¨s** | Restreint aux testeurs UAT |
| **Refresh** | RafraÃ®chissement rÃ©gulier des donnÃ©es |
| **Monitoring** | Logs accessibles pour debug |

#### DonnÃ©es UAT pour e-commerce

```
DONNÃ‰ES UAT

CATALOGUE:
- 1000 produits (subset reprÃ©sentatif)
- Toutes les catÃ©gories
- Mix de stocks (dispo, low stock, rupture)
- Prix variÃ©s (promos, soldes)

UTILISATEURS:
- 50 comptes test avec diffÃ©rents profils
- Historique de commandes
- Adresses variÃ©es (France, DOM-TOM, Europe)
- Moyens de paiement sandbox

COMMANDES:
- Historique de commandes (tous statuts)
- Retours et remboursements
- Litiges

CONFIGURATION:
- Frais de port rÃ©els
- Promotions actives
- RÃ¨gles de livraison
```

---

## 6.5 Sign-off Process

### Niveau 1 - Vulgarisation

Le **sign-off** est la **validation formelle** que l'UAT est terminÃ© et que le systÃ¨me est acceptÃ© pour la production. C'est comme signer le bon de livraison.

### Niveau 2 - Approfondissement Expert

#### Processus de sign-off

```
UAT SIGN-OFF PROCESS

1. COMPILATION DES RÃ‰SULTATS
   â””â”€â”€ Tous les scÃ©narios exÃ©cutÃ©s
   â””â”€â”€ DÃ©fauts documentÃ©s avec statut
   â””â”€â”€ MÃ©triques calculÃ©es

2. REVIEW MEETING
   â””â”€â”€ PrÃ©sentation des rÃ©sultats
   â””â”€â”€ Discussion des dÃ©fauts ouverts
   â””â”€â”€ DÃ©cision sur les risques rÃ©siduels

3. DÃ‰CISION
   â””â”€â”€ GO : UAT acceptÃ©, release approuvÃ©e
   â””â”€â”€ CONDITIONAL GO : Avec rÃ©serves documentÃ©es
   â””â”€â”€ NO-GO : Corrections requises, re-test

4. FORMALISATION
   â””â”€â”€ Document de sign-off signÃ©
   â””â”€â”€ Liste des known issues
   â””â”€â”€ Plan de mitigation si conditional

5. ARCHIVAGE
   â””â”€â”€ Rapport UAT complet
   â””â”€â”€ Evidence des tests
   â””â”€â”€ Signatures
```

#### Template de Sign-off

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    UAT SIGN-OFF DOCUMENT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Projet: Refonte Checkout v2.0
Date: 15 janvier 2025
Version testÃ©e: 2.0.0-rc1

RÃ‰SUMÃ‰ EXÃ‰CUTIF
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ScÃ©narios exÃ©cutÃ©s: 45/45 (100%)
ScÃ©narios rÃ©ussis: 43/45 (95.5%)
ScÃ©narios Ã©chouÃ©s: 2/45 (4.5%)

DÃ©fauts trouvÃ©s: 12
- Critiques: 0
- Majeurs: 2 (rÃ©solus)
- Mineurs: 7 (5 rÃ©solus, 2 acceptÃ©s)
- CosmÃ©tiques: 3 (acceptÃ©s, backlog)

DÃ‰CISION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[X] GO - Release approuvÃ©e
[ ] CONDITIONAL GO - Avec rÃ©serves
[ ] NO-GO - Corrections requises

RÃ‰SERVES / KNOWN ISSUES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- BUG-234: Affichage prix arrondi sur mobile (mineur)
- BUG-237: Lenteur filtre pays DOM-TOM (cosmÃ©tique)

APPROBATIONS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Product Owner:     _________________ Date: __/__/____
Responsable QA:    _________________ Date: __/__/____
Sponsor Business:  _________________ Date: __/__/____
```

---

## 6.6 Feedback Collection et Triage

### Niveau 1 - Vulgarisation

Pendant l'UAT, les testeurs remontent des **feedbacks** (bugs, suggestions, questions). Il faut les **collecter efficacement** et les **trier** pour dÃ©cider quoi faire.

### Niveau 2 - Approfondissement Expert

#### Canaux de feedback

| Canal | Usage | Avantage |
|-------|-------|----------|
| **Bug tracker** (Jira, Linear) | Bugs formels | TraÃ§abilitÃ©, workflow |
| **Formulaire intÃ©grÃ©** | Feedback rapide in-app | Contexte automatique |
| **Slack/Teams channel** | Questions rapides | RÃ©activitÃ© |
| **Session de debrief** | Feedback qualitatif | Discussion, nuances |

#### Process de triage

```
FEEDBACK TRIAGE

1. CATÃ‰GORISATION
   â”œâ”€â”€ Bug (dÃ©faut)
   â”œâ”€â”€ Enhancement (amÃ©lioration)
   â”œâ”€â”€ Question (clarification)
   â””â”€â”€ Out of scope (hors pÃ©rimÃ¨tre)

2. PRIORISATION (si bug)
   â”œâ”€â”€ Severity (impact technique)
   â””â”€â”€ Priority (urgence business)

3. DÃ‰CISION
   â”œâ”€â”€ Fix now (bloquant pour UAT)
   â”œâ”€â”€ Fix before release
   â”œâ”€â”€ Fix after release (known issue)
   â””â”€â”€ Won't fix (acceptÃ© ou hors scope)

4. COMMUNICATION
   â””â”€â”€ Informer le rapporteur de la dÃ©cision
```

---

## 6.7 UAT dans un contexte e-commerce

### Niveau 1 - Vulgarisation

L'UAT e-commerce a des **spÃ©cificitÃ©s** : tester les paiements (sans vraie transaction), vÃ©rifier la logistique, simuler les pics de charge.

### Niveau 2 - Approfondissement Expert

#### ScÃ©narios UAT critiques e-commerce

**Tunnel d'achat**
- [ ] Achat guest (sans compte)
- [ ] Achat client connectÃ©
- [ ] Achat avec code promo
- [ ] Achat avec points fidÃ©litÃ©
- [ ] Multi-adresses (livraison â‰  facturation)
- [ ] Click & Collect
- [ ] Livraison express

**Paiements**
- [ ] Carte bancaire (Visa, Mastercard, Amex)
- [ ] PayPal
- [ ] Apple Pay / Google Pay
- [ ] Paiement en 3x/4x
- [ ] Carte cadeau
- [ ] Ã‰chec paiement et retry
- [ ] 3D Secure

**Gestion commande**
- [ ] Suivi de commande
- [ ] Modification commande (si possible)
- [ ] Annulation commande
- [ ] Retour produit
- [ ] Remboursement

**Cas particuliers**
- [ ] Produit en rupture pendant checkout
- [ ] Promotion expirÃ©e pendant checkout
- [ ] Session timeout
- [ ] Multi-devises (si applicable)
- [ ] TVA intra-EU / export

### Niveau 3 - Application Pratique

#### Environnement de test paiement

| Provider | Mode Test |
|----------|-----------|
| Stripe | Test mode avec cartes test (4242...) |
| PayPal | Sandbox accounts |
| Adyen | Test environment |
| Alma (3x) | Sandbox |

**Cartes de test Stripe** :
- `4242 4242 4242 4242` : SuccÃ¨s
- `4000 0000 0000 0002` : RefusÃ©e
- `4000 0027 6000 3184` : Requiert 3DS

---

# Section 7 : Gestion des Bugs

---

## 7.1 Bug Lifecycle

### Niveau 1 - Vulgarisation

Un bug suit un **cycle de vie** depuis sa dÃ©couverte jusqu'Ã  sa rÃ©solution (ou son rejet). Comprendre ce cycle permet de suivre efficacement l'Ã©tat de chaque dÃ©faut.

**Analogie** : C'est comme le suivi d'un colis : commandÃ© â†’ expÃ©diÃ© â†’ en transit â†’ livrÃ©. Chaque Ã©tape a un statut clair.

### Niveau 2 - Approfondissement Expert

#### Cycle de vie standard

```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   NEW   â”‚ â—„â”€â”€ Bug dÃ©couvert et rapportÃ©
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  OPEN   â”‚ â—„â”€â”€ Bug confirmÃ©, assignÃ©
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                  â”‚
    â–¼                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚IN PROGRESSâ”‚                    â”‚ REJECTED â”‚ â—„â”€â”€ Non reproductible,
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     ou "by design"
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FIXED  â”‚ â—„â”€â”€ Correction implÃ©mentÃ©e
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚IN REVIEWâ”‚ â—„â”€â”€ Code review / QA review
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚VERIFIED â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ REOPENED â”‚ â—„â”€â”€ Fix incorrect
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                     â”‚
     â”‚                     â”‚
     â–¼                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚ CLOSED  â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Ã‰tats possibles

| Ã‰tat | Description | Responsable |
|------|-------------|-------------|
| **New** | Vient d'Ãªtre crÃ©Ã© | Rapporteur |
| **Open** | ConfirmÃ©, en attente d'assignation | QA Lead |
| **Assigned** | AssignÃ© Ã  un dÃ©veloppeur | Dev assignÃ© |
| **In Progress** | DÃ©veloppeur travaille dessus | Dev |
| **Fixed** | Correction commitÃ©e | Dev |
| **In Review** | En code review | Reviewer |
| **Ready for QA** | PrÃªt pour vÃ©rification | QA |
| **Verified** | Fix vÃ©rifiÃ© fonctionnel | QA |
| **Closed** | RÃ©solu dÃ©finitivement | QA/PM |
| **Reopened** | Fix incorrect, bug persiste | QA |
| **Rejected** | Pas un bug (by design, duplicate, etc.) | QA/Dev |
| **Deferred** | ReportÃ© Ã  une version future | PM |
| **Won't Fix** | Ne sera pas corrigÃ© (acceptÃ©) | PM |

### Niveau 3 - Application Pratique

#### Configuration Jira typique

```
WORKFLOW JIRA - BUG

Transitions autorisÃ©es :
- New â†’ Open (triage)
- Open â†’ In Progress | Rejected | Deferred
- In Progress â†’ Fixed | Won't Fix
- Fixed â†’ In Review
- In Review â†’ Ready for QA | In Progress (fix needed)
- Ready for QA â†’ Verified | Reopened
- Verified â†’ Closed
- Reopened â†’ In Progress
- Deferred â†’ Open (repriorisation)

Champs obligatoires par transition :
- Open â†’ In Progress : Assignee
- In Progress â†’ Fixed : Fix Version, Commit Link
- Ready for QA â†’ Verified : Test Evidence
- * â†’ Rejected : Reason
```

---

## 7.2 Bug Report Writing

### Niveau 1 - Vulgarisation

Un **bon rapport de bug** permet au dÃ©veloppeur de **comprendre et reproduire le problÃ¨me** rapidement. Un mauvais rapport fait perdre du temps Ã  tout le monde.

**RÃ¨gle d'or** : Imaginez que quelqu'un qui n'a jamais vu l'application doit reproduire le bug. Donnez-lui TOUTES les informations nÃ©cessaires.

### Niveau 2 - Approfondissement Expert

#### Structure d'un bug report

| Champ | Description | Exemple |
|-------|-------------|---------|
| **Title** | RÃ©sumÃ© clair et concis | "[Checkout] Erreur 500 lors du paiement PayPal sur mobile" |
| **Environment** | OÃ¹ le bug se produit | Staging, Chrome 120, iPhone 14 |
| **Preconditions** | Ã‰tat initial requis | Utilisateur connectÃ©, panier avec 2 articles |
| **Steps to Reproduce** | Ã‰tapes exactes | 1. Aller au panier 2. Cliquer "Payer" 3. Choisir PayPal... |
| **Expected Result** | Ce qui devrait se passer | Redirection vers PayPal |
| **Actual Result** | Ce qui se passe rÃ©ellement | Page erreur 500 |
| **Severity** | Impact technique | Critical / Major / Minor / Trivial |
| **Priority** | Urgence business | P1 / P2 / P3 / P4 |
| **Attachments** | Preuves | Screenshots, vidÃ©os, logs |
| **Workaround** | Solution temporaire | Utiliser CB au lieu de PayPal |

#### Template de bug report

```markdown
## [Checkout] Erreur 500 lors du paiement PayPal sur mobile

### Environment
- **URL**: https://staging.example.com
- **Browser**: Chrome 120.0.6099.109 (Mobile)
- **Device**: iPhone 14 Pro (iOS 17.2)
- **User**: test-user-42@example.com

### Preconditions
- Utilisateur connectÃ©
- Panier contenant au moins 1 article
- Adresse de livraison renseignÃ©e

### Steps to Reproduce
1. AccÃ©der au panier (`/cart`)
2. Cliquer sur "ProcÃ©der au paiement"
3. Remplir les informations de livraison
4. SÃ©lectionner "PayPal" comme moyen de paiement
5. Cliquer sur "Payer avec PayPal"

### Expected Result
- Redirection vers la page de connexion PayPal
- PossibilitÃ© de finaliser le paiement

### Actual Result
- Page d'erreur 500 affichÃ©e
- Message : "Une erreur est survenue. Veuillez rÃ©essayer."
- Aucune redirection vers PayPal

### Severity: Critical
### Priority: P1

### Attachments
- Screenshot: [error-500-paypal.png]
- Video: [reproduction-bug.mp4]
- Console logs: [console-errors.txt]

### Additional Information
- Le bug ne se produit PAS sur desktop Chrome
- Le bug se produit aussi sur Safari mobile
- PremiÃ¨re occurrence : 12/01/2025 14h30
- FrÃ©quence : 100% reproductible

### Workaround
Utiliser le paiement par carte bancaire ou accÃ©der au site via desktop.
```

#### Bonnes pratiques

**DO** :
- Un bug = un ticket (pas de combo)
- Reproduire avant de reporter
- Inclure les logs/stack traces
- Tester le workaround suggÃ©rÃ©

**DON'T** :
- "Ã‡a marche pas" (trop vague)
- Supposer la cause ("C'est un bug JavaScript")
- Reporter des duplicates (chercher d'abord)
- Oublier les infos d'environnement

### Niveau 3 - Application Pratique

#### Outils de capture

| Outil | Usage |
|-------|-------|
| **Loom** | VidÃ©os de reproduction |
| **Jam** | Capture enrichie (console, network) |
| **Marker.io** | Annotation screenshots |
| **BugHerd** | Feedback visuel in-page |

---

## 7.3 Severity vs Priority

### Niveau 1 - Vulgarisation

- **Severity** (SÃ©vÃ©ritÃ©) : Ã€ quel point le bug est **techniquement grave** ?
- **Priority** (PrioritÃ©) : Ã€ quel point c'est **urgent business** de le corriger ?

**Exemple** : Une faute d'orthographe sur la homepage (severity: trivial, priority: haute car visible par tous). Un crash dans une feature que personne n'utilise (severity: critical, priority: basse).

### Niveau 2 - Approfondissement Expert

#### DÃ©finitions

**Severity (impact technique)**

| Niveau | DÃ©finition | Exemple |
|--------|------------|---------|
| **Critical** | SystÃ¨me inutilisable, perte de donnÃ©es, sÃ©curitÃ© | Crash app, faille SQL injection |
| **Major** | Fonction majeure non fonctionnelle, pas de workaround | Impossible de payer |
| **Minor** | Fonction impactÃ©e mais workaround existe | Filtre cassÃ©, mais recherche fonctionne |
| **Trivial** | ProblÃ¨me cosmÃ©tique | Typo, alignement pixel |

**Priority (urgence business)**

| Niveau | DÃ©finition | Action |
|--------|------------|--------|
| **P1 - Critical** | Blocker release, impact business immÃ©diat | Fix immÃ©diat (heures) |
| **P2 - High** | Impact significatif, pas de workaround viable | Fix ce sprint |
| **P3 - Medium** | Impact modÃ©rÃ©, workaround acceptable | Planifier prochains sprints |
| **P4 - Low** | Impact faible | Backlog, si le temps permet |

#### Matrice Severity Ã— Priority

| | P1 (Now) | P2 (Sprint) | P3 (Soon) | P4 (Later) |
|---|----------|-------------|-----------|------------|
| **Critical** | ğŸ”´ HOTFIX | ğŸ”´ Urgent | ğŸŸ  Plan | ğŸŸ¡ Review |
| **Major** | ğŸ”´ Urgent | ğŸŸ  High | ğŸŸ¡ Normal | ğŸŸ¢ Low |
| **Minor** | ğŸŸ  High | ğŸŸ¡ Normal | ğŸŸ¢ Low | ğŸŸ¢ Backlog |
| **Trivial** | ğŸŸ¡ Normal | ğŸŸ¢ Low | ğŸŸ¢ Backlog | ğŸ”µ Optional |

**Cas spÃ©ciaux** :
- Severity Critical + Priority Low = Rare mais possible (feature non utilisÃ©e)
- Severity Trivial + Priority High = Visible par tous (homepage)

### Niveau 3 - Application Pratique

#### Qui dÃ©cide ?

| Attribut | DÃ©cideur | CritÃ¨re |
|----------|----------|---------|
| Severity | QA / Tech Lead | Impact technique objectif |
| Priority | Product Owner / Business | Valeur business, risque |

**Processus** : QA assigne la severity, PO assigne la priority pendant le triage.

---

## 7.4 Bug Triage Meetings

### Niveau 1 - Vulgarisation

Le **bug triage** est une rÃ©union rÃ©guliÃ¨re pour **passer en revue les nouveaux bugs** et dÃ©cider quoi en faire : corriger, reporter, rejeter.

**Analogie** : C'est comme le tri aux urgences d'un hÃ´pital. On Ã©value rapidement chaque patient pour dÃ©cider qui passe en premier.

### Niveau 2 - Approfondissement Expert

#### Format type

```
BUG TRIAGE MEETING

FrÃ©quence: 2-3x par semaine (ou daily si release proche)
DurÃ©e: 30 minutes max
Participants: QA Lead, Tech Lead, Product Owner, Dev reprÃ©sentant

Agenda:
1. Review nouveaux bugs (5 min max par bug)
   - Confirmer la reproduction
   - Assigner severity
   - Assigner priority
   - DÃ©cider : Fix / Defer / Reject

2. Review bugs bloquants en cours (5 min)
   - Statut des corrections
   - Blockers ?

3. MÃ©triques (2 min)
   - Bugs ouverts par severity
   - Tendances

RÃ¨gles:
- Time-boxed: Si pas de dÃ©cision en 5 min â†’ hors-meeting
- Pas de debugging en sÃ©ance
- DÃ©cisions documentÃ©es immÃ©diatement
```

#### DÃ©cisions possibles

| DÃ©cision | Quand | Action |
|----------|-------|--------|
| **Fix** | Bug confirmÃ©, prioritÃ© suffisante | Assigner, planifier |
| **Defer** | Pas critique, pas le moment | Mettre en backlog |
| **Reject** | Pas un bug (by design, duplicate) | Fermer avec raison |
| **Need Info** | Manque d'informations | Renvoyer au rapporteur |
| **Investigate** | Cause incertaine | Assigner investigation |

### Niveau 3 - Application Pratique

#### Dashboard de triage

```
BUG DASHBOARD - 15 Janvier 2025

NOUVEAUX (Ã  trier): 8
â”œâ”€â”€ Critical: 1 ğŸ”´
â”œâ”€â”€ Major: 3
â”œâ”€â”€ Minor: 2
â””â”€â”€ Unclassified: 2

OUVERTS PAR PRIORITY:
â”œâ”€â”€ P1: 2 (objectif: 0)
â”œâ”€â”€ P2: 5 (objectif: < 5)
â”œâ”€â”€ P3: 12
â””â”€â”€ P4: 23

TENDANCE (7 jours):
â”œâ”€â”€ CrÃ©Ã©s: 15
â”œâ”€â”€ FermÃ©s: 18
â””â”€â”€ Net: -3 âœ…

BUGS CRITIQUES OUVERTS:
1. BUG-456: Paiement Ã©choue alÃ©atoirement (P1, assigned: @dev1, ETA: today)
2. BUG-461: Fuite mÃ©moire checkout (P1, assigned: @dev2, ETA: tomorrow)
```

---

## 7.5 Root Cause Analysis

### Niveau 1 - Vulgarisation

La **Root Cause Analysis (RCA)** cherche Ã  comprendre **pourquoi** un bug s'est produit, pas juste Ã  le corriger. L'objectif : Ã©viter que le mÃªme type de bug se reproduise.

**Analogie** : Si votre voiture tombe en panne, rÃ©parer le symptÃ´me (batterie morte) ne suffit pas. Il faut comprendre pourquoi (alternateur dÃ©faillant) pour Ã©viter la rÃ©cidive.

### Niveau 2 - Approfondissement Expert

#### Techniques de RCA

**5 Whys (5 Pourquoi)**

Poser "Pourquoi ?" 5 fois pour remonter Ã  la cause racine.

```
Bug: Les emails de confirmation ne sont pas envoyÃ©s

Pourquoi 1: Le service d'email a crashÃ©
  Pourquoi 2: Il a manquÃ© de mÃ©moire
    Pourquoi 3: Une boucle infinie crÃ©ait des objets
      Pourquoi 4: Une condition de sortie Ã©tait incorrecte
        Pourquoi 5: Le code review n'a pas dÃ©tectÃ© l'erreur

Cause racine: Process de code review insuffisant pour ce type de code
Action: Ajouter checklist spÃ©cifique pour les boucles/rÃ©cursions
```

**Ishikawa (Fishbone Diagram)**

CatÃ©gories de causes :
- **People** : Formation, erreur humaine
- **Process** : MÃ©thodologie, workflow
- **Tools** : Outils dÃ©faillants
- **Environment** : Infra, configuration
- **Materials** : DonnÃ©es, inputs
- **Measurement** : Monitoring, alerting

#### Quand faire une RCA

| Situation | RCA requise |
|-----------|-------------|
| Incident production majeur | Oui, obligatoire |
| Bug critique rÃ©current | Oui |
| Near-miss (Ã©vitÃ© de justesse) | RecommandÃ© |
| Bug mineur ponctuel | Non nÃ©cessaire |

### Niveau 3 - Application Pratique

#### Template RCA

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
              ROOT CAUSE ANALYSIS REPORT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

INCIDENT: Perte de commandes du 12 janvier 2025
IMPACT: 47 commandes non enregistrÃ©es, ~15,000â‚¬ de CA
DURÃ‰E: 2h30 (14h15 - 16h45)

TIMELINE:
- 14h15: PremiÃ¨res alertes monitoring (queue RabbitMQ)
- 14h30: Support signale plaintes clients
- 15h00: Investigation commence
- 15h45: Cause identifiÃ©e (disque full)
- 16h15: Fix dÃ©ployÃ© (cleanup + resize)
- 16h45: Service restaurÃ©

ROOT CAUSE ANALYSIS (5 Whys):
1. Pourquoi les commandes n'Ã©taient pas enregistrÃ©es ?
   â†’ Le worker de traitement des commandes Ã©tait arrÃªtÃ©

2. Pourquoi le worker Ã©tait arrÃªtÃ© ?
   â†’ Il a crashÃ© suite Ã  une erreur d'Ã©criture disque

3. Pourquoi l'Ã©criture disque a Ã©chouÃ© ?
   â†’ Le disque Ã©tait plein Ã  100%

4. Pourquoi le disque Ã©tait plein ?
   â†’ Les logs n'Ã©taient pas rotÃ©s depuis 3 mois

5. Pourquoi les logs n'Ã©taient pas rotÃ©s ?
   â†’ La tÃ¢che cron de rotation avait Ã©tÃ© dÃ©sactivÃ©e
      lors d'une maintenance et jamais rÃ©activÃ©e

CAUSE RACINE: Absence de vÃ©rification post-maintenance

ACTIONS CORRECTIVES:
1. [ImmÃ©diat] RÃ©activer la rotation de logs âœ…
2. [Court terme] Ajouter alerte sur espace disque > 80%
3. [Moyen terme] Checklist de fin de maintenance obligatoire
4. [Long terme] Revue des processus de maintenance

RESPONSABLE: @ops-lead
SUIVI: Weekly check pendant 1 mois
```

---

## 7.6 Defect Metrics

### Niveau 1 - Vulgarisation

Les **mÃ©triques de dÃ©fauts** permettent de **mesurer la qualitÃ©** du produit et du processus de test. Sans mÃ©triques, on navigue Ã  l'aveugle.

### Niveau 2 - Approfondissement Expert

#### MÃ©triques clÃ©s

| MÃ©trique | Formule | InterprÃ©tation |
|----------|---------|----------------|
| **Defect Density** | Bugs / KLOC ou Bugs / Story Points | QualitÃ© du code |
| **Defect Escape Rate** | Bugs prod / Bugs totaux | EfficacitÃ© des tests |
| **Defect Removal Efficiency** | Bugs prÃ©-prod / Bugs totaux | Idem, autre angle |
| **Mean Time to Resolution** | Î£(temps rÃ©solution) / N bugs | EfficacitÃ© Ã©quipe |
| **Defect Reopen Rate** | Bugs rÃ©ouverts / Bugs fermÃ©s | QualitÃ© des fixes |
| **Defect Age** | Jours depuis crÃ©ation | VÃ©locitÃ© de rÃ©solution |

#### Formules dÃ©taillÃ©es

**Defect Density** :
```
Defect Density = Nombre de dÃ©fauts / Taille du projet

Exemple:
- 45 bugs pour 15,000 lignes de code
- Density = 45 / 15 = 3 bugs / KLOC
- Benchmark: < 5 bugs/KLOC acceptable, < 1 excellent
```

**Defect Escape Rate** :
```
DER = Bugs trouvÃ©s en production / Total bugs trouvÃ©s Ã— 100

Exemple:
- 10 bugs trouvÃ©s en production
- 90 bugs trouvÃ©s avant production
- DER = 10 / 100 Ã— 100 = 10%
- Objectif: < 5%
```

**Defect Removal Efficiency** :
```
DRE = Bugs trouvÃ©s avant release / Total bugs Ã— 100

Exemple:
- DRE = 90 / 100 Ã— 100 = 90%
- Objectif: > 95%
```

#### Dashboard mÃ©triques

```
QUALITY METRICS DASHBOARD - Sprint 23

DEFECT DENSITY
â”œâ”€â”€ This sprint: 2.3 bugs/SP
â”œâ”€â”€ Average: 2.8 bugs/SP
â””â”€â”€ Trend: â†“ Improving

DEFECT ESCAPE RATE
â”œâ”€â”€ This quarter: 6.2%
â”œâ”€â”€ Previous: 8.5%
â””â”€â”€ Target: < 5%

MTTR (Mean Time to Resolution)
â”œâ”€â”€ Critical: 4h (target: < 8h) âœ…
â”œâ”€â”€ Major: 2.1 days (target: < 3d) âœ…
â”œâ”€â”€ Minor: 5.2 days (target: < 7d) âœ…
â””â”€â”€ Overall: 3.4 days

DEFECT BY STATUS
â”œâ”€â”€ Open: 23
â”œâ”€â”€ In Progress: 8
â”œâ”€â”€ Fixed (awaiting QA): 5
â””â”€â”€ Closed this sprint: 31

DEFECT BY ROOT CAUSE
â”œâ”€â”€ Logic error: 35%
â”œâ”€â”€ Edge case missed: 25%
â”œâ”€â”€ Integration issue: 20%
â”œâ”€â”€ UI/UX: 12%
â””â”€â”€ Other: 8%
```

### Niveau 3 - Application Pratique

#### Utilisation des mÃ©triques

| MÃ©trique | UtilisÃ© pour |
|----------|--------------|
| Defect Density | Ã‰valuer la qualitÃ© du code, identifier hotspots |
| Escape Rate | Ã‰valuer l'efficacitÃ© des tests |
| MTTR | Planification des capacitÃ©s |
| Reopen Rate | QualitÃ© des revues de code |
| Root Cause Distribution | AmÃ©lioration des processus |

---

# Section 8 : DÃ©cision Go/No-Go

---

## 8.1 Release Criteria et Exit Criteria

### Niveau 1 - Vulgarisation

Les **exit criteria** dÃ©finissent **quand on peut considÃ©rer la phase de test terminÃ©e**. Les **release criteria** dÃ©finissent **quand le produit est prÃªt pour la production**.

**Analogie** : Pour un examen, les exit criteria disent "vous avez terminÃ© quand vous avez rÃ©pondu Ã  toutes les questions". Les release criteria disent "vous Ãªtes admis si vous avez au moins 10/20".

### Niveau 2 - Approfondissement Expert

#### Exit Criteria (Phase Test)

Conditions pour considÃ©rer la phase de test terminÃ©e :

| CritÃ¨re | Seuil exemple |
|---------|---------------|
| Test coverage | > 80% code, 100% requirements |
| Test execution | 100% des tests planifiÃ©s exÃ©cutÃ©s |
| Pass rate | > 95% des tests passent |
| Critical bugs | 0 ouvert |
| Major bugs | Tous assignÃ©s avec ETA |
| Test documentation | Rapport complet |
| UAT | Sign-off obtenu |

#### Release Criteria (Go-Live)

Conditions pour mettre en production :

| CatÃ©gorie | CritÃ¨re | Seuil |
|-----------|---------|-------|
| **Fonctionnel** | Critical/Blocker bugs | 0 |
| **Fonctionnel** | Major bugs | < 3 avec workaround |
| **Performance** | Page load time | < 3s |
| **Performance** | API response time P95 | < 500ms |
| **SÃ©curitÃ©** | OWASP Top 10 | 0 critical vulnerability |
| **SÃ©curitÃ©** | Dependencies | 0 known critical CVE |
| **AccessibilitÃ©** | WCAG AA | Compliance |
| **Documentation** | Release notes | ComplÃ¨tes |
| **Operations** | Rollback plan | TestÃ© |
| **Operations** | Monitoring | Dashboards prÃªts |
| **Business** | UAT sign-off | Obtenu |

### Niveau 3 - Application Pratique

#### Template Exit Criteria

```
EXIT CRITERIA - PHASE TEST
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Projet: Checkout v2.0
Date Ã©valuation: 18/01/2025

FONCTIONNEL
â”œâ”€â”€ [âœ…] Test coverage: 85% (target: >80%)
â”œâ”€â”€ [âœ…] Tests exÃ©cutÃ©s: 450/450 (100%)
â”œâ”€â”€ [âœ…] Pass rate: 97% (target: >95%)
â”œâ”€â”€ [âœ…] Critical bugs: 0 (target: 0)
â”œâ”€â”€ [âœ…] Major bugs: 2 open, tous avec workaround
â””â”€â”€ [âœ…] UAT sign-off: obtenu 17/01

PERFORMANCE
â”œâ”€â”€ [âœ…] Load test: 500 users concurrent OK
â”œâ”€â”€ [âœ…] Page load: 2.1s (target: <3s)
â””â”€â”€ [âœ…] API P95: 320ms (target: <500ms)

SÃ‰CURITÃ‰
â”œâ”€â”€ [âœ…] SAST scan: 0 critical, 2 medium (accepted)
â”œâ”€â”€ [âœ…] DAST scan: 0 findings
â”œâ”€â”€ [âœ…] Dependency check: 0 critical CVE
â””â”€â”€ [âš ï¸] Pen test: PlanifiÃ© post-release

DOCUMENTATION
â”œâ”€â”€ [âœ…] Release notes: complÃ¨tes
â”œâ”€â”€ [âœ…] API docs: Ã  jour
â””â”€â”€ [âœ…] Runbook: mis Ã  jour

VERDICT: âœ… EXIT CRITERIA MET
```

---

## 8.2 Quality Gates

### Niveau 1 - Vulgarisation

Les **Quality Gates** sont des **checkpoints automatisÃ©s** qui bloquent la progression si la qualitÃ© n'est pas au niveau requis. C'est comme un tourniquet qui ne s'ouvre que si vous avez le bon badge.

### Niveau 2 - Approfondissement Expert

#### Quality Gates par Ã©tape

```
CODE â”€â”€â–¶ GATE 1 â”€â”€â–¶ BUILD â”€â”€â–¶ GATE 2 â”€â”€â–¶ DEPLOY â”€â”€â–¶ GATE 3 â”€â”€â–¶ RELEASE
         â”‚                    â”‚                    â”‚
         â”‚                    â”‚                    â”‚
         â–¼                    â–¼                    â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Lint    â”‚          â”‚ Tests   â”‚          â”‚ UAT     â”‚
    â”‚ SAST    â”‚          â”‚ pass    â”‚          â”‚ Sign-offâ”‚
    â”‚ Coverageâ”‚          â”‚ Perf OK â”‚          â”‚ Criteriaâ”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Configuration SonarQube Quality Gate

```yaml
# sonar-project.properties
sonar.qualitygate.wait=true

# Quality Gate Conditions (exemple)
# - Coverage on New Code >= 80%
# - Duplicated Lines on New Code <= 3%
# - Maintainability Rating on New Code is A
# - Reliability Rating on New Code is A
# - Security Rating on New Code is A
# - Security Hotspots Reviewed >= 100%
```

### Niveau 3 - Application Pratique

#### Gates e-commerce

| Gate | Quand | CritÃ¨res | Bloquant |
|------|-------|----------|----------|
| **Commit Gate** | Push | Lint, format, unit tests | Oui |
| **PR Gate** | Pull Request | Coverage > 80%, SAST clean | Oui |
| **Build Gate** | CI | Integration tests pass | Oui |
| **Deploy Gate** | Pre-staging | E2E smoke pass | Oui |
| **Release Gate** | Pre-prod | Full regression, perf, UAT | Oui |

---

## 8.3 Go/No-Go Checklist

### Niveau 1 - Vulgarisation

La **checklist Go/No-Go** est la liste complÃ¨te des Ã©lÃ©ments Ã  vÃ©rifier avant de dÃ©cider si on lance ou non en production.

### Niveau 2 - Approfondissement Expert

#### Checklist complÃ¨te

```
GO/NO-GO CHECKLIST
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

QUALITÃ‰
â”œâ”€â”€ [ ] Tous les tests critiques passent
â”œâ”€â”€ [ ] Aucun bug Critical/Blocker ouvert
â”œâ”€â”€ [ ] Coverage > seuil dÃ©fini
â”œâ”€â”€ [ ] UAT sign-off obtenu
â””â”€â”€ [ ] Known issues documentÃ©s et acceptÃ©s

PERFORMANCE
â”œâ”€â”€ [ ] Load test validÃ©
â”œâ”€â”€ [ ] Performance baseline respectÃ©e
â”œâ”€â”€ [ ] CDN configurÃ©
â””â”€â”€ [ ] Auto-scaling testÃ©

SÃ‰CURITÃ‰
â”œâ”€â”€ [ ] Scan sÃ©curitÃ© passÃ©
â”œâ”€â”€ [ ] Secrets rotÃ©s si nÃ©cessaire
â”œâ”€â”€ [ ] Backup vÃ©rifiÃ©
â””â”€â”€ [ ] SSL/TLS configurÃ©

OPÃ‰RATIONS
â”œâ”€â”€ [ ] Runbook Ã  jour
â”œâ”€â”€ [ ] Monitoring configurÃ©
â”œâ”€â”€ [ ] Alerting en place
â”œâ”€â”€ [ ] On-call identifiÃ©
â”œâ”€â”€ [ ] Rollback testÃ©
â””â”€â”€ [ ] CapacitÃ© suffisante

COMMUNICATION
â”œâ”€â”€ [ ] Release notes prÃªtes
â”œâ”€â”€ [ ] Ã‰quipe support informÃ©e
â”œâ”€â”€ [ ] Clients prÃ©venus (si breaking changes)
â””â”€â”€ [ ] Communication interne faite

BUSINESS
â”œâ”€â”€ [ ] Pas de conflit calendrier (Black Friday, etc.)
â”œâ”€â”€ [ ] Ã‰quipe disponible post-release
â”œâ”€â”€ [ ] Sponsor business OK
â””â”€â”€ [ ] Legal/Compliance OK

DÃ‰CISION: [ ] GO  [ ] NO-GO  [ ] CONDITIONNEL

Si NO-GO, raisons:
_________________________________________________

Si CONDITIONNEL, conditions:
_________________________________________________

Signataires:
- Tech Lead: _________________ Date: __/__/__
- QA Lead: __________________ Date: __/__/__
- Product Owner: _____________ Date: __/__/__
- Ops Lead: _________________ Date: __/__/__
```

---

## 8.4 Risk Acceptance Documentation

### Niveau 1 - Vulgarisation

Parfois, on dÃ©cide de livrer malgrÃ© des risques connus. La **documentation d'acceptation des risques** formalise ces dÃ©cisions pour protÃ©ger tout le monde.

### Niveau 2 - Approfondissement Expert

#### Template Risk Acceptance

```
RISK ACCEPTANCE DOCUMENT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Risk ID: RISK-2025-001
Date: 18/01/2025
Project: Checkout v2.0

DESCRIPTION DU RISQUE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Le bug BUG-789 (affichage incorrect du prix en centimes
d'euros pour les commandes DOM-TOM) n'est pas corrigÃ©
avant la release.

IMPACT
- ProbabilitÃ©: Moyenne (10% des commandes DOM-TOM)
- Impact: Mineur (confusion visuelle, pas de perte financiÃ¨re)
- Clients affectÃ©s: ~50/mois

MITIGATION
- Workaround: Le prix correct apparaÃ®t au rÃ©capitulatif final
- Communication: Note au service client
- Fix planifiÃ©: Sprint suivant (release 2.0.1)

DÃ‰CISION
[X] Risque acceptÃ©  [ ] Risque refusÃ©

JUSTIFICATION
Le volume de commandes DOM-TOM est faible et le workaround
existe. Reporter la release aurait un coÃ»t business supÃ©rieur.

APPROBATIONS
- Product Owner: Marie D. __________ Date: 18/01/2025
- Tech Lead: Pierre M. _____________ Date: 18/01/2025
- Sponsor: Jean-Claude R. __________ Date: 18/01/2025
```

---

## 8.5 Stakeholder Sign-off

### Niveau 1 - Vulgarisation

Le **sign-off des parties prenantes** est l'accord formel des dÃ©cideurs pour la mise en production. C'est le "feu vert" officiel.

### Niveau 2 - Approfondissement Expert

#### Qui doit signer

| RÃ´le | ResponsabilitÃ© | Signe pour |
|------|----------------|------------|
| **Product Owner** | Vision produit | FonctionnalitÃ©s conformes |
| **Tech Lead** | Architecture | StabilitÃ© technique |
| **QA Lead** | QualitÃ© | Tests suffisants |
| **Ops Lead** | OpÃ©rations | DÃ©ployable, monitorable |
| **Security** | SÃ©curitÃ© | Pas de vulnÃ©rabilitÃ© critique |
| **Business Sponsor** | Business | ROI, timing OK |

---

## 8.6 Release Readiness Review

### Niveau 1 - Vulgarisation

La **Release Readiness Review** est la rÃ©union finale oÃ¹ tous les stakeholders valident ensemble que tout est prÃªt pour le Go-Live.

### Niveau 2 - Approfondissement Expert

#### Agenda type

```
RELEASE READINESS REVIEW
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Date: 18/01/2025 15h00
DurÃ©e: 1h
Participants: PO, Tech Lead, QA Lead, Ops, Business Sponsor

AGENDA

1. RÃ©sumÃ© exÃ©cutif (5 min)
   - Scope de la release
   - Dates clÃ©s

2. Statut QualitÃ© (10 min)
   - RÃ©sultats des tests
   - Bugs ouverts
   - Coverage

3. Statut Technique (10 min)
   - Changements d'architecture
   - DÃ©pendances
   - Performance

4. Statut OpÃ©rations (10 min)
   - Plan de dÃ©ploiement
   - Rollback
   - Monitoring

5. Risques et Known Issues (10 min)
   - Risques identifiÃ©s
   - Mitigation
   - Acceptation

6. Checklist Go/No-Go (10 min)
   - Revue point par point

7. DÃ©cision (5 min)
   - Vote Go / No-Go / Conditionnel
   - Actions si No-Go

OUTPUT: DÃ©cision documentÃ©e et signÃ©e
```

---

# Section 9 : Questions Transversales

---

## 9.1 Combien de tests automatisÃ©s est "assez" ?

### RÃ©ponse courte

Il n'y a pas de chiffre magique. L'objectif n'est pas un pourcentage mais une **confiance suffisante** pour dÃ©ployer sereinement.

### Indicateurs pratiques

| Indicateur | Bon signe | Mauvais signe |
|------------|-----------|---------------|
| Escape rate | < 5% bugs en prod | > 10% bugs en prod |
| DÃ©ploiement | Confiance Ã  dÃ©ployer sans stress | Peur de casser quelque chose |
| Feedback | < 15 min pour savoir si c'est cassÃ© | DÃ©couverte le lendemain |
| RÃ©gression | Rare | FrÃ©quente |

### Seuils indicatifs par contexte

| Contexte | Unit | Integration | E2E | Total Coverage |
|----------|------|-------------|-----|----------------|
| MVP/Startup | 50% | Smoke | Critical paths | 40-50% |
| SaaS B2B | 70% | Core flows | Happy paths | 60-70% |
| E-commerce | 80% | Payments, Cart | Checkout, Account | 70-80% |
| Fintech/SantÃ© | 90%+ | Complet | Complet | 85%+ |

---

## 9.2 Balance automated vs manual testing

### Matrice de dÃ©cision

| CritÃ¨re | Automatiser | Garder manuel |
|---------|-------------|---------------|
| FrÃ©quence | RÃ©pÃ©titif | Ponctuel |
| StabilitÃ© | Comportement stable | FonctionnalitÃ© volatile |
| Valeur | Critique, regression | Exploration, UX |
| ROI | Positif sur 6 mois | NÃ©gatif |
| CompÃ©tence | Humain = machine | Humain > machine |

### RÃ©partition type

```
EFFORT DE TEST

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚  AUTOMATISÃ‰ (70-80%)                                   â”‚
â”‚  â”œâ”€â”€ Unit tests                                        â”‚
â”‚  â”œâ”€â”€ Integration tests                                 â”‚
â”‚  â”œâ”€â”€ E2E regression                                    â”‚
â”‚  â”œâ”€â”€ Performance tests                                 â”‚
â”‚  â””â”€â”€ Security scans                                    â”‚
â”‚                                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  MANUEL (20-30%)                                       â”‚
â”‚  â”œâ”€â”€ Exploratory testing                               â”‚
â”‚  â”œâ”€â”€ Usability testing                                 â”‚
â”‚  â”œâ”€â”€ UAT                                               â”‚
â”‚  â”œâ”€â”€ Edge cases complexes                              â”‚
â”‚  â””â”€â”€ Nouvelles fonctionnalitÃ©s (avant automation)      â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 9.3 QA intÃ©grÃ© Ã  l'Ã©quipe vs QA team sÃ©parÃ©e

### Comparaison

| Aspect | QA intÃ©grÃ© (embedded) | QA team sÃ©parÃ©e |
|--------|----------------------|-----------------|
| **Avantages** | Feedback rapide, ownership | Expertise centralisÃ©e, standards |
| | Shift-left naturel | Vision cross-projet |
| | Moins de "handoff" | IndÃ©pendance |
| **InconvÃ©nients** | Dilution expertise | Silos, "mur" |
| | Pression sprint | Feedback tardif |
| | Isolation | Moins d'ownership |
| **Convient Ã ** | Agile, petites Ã©quipes | Grandes organisations, compliance |

### Recommandation

**ModÃ¨le hybride** :
- QA embedded dans chaque Ã©quipe pour le quotidien
- QA Chapter/Guild pour le partage de pratiques
- QA centralisÃ© pour les sujets transverses (sÃ©curitÃ©, perf, outils)

---

## 9.4 Testing sous pression temporelle

### StratÃ©gies

Quand le temps manque, **prioriser par le risque** :

```
STRATÃ‰GIE "TIME-CRUNCH"

TOUJOURS TESTER (mÃªme en urgence):
â”œâ”€â”€ Parcours critique (checkout, login)
â”œâ”€â”€ Changements de code Ã  haut risque
â”œâ”€â”€ Smoke test post-dÃ©ploiement
â””â”€â”€ Security basics

SI UN PEU DE TEMPS:
â”œâ”€â”€ Happy paths des nouvelles features
â”œâ”€â”€ Regression sur zones modifiÃ©es
â””â”€â”€ Cross-browser sur critiques

SI PLUS DE TEMPS:
â”œâ”€â”€ Edge cases
â”œâ”€â”€ Performance basique
â””â”€â”€ Full regression

ACCEPTER DE NE PAS FAIRE:
â”œâ”€â”€ Tests exhaustifs sur features secondaires
â”œâ”€â”€ Full cross-browser/device
â””â”€â”€ Performance optimization tests
```

### Communication

**Documenter les compromis** :
- Ce qui a Ã©tÃ© testÃ©
- Ce qui n'a pas Ã©tÃ© testÃ© (et pourquoi)
- Risques associÃ©s acceptÃ©s

---

## 9.5 Testing en solo (one-person QA)

### PrioritÃ©s pour un QA solo

```
STRATÃ‰GIE ONE-PERSON QA

SEMAINE TYPE:
â”œâ”€â”€ Lundi: Triage bugs, planification
â”œâ”€â”€ Mardi-Mercredi: Tests manuels nouvelles features
â”œâ”€â”€ Jeudi: Maintenance automation, code review tests
â””â”€â”€ Vendredi: Exploratory, reporting

INVESTIR DANS:
â”œâ”€â”€ Automation des smokes (ROI max)
â”œâ”€â”€ Templates et checklists
â”œâ”€â”€ Self-service pour devs (lint, unit tests)
â””â”€â”€ Documentation claire

DÃ‰LÃ‰GUER AUX DEVS:
â”œâ”€â”€ Unit tests (obligatoire)
â”œâ”€â”€ Integration tests basiques
â”œâ”€â”€ Fix + verify propre bug
â””â”€â”€ Code review orientÃ©e qualitÃ©

NE PAS FAIRE:
â”œâ”€â”€ Tout automatiser
â”œâ”€â”€ Tester chaque dÃ©tail
â”œâ”€â”€ ÃŠtre le seul Ã  connaÃ®tre l'app
â””â”€â”€ Gatekeeping sans collaboration
```

---

## 9.6 SpÃ©cificitÃ©s du testing e-commerce

### Zones critiques

| Zone | Tests critiques | Risque si Ã©chec |
|------|-----------------|-----------------|
| **Checkout** | Tous les moyens de paiement, calculs | Perte de CA directe |
| **Pricing** | Prix, promos, taxes, devises | Perte financiÃ¨re, lÃ©gal |
| **Inventory** | Stock, rÃ©servation, rupture | Sur-vente, clients mÃ©contents |
| **Performance** | Pics de charge (Black Friday) | Site down = CA perdu |
| **Security** | Paiement, donnÃ©es perso | Fraude, RGPD, rÃ©putation |

### Checklist spÃ©cifique

```
E-COMMERCE TEST CHECKLIST

CATALOGUE
â”œâ”€â”€ [ ] Recherche produits fonctionne
â”œâ”€â”€ [ ] Filtres et tri OK
â”œâ”€â”€ [ ] Stock affichÃ© correctement
â”œâ”€â”€ [ ] Prix et promos corrects
â””â”€â”€ [ ] Images chargent

PANIER
â”œâ”€â”€ [ ] Ajout/suppression/modification quantitÃ©
â”œâ”€â”€ [ ] Calcul total correct
â”œâ”€â”€ [ ] Promos appliquÃ©es correctement
â”œâ”€â”€ [ ] Panier persistant (session, cookie)
â””â”€â”€ [ ] Stock rÃ©servÃ©/libÃ©rÃ©

CHECKOUT
â”œâ”€â”€ [ ] Guest checkout
â”œâ”€â”€ [ ] Compte client checkout
â”œâ”€â”€ [ ] Multi-adresses
â”œâ”€â”€ [ ] Tous moyens de paiement
â”œâ”€â”€ [ ] 3D Secure
â”œâ”€â”€ [ ] Echec paiement + retry
â”œâ”€â”€ [ ] Emails transactionnels
â””â”€â”€ [ ] Confirmation commande

COMPTE CLIENT
â”œâ”€â”€ [ ] Inscription
â”œâ”€â”€ [ ] Connexion / dÃ©connexion
â”œâ”€â”€ [ ] Mot de passe oubliÃ©
â”œâ”€â”€ [ ] Historique commandes
â”œâ”€â”€ [ ] Gestion adresses
â””â”€â”€ [ ] PrÃ©fÃ©rences

POST-ACHAT
â”œâ”€â”€ [ ] Suivi commande
â”œâ”€â”€ [ ] Retours
â”œâ”€â”€ [ ] Remboursements
â”œâ”€â”€ [ ] SAV

MOBILE
â”œâ”€â”€ [ ] Responsive design
â”œâ”€â”€ [ ] Touch interactions
â”œâ”€â”€ [ ] Performance mobile
â””â”€â”€ [ ] Paiement mobile (Apple Pay, etc.)
```

---

# Section 10 : MÃ©tiers et CompÃ©tences

---

## 10.1 QA Engineer / Test Engineer

### DÃ©finition du rÃ´le

Le **QA Engineer** (ou Test Engineer) est le professionnel responsable de la **conception, exÃ©cution et analyse des tests** pour garantir la qualitÃ© du logiciel.

### ResponsabilitÃ©s clÃ©s en phase QualitÃ©

| ResponsabilitÃ© | Description |
|----------------|-------------|
| **Test Planning** | DÃ©finir la stratÃ©gie de test, estimer l'effort |
| **Test Design** | Concevoir les cas de test, scÃ©narios |
| **Test Execution** | ExÃ©cuter les tests manuels et superviser les automatisÃ©s |
| **Defect Management** | Rapporter, suivre, vÃ©rifier les bugs |
| **Reporting** | Produire les mÃ©triques et rapports de qualitÃ© |
| **Process Improvement** | AmÃ©liorer continuellement les pratiques de test |

### CompÃ©tences requises

**Techniques** :
- MÃ©thodologies de test (ISTQB)
- Outils de test management (Jira, TestRail)
- Outils d'automatisation (Selenium, Playwright, Cypress)
- Langages de scripting (JavaScript, Python)
- SQL pour tests base de donnÃ©es
- API testing (Postman, REST)
- CI/CD basics (GitHub Actions, Jenkins)

**Soft skills** :
- Attention aux dÃ©tails
- PensÃ©e critique et analytique
- Communication Ã©crite (bug reports)
- Collaboration avec dÃ©veloppeurs
- CuriositÃ© et persistance

### Parcours type

```
PARCOURS QA ENGINEER

DÃ©butant (0-2 ans)
â”œâ”€â”€ Formation: BTS/DUT Info, Bootcamp, Autodidacte
â”œâ”€â”€ RÃ´le: QA Manual Tester, Junior QA
â”œâ”€â”€ Focus: Tests manuels, bug reports, process de base
â””â”€â”€ Salaire FR: 28-35kâ‚¬

ConfirmÃ© (2-5 ans)
â”œâ”€â”€ Ã‰volution: QA Engineer
â”œâ”€â”€ Focus: Automation, test design, stratÃ©gie
â”œâ”€â”€ Certifications: ISTQB Foundation, ISTQB Agile
â””â”€â”€ Salaire FR: 38-48kâ‚¬

Senior (5+ ans)
â”œâ”€â”€ Ã‰volution: Senior QA, QA Lead
â”œâ”€â”€ Focus: Architecture test, mentoring, process
â”œâ”€â”€ Certifications: ISTQB Advanced (TA, TM)
â””â”€â”€ Salaire FR: 50-65kâ‚¬

Expert/Management
â”œâ”€â”€ Ã‰volution: QA Manager, Head of QA, Quality Director
â”œâ”€â”€ Focus: StratÃ©gie org, Ã©quipes, transformation
â””â”€â”€ Salaire FR: 65-90kâ‚¬+
```

### Certifications reconnues

| Certification | Organisme | Niveau | Description |
|---------------|-----------|--------|-------------|
| **ISTQB CTFL** | ISTQB | Foundation | Base essentielle |
| **ISTQB CTFL-AT** | ISTQB | Foundation | Agile Tester |
| **ISTQB CTAL-TA** | ISTQB | Advanced | Test Analyst |
| **ISTQB CTAL-TM** | ISTQB | Advanced | Test Manager |
| **ISTQB CTAL-TAE** | ISTQB | Advanced | Test Automation Engineer |
| **CAT** | QAI | - | Certified Associate Tester |
| **PSM I** | Scrum.org | - | Pour contexte Agile |

### Ã‰volution de carriÃ¨re

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  CTO / VP Eng   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                 â”‚                 â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Head of Quality â”‚ â”‚ Engineering â”‚ â”‚    Product    â”‚
  â”‚    / QA Dir     â”‚ â”‚   Manager   â”‚ â”‚    Manager    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   QA Manager    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚    QA Lead      â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Senior QA Eng  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
           â”‚                           â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚    QA Engineer  â”‚         â”‚     SDET        â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Junior QA / QA  â”‚
  â”‚    Analyst      â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 10.2 QA Lead / Test Manager

### DÃ©finition du rÃ´le

Le **QA Lead** ou **Test Manager** supervise une Ã©quipe de QA et est responsable de la **stratÃ©gie de test globale** d'un projet ou d'un produit.

### ResponsabilitÃ©s clÃ©s

| ResponsabilitÃ© | Description |
|----------------|-------------|
| **Test Strategy** | DÃ©finir et faire Ã©voluer la stratÃ©gie de test |
| **Team Management** | GÃ©rer l'Ã©quipe QA (recrutement, formation, objectifs) |
| **Planning** | Estimer et planifier les activitÃ©s de test |
| **Coordination** | Interface avec dev leads, PO, stakeholders |
| **Quality Metrics** | DÃ©finir et suivre les KPIs qualitÃ© |
| **Process** | Ã‰tablir et amÃ©liorer les processus QA |
| **Risk Management** | Identifier et gÃ©rer les risques qualitÃ© |

### CompÃ©tences requises

**Techniques** :
- Toutes les compÃ©tences QA Engineer
- ExpÃ©rience en automatisation
- Connaissance approfondie CI/CD
- MÃ©triques et reporting avancÃ©
- Gestion de budget test

**Management** :
- Leadership d'Ã©quipe
- Gestion des conflits
- Communication stakeholders
- NÃ©gociation (ressources, dÃ©lais)
- Recrutement et Ã©valuation

### Certifications recommandÃ©es

- **ISTQB CTAL-TM** (Test Manager)
- **ISTQB Expert Level Test Management**
- **PMP** ou **PMI-ACP** (gestion de projet)
- **SAFe** (contexte Agile scale)

### Salaire indicatif

| Niveau | France | International (US/UK) |
|--------|--------|----------------------|
| QA Lead | 50-65kâ‚¬ | $90-130k / Â£60-80k |
| Test Manager | 60-80kâ‚¬ | $110-150k / Â£70-100k |
| Head of QA | 80-120kâ‚¬ | $150-200k / Â£100-140k |

**[Ã€ VÃ‰RIFIER]** : Ces fourchettes sont indicatives et varient selon la rÃ©gion, la taille de l'entreprise et le secteur.

---

## 10.3 SDET (Software Development Engineer in Test)

### DÃ©finition du rÃ´le

Le **SDET** est un hybride entre dÃ©veloppeur et testeur, spÃ©cialisÃ© dans l'**automatisation des tests** et les **outils de qualitÃ©**. Il Ã©crit du code de test avec les mÃªmes standards qu'un dÃ©veloppeur.

### DiffÃ©rence avec QA Engineer

| Aspect | QA Engineer | SDET |
|--------|-------------|------|
| Focus | QualitÃ© globale, tests variÃ©s | Automatisation, tooling |
| Code | Scripting basique | DÃ©veloppement avancÃ© |
| Ratio dev/test | 30/70 | 70/30 |
| Frameworks | Utilise | CrÃ©e, maintient |
| Tests manuels | FrÃ©quents | Rares |

### ResponsabilitÃ©s clÃ©s

| ResponsabilitÃ© | Description |
|----------------|-------------|
| **Test Automation** | DÃ©velopper et maintenir les frameworks de test |
| **CI/CD Integration** | IntÃ©grer les tests dans les pipelines |
| **Tooling** | CrÃ©er des outils internes de test |
| **Code Review** | Participer aux reviews (code de test et prod) |
| **Infrastructure** | GÃ©rer les environnements de test |
| **Mentoring** | Former les QA Ã  l'automation |

### CompÃ©tences requises

**Techniques** :
- Langages de programmation (JavaScript/TypeScript, Python, Java)
- Frameworks de test (Playwright, Cypress, Selenium, pytest)
- Design patterns (Page Object, Factory, etc.)
- Architecture logicielle
- Docker, Kubernetes basics
- Cloud (AWS, GCP, Azure)
- Git avancÃ©

**QualitÃ©** :
- Principes de test (ISTQB)
- Test design
- Analyse de risques

### Parcours type

```
PARCOURS SDET

Option A: QA â†’ SDET
â”œâ”€â”€ QA Engineer (2-3 ans)
â”œâ”€â”€ Formation dev intensive
â”œâ”€â”€ Transition progressive vers automation
â””â”€â”€ SDET Junior puis confirmÃ©

Option B: Dev â†’ SDET
â”œâ”€â”€ Developer (2-3 ans)
â”œâ”€â”€ IntÃ©rÃªt pour la qualitÃ©
â”œâ”€â”€ Formation testing (ISTQB)
â””â”€â”€ SDET

Ã‰volution SDET:
â”œâ”€â”€ Senior SDET
â”œâ”€â”€ Staff SDET / Principal
â”œâ”€â”€ Test Architect
â””â”€â”€ Engineering Manager (Test Platform)
```

### Salaire indicatif

| Niveau | France | International |
|--------|--------|---------------|
| SDET Junior | 38-45kâ‚¬ | $80-100k |
| SDET ConfirmÃ© | 48-60kâ‚¬ | $100-140k |
| Senior SDET | 60-80kâ‚¬ | $140-180k |
| Staff SDET | 80-100kâ‚¬ | $180-220k |

---

## 10.4 Test Automation Engineer

### DÃ©finition du rÃ´le

Le **Test Automation Engineer** est spÃ©cialisÃ© dans l'**automatisation des tests** sans nÃ©cessairement avoir le profil dÃ©veloppeur du SDET. Focus sur l'utilisation des outils plutÃ´t que leur crÃ©ation.

### DiffÃ©rence avec SDET

| Aspect | Test Automation Engineer | SDET |
|--------|-------------------------|------|
| Profil | QA orientÃ© automation | Dev orientÃ© test |
| Code | Scripts, configuration | Architecture, frameworks |
| CrÃ©ation outils | Rare | FrÃ©quent |
| Tests manuels | Occasionnels | TrÃ¨s rares |

### CompÃ©tences requises

- Outils d'automatisation (Playwright, Cypress, Selenium)
- Scripting (JavaScript, Python)
- CI/CD (Jenkins, GitHub Actions)
- API testing
- Gestion de donnÃ©es de test
- Reporting et analyse

### Certifications

- **ISTQB CTAL-TAE** (Test Automation Engineer)
- Certifications outils spÃ©cifiques (Selenium, etc.)

---

## 10.5 Performance Engineer

### DÃ©finition du rÃ´le

Le **Performance Engineer** est spÃ©cialisÃ© dans les **tests de performance** : charge, stress, endurance, scalabilitÃ©. Il identifie les goulots d'Ã©tranglement et optimise les performances.

### ResponsabilitÃ©s clÃ©s

| ResponsabilitÃ© | Description |
|----------------|-------------|
| **Performance Testing** | Concevoir et exÃ©cuter les tests de charge |
| **Analysis** | Analyser les rÃ©sultats, identifier les bottlenecks |
| **Optimization** | Recommander des optimisations |
| **Monitoring** | Mettre en place le monitoring de performance |
| **Capacity Planning** | Aider Ã  dimensionner l'infrastructure |
| **Benchmarking** | Ã‰tablir et suivre les baselines |

### CompÃ©tences requises

**Techniques** :
- Outils de load testing (k6, JMeter, Gatling)
- Profiling et APM (New Relic, Datadog, Dynatrace)
- Infrastructure (servers, load balancers, CDN)
- Databases (query optimization, indexing)
- Networking basics
- Scripting (Python, JavaScript)
- Cloud services (auto-scaling, etc.)

**Analytiques** :
- Analyse de donnÃ©es
- Statistiques
- Visualisation (Grafana)
- Root cause analysis

### Certifications

- Pas de certification standardisÃ©e dominante
- Certifications cloud (AWS, GCP) utiles
- ISTQB Performance Testing (module spÃ©cialisÃ©)

### Salaire indicatif

| Niveau | France |
|--------|--------|
| Junior | 40-50kâ‚¬ |
| ConfirmÃ© | 50-65kâ‚¬ |
| Senior | 65-85kâ‚¬ |
| Expert | 85-110kâ‚¬ |

---

## 10.6 Security Tester / Penetration Tester

### DÃ©finition du rÃ´le

Le **Security Tester** (ou **Penetration Tester / Ethical Hacker**) est spÃ©cialisÃ© dans l'**identification des vulnÃ©rabilitÃ©s** de sÃ©curitÃ© via des tests offensifs.

### ResponsabilitÃ©s clÃ©s

| ResponsabilitÃ© | Description |
|----------------|-------------|
| **Penetration Testing** | Tests d'intrusion (web, mobile, rÃ©seau) |
| **Vulnerability Assessment** | Identification des failles |
| **Security Audits** | Audits de conformitÃ© (OWASP, PCI-DSS) |
| **Reporting** | Rapports de vulnÃ©rabilitÃ©s avec remÃ©diation |
| **Red Teaming** | Simulation d'attaques rÃ©alistes |
| **Security Training** | Former les Ã©quipes dev |

### CompÃ©tences requises

**Techniques** :
- OWASP Top 10, WSTG
- Outils : Burp Suite, OWASP ZAP, Metasploit
- Scripting : Python, Bash
- Networking : TCP/IP, protocoles
- Web technologies : HTTP, APIs, OAuth
- Cryptographie basics
- Reverse engineering (avancÃ©)

**MÃ©thodologies** :
- OWASP Testing Guide
- PTES (Penetration Testing Execution Standard)
- NIST Cybersecurity Framework

### Certifications reconnues

| Certification | Organisme | Description |
|---------------|-----------|-------------|
| **CEH** | EC-Council | Certified Ethical Hacker |
| **OSCP** | Offensive Security | TrÃ¨s respectÃ©e, pratique |
| **GPEN** | GIAC | Penetration Tester |
| **eJPT** | INE | Entry-level, accessible |
| **CREST** | CREST | Reconnue UK/International |
| **CompTIA Security+** | CompTIA | Base sÃ©curitÃ© |
| **CISSP** | (ISC)Â² | Niveau management |

### Parcours type

```
PARCOURS SECURITY TESTER

EntrÃ©e:
â”œâ”€â”€ Dev â†’ Security (shift)
â”œâ”€â”€ Admin sys â†’ Security
â”œâ”€â”€ QA â†’ Security
â””â”€â”€ Ã‰cole spÃ©cialisÃ©e cybersÃ©curitÃ©

Progression:
â”œâ”€â”€ Junior Pentester / Security Analyst (0-2 ans)
â”œâ”€â”€ Pentester (2-5 ans)
â”œâ”€â”€ Senior Pentester / Security Consultant (5+ ans)
â”œâ”€â”€ Security Architect / Red Team Lead
â””â”€â”€ CISO (Chief Information Security Officer)
```

### Salaire indicatif

| Niveau | France | International |
|--------|--------|---------------|
| Junior | 35-45kâ‚¬ | $70-90k |
| ConfirmÃ© | 50-70kâ‚¬ | $100-140k |
| Senior | 70-100kâ‚¬ | $140-200k |
| Expert/Consultant | 100-150kâ‚¬+ | $200k+ |

---

## Tableau rÃ©capitulatif des mÃ©tiers

| MÃ©tier | Focus | Entry Level | Senior | Certification clÃ© |
|--------|-------|-------------|--------|-------------------|
| QA Engineer | QualitÃ© globale | 28-35kâ‚¬ | 50-65kâ‚¬ | ISTQB CTFL |
| QA Lead | Management QA | 50-65kâ‚¬ | 70-90kâ‚¬ | ISTQB CTAL-TM |
| SDET | Dev + Test | 38-45kâ‚¬ | 60-80kâ‚¬ | ISTQB + Dev |
| Test Auto Engineer | Automatisation | 35-42kâ‚¬ | 55-70kâ‚¬ | ISTQB TAE |
| Performance Engineer | Performance | 40-50kâ‚¬ | 65-85kâ‚¬ | - |
| Security Tester | SÃ©curitÃ© | 35-45kâ‚¬ | 70-100kâ‚¬ | OSCP, CEH |

**Note** : Salaires indicatifs France, grandes villes. Varient selon rÃ©gion, secteur, taille entreprise.

---

# Section 11 : Checklist de Phase QualitÃ©

## Checklist complÃ¨te

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
           CHECKLIST PHASE QUALITÃ‰ - E-COMMERCE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PRÃ‰PARATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â–¡ Test strategy dÃ©finie et approuvÃ©e
â–¡ Test plan documentÃ©
â–¡ Environnements de test configurÃ©s
â–¡ DonnÃ©es de test prÃ©parÃ©es
â–¡ Ã‰quipe QA briefÃ©e sur le scope
â–¡ CritÃ¨res d'acceptation clairs pour chaque feature
â–¡ Outils de tracking configurÃ©s (Jira, etc.)
â–¡ Access credentials pour tous les testeurs

TESTS FONCTIONNELS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Unit Tests
â–¡ Coverage > 80%
â–¡ Tous les tests passent
â–¡ Pas de tests dÃ©sactivÃ©s sans raison

Integration Tests
â–¡ APIs testÃ©es (tous endpoints critiques)
â–¡ Database operations vÃ©rifiÃ©es
â–¡ External services mockÃ©s/testÃ©s

E2E Tests
â–¡ Happy paths automatisÃ©s
â–¡ Parcours critique checkout couvert
â–¡ Cross-browser vÃ©rifiÃ© (Chrome, Safari, Firefox)
â–¡ Mobile responsive testÃ©

Regression
â–¡ Suite de rÃ©gression exÃ©cutÃ©e
â–¡ Aucune rÃ©gression critique

TESTS NON-FONCTIONNELS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Performance
â–¡ Load test exÃ©cutÃ© (charge attendue)
â–¡ Stress test exÃ©cutÃ© (limites connues)
â–¡ Response times dans les seuils
â–¡ Core Web Vitals OK (LCP < 2.5s, INP < 200ms, CLS < 0.1)

SÃ©curitÃ©
â–¡ SAST scan clean (0 critical)
â–¡ DAST scan exÃ©cutÃ©
â–¡ Dependency check (0 critical CVE)
â–¡ OWASP Top 10 vÃ©rifiÃ©
â–¡ Headers de sÃ©curitÃ© configurÃ©s
â–¡ Secrets non exposÃ©s

AccessibilitÃ©
â–¡ Audit Lighthouse > 90
â–¡ Navigation clavier fonctionnelle
â–¡ Screen reader testÃ© (parcours critique)
â–¡ Contrastes vÃ©rifiÃ©s

TESTS E-COMMERCE SPÃ‰CIFIQUES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Catalogue
â–¡ Recherche fonctionne
â–¡ Filtres fonctionnent
â–¡ Prix affichÃ©s correctement
â–¡ Stock affichÃ© correctement

Panier
â–¡ Ajout/suppression/modification OK
â–¡ Calculs corrects (sous-total, taxes, shipping)
â–¡ Codes promo fonctionnent
â–¡ Panier persistant

Checkout
â–¡ Guest checkout
â–¡ Checkout utilisateur connectÃ©
â–¡ Toutes les adresses testÃ©es (France, DOM-TOM, EU)
â–¡ Tous les moyens de paiement testÃ©s
  â–¡ CB Visa/Mastercard
  â–¡ PayPal
  â–¡ Apple Pay / Google Pay
  â–¡ Paiement 3x/4x (si applicable)
â–¡ 3D Secure testÃ©
â–¡ Ã‰chec paiement + retry testÃ©
â–¡ Emails transactionnels envoyÃ©s

Post-achat
â–¡ Confirmation commande affichÃ©e
â–¡ Email confirmation reÃ§u
â–¡ Commande visible dans compte
â–¡ Suivi commande fonctionnel

UAT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â–¡ Tous les scÃ©narios UAT exÃ©cutÃ©s
â–¡ Feedback collectÃ© et triÃ©
â–¡ Bugs critiques/majeurs rÃ©solus
â–¡ Sign-off obtenu

RELEASE READINESS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â–¡ Release notes prÃ©parÃ©es
â–¡ Documentation mise Ã  jour
â–¡ Ã‰quipe support formÃ©e
â–¡ Monitoring configurÃ©
â–¡ Alerting en place
â–¡ Rollback plan testÃ©
â–¡ On-call identifiÃ©
â–¡ Communication planifiÃ©e
â–¡ Go/No-Go meeting tenu
â–¡ Toutes les approbations obtenues

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

# Section 12 : Red Flags et Anti-Patterns

## Tableau des signaux d'alerte

| Signal d'alerte | Cause probable | Action recommandÃ©e |
|-----------------|----------------|-------------------|
| Tests toujours verts | Tests trop faibles, pas d'assertions | Review des tests, mutation testing |
| Tests toujours rouges | Tests abandonnÃ©s ou environment cassÃ© | Fix ou suppression |
| Flaky tests ignorÃ©s | Pression delivery, pas de temps | Time-box pour fix |
| Coverage qui baisse | Nouveau code non testÃ© | Quality gate bloquante |
| UAT en derniÃ¨re minute | Mauvaise planification | Shift-left UAT |
| "On testera en prod" | Manque de confiance dans les tests | Renforcer l'automation |
| Bugs critiques en prod | Testing insuffisant | RCA, amÃ©liorer coverage |
| QA bottleneck | QA trop tard, trop centralisÃ© | Shift-left, embedded QA |
| Pas de regression suite | Trop de manual testing | Investir en automation |
| Bug reports incomplets | Manque de formation/process | Template obligatoire |
| Triage jamais fait | Pas de prioritÃ© qualitÃ© | Meeting rÃ©gulier obligatoire |
| Tests manuels rÃ©pÃ©titifs | Automation non priorisÃ©e | ROI analysis, automatiser |
| Environnement instable | Infra nÃ©gligÃ©e | Investir en test infra |
| Personne ne lit les rapports | Rapports trop longs/complexes | Simplifier, dashboards |
| "Ce n'est pas un bug" | Specs floues | Clarifier acceptance criteria |

---

# Section 13 : Quick Reference

## Testing Quadrants

```
                        Business-Facing
                              â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚         Q2             â”‚          Q3            â”‚
     â”‚    Functional Tests    â”‚   Exploratory Testing  â”‚
     â”‚    Story Tests         â”‚   Usability Testing    â”‚
     â”‚    [AUTO]              â”‚   UAT [MANUAL]         â”‚
     â”‚                        â”‚                        â”‚
Supportâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€Critique
Team   â”‚         Q1             â”‚          Q4            â”‚Product
     â”‚    Unit Tests          â”‚   Performance Testing  â”‚
     â”‚    Component Tests     â”‚   Security Testing     â”‚
     â”‚    [AUTO]              â”‚   [TOOLS]              â”‚
     â”‚                        â”‚                        â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                        Technology-Facing
```

## Test Automation Pyramid

```
            /\
           /  \         E2E (10%)
          /â”€â”€â”€â”€\        Lent, fragile, cher
         /      \
        /â”€â”€â”€â”€â”€â”€â”€â”€\      Integration (20%)
       /          \     Moyen
      /â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\
     /              \   Unit (70%)
    /â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\  Rapide, stable, pas cher
```

## Severity vs Priority

| | Impact Faible | Impact Moyen | Impact Ã‰levÃ© |
|---|---------------|--------------|--------------|
| **Proba Ã‰levÃ©e** | P3 | P2 | P1 |
| **Proba Moyenne** | P4 | P3 | P2 |
| **Proba Faible** | P4 | P4 | P3 |

**Severity** (technique) :
- **Critical** : SystÃ¨me inutilisable
- **Major** : Feature majeure cassÃ©e
- **Minor** : ProblÃ¨me avec workaround
- **Trivial** : CosmÃ©tique

**Priority** (business) :
- **P1** : Fix immÃ©diat (heures)
- **P2** : Fix ce sprint
- **P3** : PlanifiÃ©
- **P4** : Backlog

## OWASP Top 10 (2021) - RÃ©sumÃ©

| # | CatÃ©gorie | Risque | Exemple |
|---|-----------|--------|---------|
| A01 | Broken Access Control | AccÃ¨s non autorisÃ© | IDOR, privilege escalation |
| A02 | Cryptographic Failures | DonnÃ©es exposÃ©es | Passwords en clair |
| A03 | Injection | ExÃ©cution de code | SQL injection, XSS |
| A04 | Insecure Design | Failles architecture | Pas de rate limiting |
| A05 | Security Misconfiguration | Mauvaise config | Debug en prod |
| A06 | Vulnerable Components | DÃ©pendances vulnÃ©rables | Log4Shell |
| A07 | Auth Failures | Auth faible | Brute force possible |
| A08 | Integrity Failures | Code/donnÃ©es non vÃ©rifiÃ©s | Supply chain attack |
| A09 | Logging Failures | Pas de monitoring | Intrusion non dÃ©tectÃ©e |
| A10 | SSRF | RequÃªtes serveur manipulÃ©es | AccÃ¨s rÃ©seau interne |

## MÃ©triques clÃ©s

| MÃ©trique | Formule | Bon seuil |
|----------|---------|-----------|
| Defect Density | Bugs / KLOC | < 5 |
| Defect Escape Rate | Bugs prod / Total | < 5% |
| Test Pass Rate | Tests pass / Total | > 95% |
| Code Coverage | Lines covered / Total | > 80% |
| MTTR | Avg resolution time | < 3 jours |
| Reopen Rate | Reopened / Closed | < 5% |

## Commandes utiles

```bash
# Jest (JavaScript)
npm test -- --coverage
npm test -- --watchAll

# Playwright
npx playwright test
npx playwright test --ui
npx playwright show-report

# Cypress
npx cypress open
npx cypress run

# k6 (Load testing)
k6 run load-test.js

# OWASP ZAP (CLI)
zap-cli quick-scan https://example.com

# Lighthouse
lighthouse https://example.com --output html
```

---

# Glossaire

| Terme | DÃ©finition |
|-------|------------|
| **A/B Testing** | Comparaison de deux versions pour mesurer les performances |
| **Acceptance Criteria** | Conditions pour qu'une feature soit considÃ©rÃ©e "terminÃ©e" |
| **API** | Application Programming Interface |
| **BDD** | Behavior-Driven Development |
| **CI/CD** | Continuous Integration / Continuous Deployment |
| **Coverage** | Pourcentage de code exÃ©cutÃ© par les tests |
| **DAST** | Dynamic Application Security Testing |
| **DRE** | Defect Removal Efficiency |
| **E2E** | End-to-End (tests bout en bout) |
| **Flaky Test** | Test instable (parfois passe, parfois Ã©choue) |
| **Happy Path** | ScÃ©nario principal sans erreur |
| **i18n** | Internationalization |
| **ISTQB** | International Software Testing Qualifications Board |
| **KPI** | Key Performance Indicator |
| **l10n** | Localization |
| **LCP** | Largest Contentful Paint (Core Web Vital) |
| **Mock** | Simulation d'un composant pour les tests |
| **MTTR** | Mean Time To Resolution |
| **OWASP** | Open Worldwide Application Security Project |
| **POM** | Page Object Model |
| **QA** | Quality Assurance |
| **QC** | Quality Control |
| **RCA** | Root Cause Analysis |
| **Regression** | Bug introduit suite Ã  une modification |
| **ROI** | Return On Investment |
| **SAST** | Static Application Security Testing |
| **SCA** | Software Composition Analysis |
| **SDET** | Software Development Engineer in Test |
| **Shift-Left** | DÃ©placer les tests plus tÃ´t dans le cycle |
| **Smoke Test** | Test basique de vÃ©rification du build |
| **Stub** | ImplÃ©mentation simplifiÃ©e pour tests |
| **SUS** | System Usability Scale |
| **TDD** | Test-Driven Development |
| **UAT** | User Acceptance Testing |
| **WCAG** | Web Content Accessibility Guidelines |
| **WSTG** | Web Security Testing Guide (OWASP) |

---

# Bibliographie et Sources

## Standards et Certifications

- **ISTQB** - International Software Testing Qualifications Board
  - Foundation Level Syllabus v4.0 (2023)
  - https://www.istqb.org/

- **IEEE 829** - Standard for Software and System Test Documentation
  - IEEE 829-2008

- **ISO/IEC 25010** - Software Quality Model
  - https://iso25000.com/index.php/en/iso-25000-standards/iso-25010

- **ISO/IEC/IEEE 29119** - Software Testing Standards
  - Parts 1-5, documentation and processes

## SÃ©curitÃ©

- **OWASP Top 10 (2021)**
  - https://owasp.org/Top10/

- **OWASP Web Security Testing Guide (WSTG) v4.2**
  - https://owasp.org/www-project-web-security-testing-guide/

- **OWASP Application Security Verification Standard (ASVS)**
  - https://owasp.org/www-project-application-security-verification-standard/

## AccessibilitÃ©

- **WCAG 2.2** - Web Content Accessibility Guidelines
  - https://www.w3.org/WAI/WCAG22/quickref/

- **WAI-ARIA** - Accessible Rich Internet Applications
  - https://www.w3.org/WAI/standards-guidelines/aria/

## Livres de rÃ©fÃ©rence

- Crispin, L., & Gregory, J. (2009). **Agile Testing: A Practical Guide for Testers and Agile Teams**. Addison-Wesley.

- Crispin, L., & Gregory, J. (2014). **More Agile Testing: Learning Journeys for the Whole Team**. Addison-Wesley.

- Kaner, C., Falk, J., & Nguyen, H. Q. (1999). **Testing Computer Software** (2nd ed.). Wiley.

- Kaner, C., Bach, J., & Pettichord, B. (2001). **Lessons Learned in Software Testing**. Wiley.

- Whittaker, J. A. (2009). **Exploratory Software Testing**. Addison-Wesley.

- Cohn, M. (2009). **Succeeding with Agile: Software Development Using Scrum**. Addison-Wesley. (Pyramide des tests)

- Humble, J., & Farley, D. (2010). **Continuous Delivery**. Addison-Wesley.

- Kim, G., et al. (2016). **The DevOps Handbook**. IT Revolution Press.

## Articles et ressources en ligne

- **Martin Fowler** - Testing articles
  - https://martinfowler.com/testing/

- **Google Testing Blog**
  - https://testing.googleblog.com/

- **Ministry of Testing**
  - https://www.ministryoftesting.com/

- **Test Automation University** (Applitools)
  - https://testautomationu.applitools.com/

## Documentation outils

- **Playwright** - https://playwright.dev/docs/intro
- **Cypress** - https://docs.cypress.io/
- **Jest** - https://jestjs.io/docs/getting-started
- **k6** - https://k6.io/docs/
- **Lighthouse** - https://developer.chrome.com/docs/lighthouse/

---

# Notes et Limitations

## Points nÃ©cessitant vÃ©rification [Ã€ VÃ‰RIFIER]

1. **CoÃ»t des dÃ©fauts par phase** (Section 2.3) : Les ratios 1x â†’ 100x sont souvent citÃ©s mais la source IBM originale est difficile Ã  tracer. L'ordre de grandeur reste valide.

2. **Seuils de coverage** (Section 2.5) : Les pourcentages recommandÃ©s varient selon les sources. Adapter au contexte spÃ©cifique.

3. **Salaires** (Section 10) : Les fourchettes indiquÃ©es sont des estimations basÃ©es sur les donnÃ©es marchÃ© 2024. Varient significativement selon rÃ©gion, secteur, taille d'entreprise.

4. **Parts de marchÃ© navigateurs** (Section 4.4) : Les chiffres Ã©voluent constamment. Consulter StatCounter ou similaire pour donnÃ©es actuelles.

## Aspects non couverts en dÃ©tail

- **Testing mobile natif** (iOS/Android apps) : Focus sur le web
- **Testing IoT/Embedded** : Hors scope
- **Testing IA/ML** : Domaine spÃ©cialisÃ© Ã©mergent
- **Compliance spÃ©cifiques** (PCI-DSS dÃ©taillÃ©, HIPAA, SOC2) : MentionnÃ©s mais non dÃ©taillÃ©s
- **Testing jeux vidÃ©o** : Domaine trÃ¨s spÃ©cifique

## Ã‰volutions Ã  surveiller

- **IA dans le testing** : Outils d'auto-gÃ©nÃ©ration de tests, auto-healing
- **Shift-Right** : Testing in production, chaos engineering
- **Contract Testing** : Adoption croissante dans les microservices
- **Playwright** : Momentum fort vs Cypress/Selenium
- **WCAG 3.0** : En dÃ©veloppement, nouvelles approches d'Ã©valuation

---

*Fin du rapport Phase 5 - QualitÃ©*

**Document gÃ©nÃ©rÃ© le 29 dÃ©cembre 2024**
**Version 1.0**
